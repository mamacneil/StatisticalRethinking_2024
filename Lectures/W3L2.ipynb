{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Lecture 2 - The Haunted DAG\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "When N things are factors we often use dummy variables (0,1)'s to represent n-1 of them against a 'baseline' category. This is very standard practice, something I have done for years. However there are problems, because the baseline category has priority over the other variables in that by setting a baseline, we are *a priori* stating that we know more about the baseline category than the other categories. In some cases this may be true - I have typically used the category for which we have the most inoformation as the baseline - but it can also get cumbersome, as we need to assert a prior for each category. \n",
    "\n",
    "Instead, let's use an **index variable**, which will make things much simpler. First import the !Kung data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "kdata = pd.read_csv('howell.csv')\n",
    "kdata['Sex'] = np.array(['Female','Male'])[kdata.male.values]\n",
    "# Display top 5 rows\n",
    "kdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need some code to do the indexing, where each factor gets it's own number. I have used the code below for about a decade for this, as it gives me both a list of unique values, as well as an index number for each observation to connect them to the list of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can easily make an index for male/female in the !Kung data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sex,Is = indexall(kdata.Sex.values)\n",
    "Sex, Is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the categories are added on as they're encountered, so because the first entry is male, the male category is given the first index position, i.e. 0.\n",
    "\n",
    "Next let's run two models, one using the dummy variable, and a second using the index variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as dummy:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Female', 178, 20)\n",
    "    # Male offset\n",
    "    β1 = pm.Normal('Male', 0, 1)\n",
    "    # Linear model\n",
    "    μ = β0+β1*kdata.male.values\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=kdata.height.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as indexed:\n",
    "    # Intercepts\n",
    "    β0 = pm.Normal('Intercpets', 178, 20, shape=2)\n",
    "    # Linear model\n",
    "    μ = β0[Is]\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=kdata.height.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dummy:\n",
    "    trace_d = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with indexed:\n",
    "    trace_i = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at dummy model results\n",
    "pm.summary(trace_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at index-based results\n",
    "tmp = pm.summary(trace_i)\n",
    "tmp['Label'] = Sex+['Sd_obs']\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "pm.plot_density([trace_d.posterior['Female'].values[0],trace_i.posterior['Intercpets'].values[0,None].T[1]], data_labels=['Dummy','Indexed'], ax=ax[0])\n",
    "ax[0].axvline(np.mean(kdata.height.values[kdata.Sex.values=='Female']))\n",
    "ax[0].set_xlim(130,145)\n",
    "ax[0].set_title('Female')\n",
    "\n",
    "pm.plot_density([trace_d.posterior['Female'].values[0]+trace_d.posterior['Male'].values[0],trace_i.posterior['Intercpets'].values[0,None].T[0]], data_labels=['Dummy','Indexed'], ax=ax[1])\n",
    "ax[1].axvline(np.mean(kdata.height.values[kdata.Sex.values=='Male']))\n",
    "ax[1].set_xlim(130,145)\n",
    "ax[1].set_title('Male');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what has happened here? Well it turns out the $N(0,1)$ prior for males in the dummy version was too informative. So if we go for something wider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as dummy:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Female', 178, 20)\n",
    "    # Male offset\n",
    "    β1 = pm.Normal('Male', 0, 5)\n",
    "    # Linear model\n",
    "    μ = β0+β1*kdata.male.values\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=kdata.height.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dummy:\n",
    "    trace_d = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "pm.plot_density([trace_d.posterior['Female'].values[0],trace_i.posterior['Intercpets'].values[0,None].T[1]], data_labels=['Dummy','Indexed'], ax=ax[0])\n",
    "ax[0].axvline(np.mean(kdata.height.values[kdata.Sex.values=='Female']))\n",
    "ax[0].set_xlim(130,145)\n",
    "ax[0].set_title('Female')\n",
    "\n",
    "pm.plot_density([trace_d.posterior['Female'].values[0]+trace_d.posterior['Male'].values[0],trace_i.posterior['Intercpets'].values[0,None].T[0]], data_labels=['Dummy','Indexed'], ax=ax[1])\n",
    "ax[1].axvline(np.mean(kdata.height.values[kdata.Sex.values=='Male']))\n",
    "ax[1].set_xlim(130,145)\n",
    "ax[1].set_title('Male');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So either version can get us to a simliar place, however the indexed version tends to be far easier to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection distortion effects\n",
    "\n",
    "Among the scariest aspects of statistical models is that things can become spurriously correlated due to the inclusion or exclusion of another, lurking variable. If we don't know about this, and how such things can be induced, we're doomed to report stuff that is complete malarky, adding noise to the scientific cannon. I'm sure I've done this at some point. But knowing is half the battle, so let's start with something called the selection-distortion effect. This is when a third, intervening variable is added that selects for a subset of the data and induces a spurroius correlation. This is something that is known as collider bias (we'll explain below), and it can be a big problem.\n",
    "\n",
    "To illustrate, we can take a look at the simulated scientific distortion example on p162 in the book:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random number seed to get same answers\n",
    "#np.random.seed(seed=1914)\n",
    "\n",
    "# Number of grant proposals\n",
    "N = 200\n",
    "# Proportion to select\n",
    "prop = 0.1\n",
    "\n",
    "# Uncorrelated Newsworthiness and trustworthiness scores\n",
    "nw = np.random.normal(0,1,N)\n",
    "tw = np.random.normal(0,1,N)\n",
    "\n",
    "# Select top 10% of combined scores\n",
    "score = nw+tw\n",
    "indx = score>np.quantile(score,.9)\n",
    "\n",
    "# How correlated?\n",
    "cor_ = np.corrcoef(nw[indx], tw[indx])[0][1]\n",
    "cor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nw,tw)\n",
    "plt.xlabel('Newsworthiness')\n",
    "plt.ylabel('Trustworthiness')\n",
    "plt.savefig('seldis0.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(nw,tw)\n",
    "plt.scatter(nw[indx],tw[indx],c='red')\n",
    "plt.xlabel('Newsworthiness')\n",
    "plt.ylabel('Trustworthiness')\n",
    "plt.savefig('seldis.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most simulations, the red dots will have a negative correlation, entirely due to their having the highest total value for the two covariates. Provided we don't condition on score, this isn't a problem. If we did, we would induce a spurrious correlation that doesn't apply to the data as a whole. Tricky eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGs and why to use 'em\n",
    "\n",
    "Directed Acyclic Graphs are a really important development in modelling observational data (if you've done an experiment, then all power to you), because they give us some sort of ground to stand on in thinking about causality. Again, much of this creit goes to [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl), whose thinking around causality is nobel-prize worthy.\n",
    "\n",
    "Pearl set out 4 key rules, that cover all the major bases. If you address these four things once you've made a causal model, causal inference can follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fork and the pipe\n",
    "\n",
    "The fork and the pipe are equivalent mathematically, in that they are about conditing on an intermediating variable, but for the fork to break a spurrious correlation, and in the not conditioning to estimate an effect. We saw a fork example in the Southern divorce example, with a fork at marriage age (A) that, once conditioned on, breaks the association between marriage rate (M) and divorce (D).\n",
    "\n",
    "For a pipe example, we can simulate some data relating to treatment effects on plants. The pipe is that the treatment (T) reduces fungus (F) and therfore aids in the growth of a plant (G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of plants\n",
    "N = 100\n",
    "\n",
    "# Simulate initial heights\n",
    "h0 = np.random.normal(10,2,N)\n",
    "\n",
    "# Assign treatments\n",
    "treatment = np.random.choice((0,1),N)\n",
    "\n",
    "# Simulate fungus conditional on treatment\n",
    "beta_t = 0.4\n",
    "fungus = np.random.binomial(1,.5-beta_t*treatment)\n",
    "\n",
    "# Generate end heights\n",
    "h1 = h0+np.random.normal(5-3*fungus,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h1[treatment==0],label='control')\n",
    "plt.hist(h1[treatment==1],label='treatment')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.legend()\n",
    "plt.savefig('treat.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from what we've simulated that things are bimodial according to if the plants received the treatment or not. So, we have covariates for `initial height`, `treatment`, and `fungus` - to the multiple regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as plants:\n",
    "    # Intercept\n",
    "    β0 = pm.Lognormal('Intercept', 0, 0.2)\n",
    "    # Treatment effect\n",
    "    β1 = pm.Normal('Treatment', 0, 0.5)\n",
    "    # Fungal effect\n",
    "    β2 = pm.Normal('Fungus', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = (β0+β1*treatment+β2*fungus)*h0\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plants:\n",
    "    trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace)\n",
    "plt.axvline(0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pipe.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the heck? We know the treatment will reduce the probability of fungus by 0.4, and that fungal-infested plants will grow an average of 3cm less (we made the data). So what's going on? Well, conditional on knowing that there is fungus, there is no benefit to knowing about the treatement, whereas conditional on knowing treatment it remains worth knowing if there is fungus (fungus happens to treated plants too). In this case fungus lies along the pipe between treatment and outcome, blocking information that would flow from treatment. Therefore if we want to estimate the effect of treatment we need a model without the post-treatment outcome, fungus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as plants_t:\n",
    "    # Intercept\n",
    "    β0 = pm.Lognormal('Intercept', 0, 0.2)\n",
    "    # Treatment effect\n",
    "    β1 = pm.Normal('Treatment', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = (β0+β1*treatment)*h0\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plants_t:\n",
    "    trace_t = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_t)\n",
    "plt.axvline(0)\n",
    "plt.axvline(beta_t,c='red')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tpipe.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now our effect is 0.15 (or so), not 0.4 - what's going on? Well, our known treatment effect of 0.4 is the reduction in the probability of fungus, while our treatment estimate above is on the effect of treatment on height. So we need to convert back to the probability scale, or convert our known number to the height effect scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average difference between full and fungal growth\n",
    "avg_diff = np.mean((h1-h0)[fungus==0])-np.mean((h1-h0)[fungus==1])\n",
    "avg_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace_t.posterior['Treatment'].values.flatten()*avg_diff)\n",
    "plt.axvline(beta_t,c='red')\n",
    "plt.xlabel('p(fungus|Δheight)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('effect.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at this model using the `daggity` package in R, or online at [DAGitty.net](http://dagitty.net/dags.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collider bias\n",
    "\n",
    "McElreath has an agent-based model of happiness baked into his `rethinking` package, as outlined on p177, with five key conditions\n",
    "\n",
    "    1. Each year, 20 people are born with uniformly distributed happiness values.\n",
    "    2. Each year, each person ages one year. Happiness does not change.\n",
    "    3. At age 18, individuals can become married. The odds of marriage each year are proportional to a person's happiness.\n",
    "    4. Once married, individuals remain married.\n",
    "    5. After age 65, individuals leave the sample (They move to Spain.)\n",
    "    \n",
    "We can import that simulated happiness data to work with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happiness data\n",
    "hdata = pd.read_csv('happiness.csv')\n",
    "hdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.scatter(hdata.age[hdata.married==0],hdata.happiness[hdata.married==0], facecolors='none', edgecolors='black', label='Unmarried')\n",
    "plt.scatter(hdata.age[hdata.married==1],hdata.happiness[hdata.married==1], edgecolors='black',  label='Married')\n",
    "plt.xlabel('Age', fontsize=17)\n",
    "plt.ylabel('Happiness', fontsize=17)\n",
    "plt.legend(loc='upper left', framealpha=1, fontsize=14)\n",
    "plt.savefig('happy.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above assumes happiness is uniformally distributed and never changes. \n",
    "\n",
    "So, pretending we don't know anything about how this was generated, what sort of model should we build? 'Is age related to happiness' we might ask. And we should control for the effect of marriage, right?? A model representing these things would be\n",
    "\n",
    "$$\n",
    "\\mu_i = \\beta_{M[i]}+\\beta_{A}A_i\n",
    "$$\n",
    "\n",
    "with indexed married/not-married intercepts ($\\beta_{M[i]}$) and a parameter for changing happieness with age ($\\beta_{A}$). \n",
    "\n",
    "What makes for sensible priors sensible here? It's difficult to think about how much happiness should increase or decrease per year of age. First we can chuck out the kids, as they can't marry. Then if we scale the ages from 18 to 65 to be over the range 1 to 0, we know the range for happiness is -2 to 2, so we should be cover that range (i.e. 4 units) over the 0 to 1 interval of scaled ages. As 95% of the posterior mass is within 2SD of the mean, setting the prior SD to 4/2 will capture most of the range.\n",
    "\n",
    "For the intercept, now equal to 0 at age 18, we can also span the range, using a $N(0,1)$ prior.\n",
    "\n",
    "\n",
    "In PyMC this would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adults only\n",
    "aindx = hdata.age.values>17\n",
    "# Marriage index\n",
    "Im = hdata.married.values[aindx]\n",
    "# Age - scaled so as to be between 0 and 1\n",
    "AGE = (hdata.age.values[aindx]-18)/(65-18)\n",
    "# Happiness\n",
    "H = hdata.happiness.values[aindx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as happy:\n",
    "    # Intercept\n",
    "    β0 = pm.Normal('Marriage', 0, 1, shape=2)\n",
    "    # Age effect\n",
    "    β1 = pm.Normal('Age', 0, 2)\n",
    "    # Linear model\n",
    "    μ = β0[Im]+β1*AGE\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with happy:\n",
    "    trace_h = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_h)\n",
    "plt.axvline(0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('happy1.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waaait a minute - we know that happiness is consistent across ages (everyone keeps their intial happiness levels in the simulation), so what the heck is the model doing? It is refelcting the model it was given, one where marriage acts as a collider to open a path between age and happiness. How does this occur? Well it occurs because marriage is a common consequence of both age and happiness - people who are older and happier are more likely to be married. Take a look again at the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.scatter(hdata.age[hdata.married==0],hdata.happiness[hdata.married==0], facecolors='none', edgecolors='black', label='Unmarried')\n",
    "plt.scatter(hdata.age[hdata.married==1],hdata.happiness[hdata.married==1], edgecolors='black',  label='Married')\n",
    "plt.xlabel('Age', fontsize=17)\n",
    "plt.ylabel('Happiness', fontsize=17)\n",
    "plt.legend(loc='upper left', framealpha=1, fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see it - the older, happier people tend to be married. The collider. If instead of including it, we leave the collider path closed, we get the right result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as happy2:\n",
    "    # Age effect\n",
    "    β1 = pm.Normal('Age', 0, 2)\n",
    "    # Linear model\n",
    "    μ = β1*AGE\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with happy2:\n",
    "    trace_h2 = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_h2)\n",
    "plt.axvline(0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('happy2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Haunted DAG\n",
    "\n",
    "Of all the many difficulties lurking in our statistical models, perhaps the scariest is that our estimates are influcenced by something we haven't measured, or even thought of. How can we deal with that? First let's look at the grandparents example. Here we are looking at the influence of both parent (P) and grandparent (G) education on children's education (C). The DAG for this is simple in the sense that $G\\rightarrow P \\rightarrow C$ (grandparents influence their kids, who influence their kids) AND $G\\rightarrow C$ (grandparents influence their grandkids directly). \n",
    "\n",
    "![](GPC.jpg)\n",
    "\n",
    "But what if we have a third, lurking variable $U$, that is a common influence on both parents and children? This could be something like the neighbourhood that the parents and kids live in, and by conditioning on the parents, we open up a backdoor path from $G$ to $C$ (via $U$)\n",
    "\n",
    "![](GPCU.jpg)\n",
    "\n",
    "What's messed up about this is that this can happen without our even knowing about $U$ or having ever observed it. Here's a simulation of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of families\n",
    "N = 200\n",
    "\n",
    "# Direct effect of G on P\n",
    "b_GP = 1\n",
    "\n",
    "# Direct effect of G on C\n",
    "b_GC = 0\n",
    "\n",
    "# Direct effect of P on C\n",
    "b_PC = 1\n",
    "\n",
    "# Direct effect of U on P and C\n",
    "b_U = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameter estimates we can simulate from some random normals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate neighbourhood effects - some are positive (+1*U) some are negative (-1*U)\n",
    "U = 2*np.random.binomial(1,0.5,N) - 1\n",
    "\n",
    "# Simulate standard normal grandparents\n",
    "G = np.random.normal(0,1,N)\n",
    "\n",
    "# Simulate Parents, conditional on their influeces (G and U)\n",
    "P = np.random.normal(b_GP*G+b_U*U,1,N)\n",
    "\n",
    "# Simulate Children, conditional on their influeces (G, P, and U)\n",
    "C = np.random.normal(b_PC*P+b_GC*G+b_U*U,1,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naievely running a regression model with parents and grandparents included, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as edu:\n",
    "    # Intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 1)\n",
    "    # Grandparent effect\n",
    "    βg = pm.Normal('Grandparents', 0, 1)\n",
    "    # Parent effect\n",
    "    βp = pm.Normal('Parents', 0, 1)\n",
    "    # Linear model\n",
    "    μ = β0+βg*G+βp*P\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with edu:\n",
    "    trace_e = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_e)\n",
    "plt.axvline(0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pappy.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So without knowing about the neighbourhood effect $U$, it seems like grandparents are messing up their grandkids!\n",
    "\n",
    "From the DAG it's clear that conditioning on P opens up a backdoor path for grandparents to influence their grandkids indirectly through their kids choice of neighbourhood. \n",
    "\n",
    "To see this mechanically, take a look at the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_ = stdize(C)\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.scatter(G[U==1],C_[U==1], facecolors='none', edgecolors='royalblue',label='N1')\n",
    "plt.scatter(G[U==-1],C_[U==-1], facecolors='none', edgecolors='black',label='N2')\n",
    "plt.xlabel('Grandparent education (G)', fontsize=17)\n",
    "plt.ylabel('Child education (C)', fontsize=17)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('GC.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First notice the positive relationship between G and C, even though the direct effect is 0 - how? Well $G\\rightarrow P = 1$ and $P\\rightarrow C = 1$ so there is a pipe from $G\\rightarrow C$ evident in the plot. Now look what happens when we condition on parents - remember that this is a form of selection? - and highlight those parents who happen to lie between the 45th and 60th percentiles of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag parents that lie within percentile range\n",
    "pflag_1 = 1*(P[U==1]>np.percentile(P,45))*(P[U==1]<np.percentile(P,60))\n",
    "pflag_2 = 1*(P[U==-1]>np.percentile(P,45))*(P[U==-1]<np.percentile(P,60))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Plot all points\n",
    "plt.scatter(G[U==1],C_[U==1], facecolors='none', edgecolors='royalblue',label='N1')\n",
    "plt.scatter(G[U==-1],C_[U==-1], facecolors='none', edgecolors='black',label='N2')\n",
    "\n",
    "# Plot filled points \n",
    "plt.scatter(G[U==1][pflag_1==1],C_[U==1][pflag_1==1], facecolors='royalblue', edgecolors='royalblue',label='N1|Pmid')\n",
    "plt.scatter(G[U==-1][pflag_2==1],C_[U==-1][pflag_2==1], facecolors='black', edgecolors='black',label='N1|Pmid')\n",
    "\n",
    "plt.xlabel('Grandparent education (G)', fontsize=17)\n",
    "plt.ylabel('Child education (C)', fontsize=17)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('GC2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is, hidden among the data - by conditioning on P, a negative association has been induced between grandparents and children that has entirely to do with the fact that the unmeasured neighbourhood effect is operating in the background, drawing down child scores in a hidden way. Spooky!\n",
    "\n",
    "So what should we do about all this? Well if we had accounted for U:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as edu_new:\n",
    "    # Intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 1)\n",
    "    # Grandparent effect\n",
    "    βg = pm.Normal('Grandparents', 0, 1)\n",
    "    # Parent effect\n",
    "    βp = pm.Normal('Parents', 0, 1)\n",
    "    # Neighbourhood effect\n",
    "    βu = pm.Normal('Neighbourhood', 0, 1)\n",
    "    # Linear model\n",
    "    μ = β0+βg*G+βp*P+βu*U\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with edu_new:\n",
    "    trace_n = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_n)\n",
    "plt.axvline(0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pappyU.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets it right. But this is kind of unsatisfactory, as we didn't know there was a problem, U stands for unmeasured after all.\n",
    "\n",
    "For now, we'll remain haunted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
