{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 11 Lecture 1 - Missing Data and other Opportunties\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=oMiSb8GKR0o&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=19\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "Dustin Stansbury has some lovely PyMC Code available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import dataframe_image as dfi\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "def stdizeNA(x):\n",
    "    xnew = x\n",
    "    mask = np.isnan(x)==False\n",
    "    xnew[mask] = (x[mask]-np.mean(x[mask]))/np.std(x[mask])\n",
    "    return xnew\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "def invlogit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent states\n",
    "\n",
    "With many observational datasets, we will have both means **and associated variances** (often called observation error) to work with. This could be a measure of variabitliy in a mass-spectrometer for some sample of plan tissue, or it could be summary data from a whole lot of studies (i.e. a meta-analysis). So far we've ignored this, but it can be crucial for accurately representing uncertainty, and making better (unbiased) estimates. \n",
    "\n",
    "By way of example, let's look back at the divorce rates data from the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddata = pd.read_csv('WaffleDivorce.csv', sep=';')\n",
    "ddata['log_population'] = np.log(ddata['Population'])\n",
    "# Grab variables\n",
    "A = stdize(ddata.MedianAgeMarriage.values)\n",
    "M = stdize(ddata.Marriage.values)\n",
    "Mse = ddata['Marriage SE'].values/np.std(ddata.Marriage.values)\n",
    "D = stdize(ddata.Divorce.values)\n",
    "Dse = ddata['Divorce SE'].values/np.std(ddata.Divorce.values)\n",
    "nstates = len(D)\n",
    "State = ddata.Loc.values\n",
    "dfi.export(ddata.head(5), 'divdata.jpg')\n",
    "ddata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely there are SE (standard error) columns, which tell us something about the level of uncertainty in the estimated marriage and divorce rates in each state. These are larger in less populous states because sample sizes there are much smaller. Let's focus on divorce rates and plot them against their mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# points\n",
    "ax[0].scatter(ddata['MedianAgeMarriage'], ddata['Divorce'], marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "# standard errors\n",
    "ax[0].errorbar(ddata['MedianAgeMarriage'], ddata['Divorce'], ddata['Divorce SE'].values, ls='none', color='k', linewidth=1, zorder=0)\n",
    "ax[0].set_xlabel('Median age marriage', fontsize=15)\n",
    "ax[0].set_ylabel('Divorce rate', fontsize=15)\n",
    "ax[0].set_ylim(4, 15)\n",
    "\n",
    "# points\n",
    "ax[1].scatter(ddata['log_population'], ddata['Divorce'], marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "# standard errors\n",
    "ax[1].errorbar(ddata['log_population'], ddata['Divorce'], ddata['Divorce SE'].values, ls='none', color='k', linewidth=1, zorder=0)\n",
    "ax[1].set_xlabel('log(population)', fontsize=15)\n",
    "ax[1].set_ylabel('Divorce rate', fontsize=15)\n",
    "ax[1].set_ylim(4, 15)\n",
    "plt.savefig('waffles.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error bars represent a really important source of variability, one that should make us more skeptical (or at least less sure) about values on the left hand side of the right hand panel. Indeed from that plot it looks like higher or lower mean divorce rates seem to occur where populations are smaller. So how should we add in this information? Well you're in luck - it's remarkably easy in a Bayesian model (WAY easier than for a frequentist one), where we just add an additional layer to incorporate this source of uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's take a look at our original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize variables\n",
    "A = stdize(ddata.MedianAgeMarriage.values)\n",
    "Msd = ddata['Marriage SE'].values/np.std(ddata.Marriage.values)\n",
    "M = stdize(ddata.Marriage.values)\n",
    "Dsd = ddata['Divorce SE'].values/np.std(ddata.Divorce.values)\n",
    "D = stdize(ddata.Divorce.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as AM:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Marriage rate\n",
    "    β1 = pm.Normal('M', 0, 0.5)\n",
    "    # Marriage age\n",
    "    β2 = pm.Normal('A', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = β0+β1*M+β2*A\n",
    "    # Error\n",
    "    σ = pm.Uniform('Sigma', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a second model that incorporates these uncertainty in divorce rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'States':State}) as AM_latent:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Marriage rate\n",
    "    β1 = pm.Normal('M', 0, 0.5)\n",
    "    # Marriage age\n",
    "    β2 = pm.Normal('A', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = β0+β1*M+β2*A\n",
    "    # Error\n",
    "    σ = pm.Uniform('Sigma', 0, 10)\n",
    "    # Latent state\n",
    "    Dμ = pm.Normal('Latent_divorce', μ, σ, dims='States')\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', Dμ, Dsd, observed=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking carefully what you can see is that there is now another layer between the observed divorce rates and the rest of the model. This layer represents the true underlying divorce rates - called a latent state - which we can estimate based on both the observation error and the divorce rate estimates in other states. This kind of latent state (or 'hidden Markov') model is very powerful and we should use it wherever we are able. \n",
    "\n",
    "Let's fire these up and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AM:\n",
    "    trace_am = pm.sample(1000)\n",
    "with AM_latent:\n",
    "    trace_aml = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_am)\n",
    "dfi.export(tmp, 'trace_am.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_aml, var_names=['Intercept', 'M', 'A', 'Sigma'])\n",
    "dfi.export(tmp, 'trace_aml.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinda similar, but take a look at Sigma - it's *gotten smaller*. Why? Because we've apportioned some of the variation to observation error, making us more sure about the variation in underlying divorce rates. This is why we model this stuff - to make better estimates.\n",
    "\n",
    "Let's take a look at what's happened to our latent state estimates relative to their observed means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent estimates\n",
    "Dl = trace_aml.posterior['Latent_divorce'].stack(sample=(\"chain\", \"draw\")).values.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot difference between observed mean and latent mean against observed standard error\n",
    "ax[0].scatter(Dsd, D-Dl, marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "ax[0].axhline(0, linestyle='--', c='grey', zorder=0)\n",
    "# Label outlying states\n",
    "indx = abs(D-Dl)>0.5\n",
    "oset=0.03\n",
    "[ax[0].text(x+oset,y+oset,d) for x,y,d in zip(Dsd[indx], (D-Dl)[indx], State[indx])]\n",
    "# Labels\n",
    "ax[0].set_xlabel('D_SD', fontsize=15)\n",
    "ax[0].set_ylabel('D_est-D_obs', fontsize=15)\n",
    "\n",
    "# Regression line\n",
    "xnew = np.linspace(min(D)-0.5,max(D)+1,100)\n",
    "ynew = trace_aml.posterior['Intercept'].values.mean()+trace_aml.posterior['A'].values.mean()*xnew\n",
    "ax[1].plot(xnew, ynew, c='dodgerblue',zorder=0)\n",
    "# Size points proportional to D SE\n",
    "pdx = Dsd*70\n",
    "# Plot regressed divorce rate vs marraige age\n",
    "ax[1].scatter(A, D, marker='o', facecolor='white', edgecolors='k', linewidth=1, label='Observed')\n",
    "ax[1].scatter(A, Dl, marker='o', facecolor='black', edgecolors='k', linewidth=1, s=pdx, label='Latent')\n",
    "[ax[1].plot((a,a),(d,dl), c='grey', zorder=0) for a,d,dl in zip(A,D,Dl)]\n",
    "# Label outlying states\n",
    "[ax[1].text(x+oset,y+oset,d) for x,y,d in zip(A[indx], (D)[indx], State[indx])]\n",
    "# Labels\n",
    "ax[1].set_ylabel('Divorce rate (std)', fontsize=15)\n",
    "ax[1].set_xlabel('Median marriage age (std)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('shrinkage.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see from these panels is that the states with the greatest observation error, and are farthest from the regression expectation, tend to have the largest shrinkage toward the regression line. Automagically.\n",
    "\n",
    "Aside from observation errors in divorce rate, the dataset also has observation error for the marriage rates, which is a covariate. No problem! In Bayesland we can incorporate this in a similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'States':State}) as AM_latent2:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Marriage rate\n",
    "    β1 = pm.Normal('M', 0, 0.5)\n",
    "    # Latent marriage rate\n",
    "    Mμ = pm.Normal('Latent_marriage', 0, 1, dims='States')\n",
    "    # Observed marriage rate\n",
    "    Zi = pm.Normal('Zi', Mμ, Msd, observed=M)\n",
    "    # Marriage age\n",
    "    β2 = pm.Normal('A', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = β0+β1*Mμ+β2*A\n",
    "    # Error\n",
    "    σ = pm.Uniform('Sigma', 0, 10)\n",
    "    # Latent state\n",
    "    Dμ = pm.Normal('Latent_divorce', μ, σ, dims='States')\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', Dμ, Dsd, observed=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've got a new hierarhical model for marriage rates nested within our larger model, including a data likelihood for the observed marriage rates ($M$), from which we estimate the latent marriage rates ($M\\mu$) that are subsequently used to estimate $\\mu$ in the model. Let's see how this goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AM_latent2:\n",
    "    trace_aml2 = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent estimates take 2\n",
    "Dl2 = trace_aml2.posterior['Latent_divorce'].stack(sample=(\"chain\", \"draw\")).values.mean(1)\n",
    "# Latent estimates take 2\n",
    "Ml = trace_aml2.posterior['Latent_marriage'].stack(sample=(\"chain\", \"draw\")).values.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot difference between observed mean and latent mean against observed standard error\n",
    "ax[0].scatter(Dsd, D-Dl, marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "ax[0].scatter(Dsd, D-Dl2, marker='o', facecolor='white', edgecolors='red', linewidth=1)\n",
    "ax[0].axhline(0, linestyle='--', c='grey', zorder=0)\n",
    "# Label outlying states\n",
    "indx = abs(D-Dl)>0.5\n",
    "oset=0.03\n",
    "[ax[0].text(x+oset,y+oset,d) for x,y,d in zip(Dsd[indx], (D-Dl)[indx], State[indx])]\n",
    "# Labels\n",
    "ax[0].set_xlabel('D_SD', fontsize=15)\n",
    "ax[0].set_ylabel('D_est-D_obs', fontsize=15)\n",
    "\n",
    "# Plot regressed divorce rate vs marraige age\n",
    "ax[1].scatter(M, D, marker='o', facecolor='white', edgecolors='k', linewidth=1, label='Observed')\n",
    "ax[1].scatter(Ml, Dl2, marker='o', facecolor='red', edgecolors='red', linewidth=1, label='Latent2')\n",
    "ax[1].scatter(M, Dl, marker='o', facecolor='black', edgecolors='k', linewidth=1, s=pdx, label='Latent',zorder=0)\n",
    "[ax[1].plot((m,ml),(d,dl), c='grey', zorder=0) for m,ml,d,dl in zip(M,Ml,D,Dl2)]\n",
    "[ax[1].plot((m,ml),(d,dl), c='grey', linestyle=':', zorder=0) for m,ml,d,dl in zip(M,Ml,Dl,Dl2)]\n",
    "# Label outlying states\n",
    "[ax[1].text(x+oset,y+oset,d) for x,y,d in zip(M[indx], (D)[indx], State[indx])]\n",
    "# Labels\n",
    "ax[1].set_ylabel('Divorce rate (std)', fontsize=15)\n",
    "ax[1].set_xlabel('Marriage rate (std)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('shrinkage2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bit more movement in the latent state estimates over the previous model, with Maine (ME) in particular showing a lot of shrinkage - possibly because it has the highest divorce rate, but a relatively small population, making the esimated marriage rates far less likely given the observation error and the model. But in the left panel we can see how now the latent estimates move toward the estimate regression line, for **both marriage rate and divorce rate**, which we can see looking back at the much larger estimated relationship between M and D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_aml, var_names=['Intercept', 'M', 'A', 'Sigma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_aml2, var_names=['Intercept', 'M', 'A', 'Sigma'])\n",
    "dfi.export(tmp, 'trace_aml2.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "\n",
    "Among the oddest realities of Bayesian statistics for the orthodox student is that the only difference between random variates and data is that the values of data are fixed. Both are conditional on some stochastic node (distribution). Given some mean $\\mu$ and standard deviation $\\sigma$ in a normal distribution for example, random values can be realized from that distribution proportional to their likelihood conditional on $\\mu$ and $\\sigma$. When data is present however, it is $\\mu$ and $\\sigma$ that become, in a sense, random - they require adujstment to maximize the data likelihood conditional on the data. The information flows in the other direction. \n",
    "\n",
    "What's cool about this for missing data is that both processes can happen at once - if you have 1000 observations, 10 of which are missing, $\\mu$ and $\\sigma$ can be estimated by the other 990 (known) observations and then used to generate random values for the 10 missing observations. \n",
    "\n",
    "By way of example, let's take a look at the primates milk data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "mdata = pd.read_csv('milk.csv', sep=';')\n",
    "# Add log(mass) column\n",
    "mdata['log(mass)'] = np.log(mdata.mass.values)\n",
    "dfi.export(mdata.head(), 'milk.jpg')\n",
    "mdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight away we can see there are a lot of missing values in the brain size (`neocortex.perc`) column. But why are they missing? This seemingly unimportant question, has three distinct (and terribly named) possibilites:\n",
    "\n",
    "1. **Missing completely at random (MCAR)** - the probability that any given datapoint is missing is equal over the whole dataset.\n",
    "\n",
    "2. **Missing at random (MAR)** - the probability that any given datapoint is missing is dependent on some other variable that is fully observed\n",
    "\n",
    "3. **Missing not at random (MNAR)** - the probability that any given datapoint is missing is dependent on some other variable that is unobserved\n",
    "\n",
    "We'll build two models to estimate brain size (B), the first being MCAR and the second being MAR, where values for B are dependent on body mass (M). Back with PyMC3 this was done through numpy's `masked_array` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab variables of interest\n",
    "kcal = stdize(mdata['kcal.per.g'])\n",
    "M = stdize(mdata['log(mass)'].values)\n",
    "# Grab B with NaN's\n",
    "B = stdizeNA(mdata['neocortex.perc'].values)\n",
    "# Mask NaN's\n",
    "B_ = np.ma.masked_array(B, mask=np.isnan(B))/100\n",
    "B_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've got here is a multifaceted object: a `data` array, with numbers and dashes, indicating observed values and a spot for the missing values, as well as a `mask` array, of true/false values for if the mask is missing. Lastly there is a `fill_value` attribute, which says what number is a placeholder for the missing values, so that the full array can be held in memory. With PyMC v4.0 however, NAs are handled automagically and the MCAR model is very similar to our observation error model, the difference being that the latent state here is only partially so - where there is data, those values don't change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC Model\n",
    "with pm.Model() as milker:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    β1 = pm.Normal('log(mass)', 0, .5)\n",
    "    β2 = pm.Normal('neocortex_perc', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Missing data\n",
    "    ν = pm.Normal('BS_mean', 0.5, 1)\n",
    "    σν = pm.Exponential('BS_sigma', 1)\n",
    "    Bμ = pm.Normal('Bμ', ν, σν, observed=B)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0+β1*M+β2*Bμ\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kcal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with milker:\n",
    "    trace_m = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so what do we get back - well from the MCAR, we get estimates for the missing brain size values conditional on the model and their group average `BS_mean`. Within the context of the model, this makes for the variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value estimates\n",
    "tmp = trace_m.posterior['Bμ'].stack(sample=(\"chain\", \"draw\")).values\n",
    "Bl = tmp.mean(1)\n",
    "Bll90 = np.percentile(tmp,5,axis=1)\n",
    "Blu90 = np.percentile(tmp,95,axis=1)\n",
    "Bll50 = np.percentile(tmp,25,axis=1)\n",
    "Blu50 = np.percentile(tmp,75,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kcal[B_.mask]),len(Bl[B_.mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot observed values\n",
    "ax[0].scatter(B, kcal, marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "# Plot imputed values\n",
    "ax[0].scatter(Bl[B_.mask], kcal[B_.mask], marker='o', facecolor='black', edgecolors='k', linewidth=1)\n",
    "[ax[0].plot((d,dl),(a,a), c='grey', zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll90,Blu90)]\n",
    "[ax[0].plot((d,dl),(a,a), c='grey', linewidth=3, zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll50,Blu50)]\n",
    "# Labels\n",
    "ax[0].set_xlabel('Neocortex percent (std)', fontsize=15)\n",
    "ax[0].set_ylabel('kcal milk (std)', fontsize=15)\n",
    "\n",
    "\n",
    "# Plot observed values\n",
    "ax[1].scatter(M, B, marker='o', facecolor='white', edgecolors='k', linewidth=1, label='Observed')\n",
    "# Plot imputed values\n",
    "ax[1].scatter(M[B_.mask], Bl[B_.mask], marker='o', facecolor='black', edgecolors='k', linewidth=1, label='Imputed')\n",
    "[ax[1].plot((a,a),(d,dl), c='grey', zorder=0) for a,d,dl in zip(M[B_.mask],Bll90,Blu90)]\n",
    "[ax[1].plot((a,a),(d,dl), c='grey', linewidth=3, zorder=0) for a,d,dl in zip(M[B_.mask],Bll50,Blu50)]\n",
    "# Labels\n",
    "ax[1].set_ylabel('Neocortex percent (std)', fontsize=15)\n",
    "ax[1].set_xlabel('log(Body mass) (std)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('missingmilk.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that the missing values are highly uncertain, but they also tend toward the central relationships present in the observed data. This is excellent stuff. \n",
    "\n",
    "MCAR does not provide information about the missing values other than the relationships already present in the model. In contrast, the MAR model assumes there is a fully-observed covariate that provides information about what the missing values might be. In other words, we're assuming they're correlated in some uknown way. You can probabily guess what happens next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask NaN's\n",
    "X = np.array([B_,M]).T\n",
    "BM = np.ma.masked_array(X, mask=np.isnan(X))\n",
    "BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC3\n",
    "with pm.Model() as milkerMvN:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    β1 = pm.Normal('log(mass)', 0, .5)\n",
    "    β2 = pm.Normal('neocortex_perc', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    ## Missing data at random\n",
    "    # Brain size prior\n",
    "    ν0 = pm.Normal('BS_mean', 0.5, 1)\n",
    "    # Body mass prior\n",
    "    ν1 = pm.Normal('BM_mean', 0.5, 1)\n",
    "    \n",
    "    # Hyperprior for LKJ\n",
    "    sd_dist = pm.HalfCauchy.dist(2)\n",
    "    chol, _, _ = pm.LKJCholeskyCov('chol_cov', eta=2, n=2, sd_dist=sd_dist)\n",
    "    \n",
    "    # MvN\n",
    "    Bμ = pm.MvNormal('Bμ', mu=[ν0, ν1], chol=chol, observed=BM)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0+β1*M+β2*Bμ.T[0]\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kcal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with milkerMvN:\n",
    "    trace_mvn = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value estimates\n",
    "tmp = trace_mvn.posterior['Bμ'].stack(sample=(\"chain\", \"draw\")).values\n",
    "Bl2 = np.percentile(tmp,50,axis=1).mean(1)\n",
    "Bll902 = np.percentile(tmp,5,axis=1).mean(1)\n",
    "Blu902 = np.percentile(tmp,95,axis=1).mean(1)\n",
    "Bll502 = np.percentile(tmp,25,axis=1).mean(1)\n",
    "Blu502 = np.percentile(tmp,75,axis=1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bll902.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot observed values\n",
    "ax[0].scatter(B, kcal, marker='o', facecolor='white', edgecolors='k', linewidth=1)\n",
    "# Plot MCAR imputed values\n",
    "ax[0].scatter(Bl[B_.mask], kcal[B_.mask], marker='o', facecolor='black', edgecolors='k', linewidth=1)\n",
    "[ax[0].plot((d,dl),(a,a), c='grey', zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll90,Blu90)]\n",
    "[ax[0].plot((d,dl),(a,a), c='grey', linewidth=3, zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll50,Blu50)]\n",
    "# Plot MAR imputed values\n",
    "ox = 0.04\n",
    "ax[0].scatter(Bl2[B_.mask], kcal[B_.mask]+ox, marker='o', facecolor='red', linewidth=1)\n",
    "[ax[0].plot((d,dl),(a+ox,a+ox), c='red', alpha=0.7, zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll902[B_.mask],Blu902[B_.mask])]\n",
    "[ax[0].plot((d,dl),(a+ox,a+ox), c='red', alpha=0.7, linewidth=3, zorder=0) for a,d,dl in zip(kcal[B_.mask],Bll502[B_.mask],Blu502[B_.mask])]\n",
    "# Labels\n",
    "ax[0].set_xlabel('Neocortex percent (std)', fontsize=15)\n",
    "ax[0].set_ylabel('kcal milk (std)', fontsize=15)\n",
    "\n",
    "\n",
    "# Plot observed values\n",
    "ax[1].scatter(M, B, marker='o', facecolor='white', edgecolors='k', linewidth=1, label='Observed')\n",
    "# Plot MCAR imputed values\n",
    "ax[1].scatter(M, Bl, marker='o', facecolor='black', edgecolors='k', linewidth=1, label='MCAR')\n",
    "[ax[1].plot((a,a),(d,dl), c='grey', zorder=0) for a,d,dl in zip(M,Bll90,Blu90)]\n",
    "[ax[1].plot((a,a),(d,dl), c='grey', linewidth=3, zorder=0) for a,d,dl in zip(M,Bll50,Blu50)]\n",
    "# Plot MCAR imputed values\n",
    "ox=0.06\n",
    "ax[1].scatter(M[B_.mask]+ox, Bl2[B_.mask], marker='o', facecolor='red',linewidth=1, label='MAR')\n",
    "[ax[1].plot((a+ox,a+ox),(d,dl), c='red', alpha=0.7, zorder=0) for a,d,dl in zip(M[B_.mask],Bll902[B_.mask],Blu902[B_.mask])]\n",
    "[ax[1].plot((a+ox,a+ox),(d,dl), c='red', alpha=0.7, linewidth=3, zorder=0) for a,d,dl in zip(M[B_.mask],Bll502[B_.mask],Blu502[B_.mask])]\n",
    "# Labels\n",
    "ax[1].set_ylabel('Neocortex percent (std)', fontsize=15)\n",
    "ax[1].set_xlabel('log(Body mass) (std)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('missingmilk2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're cooking - the information coming from body mass, which is correlated with brain size (possibly due to some unmeasured variable), shrinks the MvN estimates further toward the relationship between them (away from midline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_m, var_names=['Sigma'])\n",
    "dfi.export(tmp, 'trace_mcar.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_mvn, var_names=['Sigma'])\n",
    "dfi.export(tmp, 'trace_mar.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get that small reduction in data variance to boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretely absent\n",
    "\n",
    "While estimation of continuous variables follows naturally enough, handline discrete missing values poses it's own particular challenges. First and foremost, discrete variables require discrete distributions, which pose real problems when they're anywhere but as part of the data likelihood in a model. HMC just doesn't do discrete, because it's based on momentum from the gradients, and other algorithms have similar problems because of the knife-edge nature of counts. \n",
    "\n",
    "The way around this is through something called a 'weighted average' which involves sampling from discrete posteriors outside of the MCMC bits, avoiding the problem altogether. To see how to do this, we'll put together a really simple model to simulate missingness.\n",
    "\n",
    "## Discrete cats\n",
    "\n",
    "The example is from p517 in Rethinking\n",
    "\n",
    "> Imagine a neighborhood in which every house contains a songbird. Suppose we survey the neighborhood and sample one minute of song from each house, recording the number of notes. You notice that some houses also have house cats, and wonder if the presence of a cat changes the amount that each bird sings. So you try to also figure out which houses have cats. You can do this easily in some cases, either by seeing the cat or by asking a human resident. But in about 20% of houses, you can’t determine whether or not a cat lives there.\n",
    "\n",
    "Helpfully, we can turn this into a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cat DAG\n",
    "mDAG = nx.DiGraph()\n",
    "mDAG.add_edges_from([(\"Rc\", \"C*\"), (\"C\", \"C*\"), (\"C\", \"N\")])\n",
    "# Plot DAG\n",
    "nx.draw_networkx(mDAG, arrows=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the presence of a cat (C) influences the number of sung notes (N) but because of missing values (Rc), we only observe C*. We simulate from a statistical model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N_i \\sim & P(\\lambda_i) \\\\\n",
    "log(\\lambda_i) = & \\beta_0 + \\beta_1C_i \\\\\n",
    "C_i \\sim & Bern(k) \\\\\n",
    "R_{C,i} \\sim & Bern(r)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of houses\n",
    "nhouse = 100\n",
    "# Intercept\n",
    "b0 = 5\n",
    "# Cat effet\n",
    "b1 = -3\n",
    "# Probability of cat in house\n",
    "k = 0.7\n",
    "# Probability of not knowing if there's a cat\n",
    "r = 0.2\n",
    "\n",
    "\n",
    "# Simulate cats\n",
    "cat = np.random.binomial(1,k,nhouse)\n",
    "# Notes counted\n",
    "notes = np.random.poisson(b0+b1*cat)\n",
    "\n",
    "# Unobserved houses\n",
    "Rc = np.random.binomial(1, r, nhouse)\n",
    "# Observed cats\n",
    "cat_obs = cat\n",
    "# Cat mask\n",
    "cat_obs[Rc==1] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these elements in place, we can build a conditional model that removes the uknowns and calculates their likely values later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC3\n",
    "with pm.Model() as cats:\n",
    "    # Probability of cat in the house\n",
    "    k = pm.Beta('k', 2, 2)\n",
    "    Zi = pm.Bernoulli('Zi',k, observed=cat_obs[Rc==0])\n",
    "    \n",
    "    # Average notes absent cat\n",
    "    β0 = pm.Normal('Intercept', 0, 5)\n",
    "    # log-odds effect of cat on notes\n",
    "    β1 = pm.Normal('Cat', 0, 5)\n",
    "\n",
    "    # Linear model\n",
    "    λ = pm.math.exp(β0+β1*cat_obs[Rc==0])\n",
    "    \n",
    "    # Known cat data likelihood\n",
    "    Yi = pm.Poisson('Yi', λ, observed=notes[Rc==0])\n",
    "    \n",
    "    ## Custom addition to the likelihood\n",
    "    δ = pm.math.log( pm.math.exp(pm.math.log(k)+pm.Poisson.logp(notes[Rc==1],pm.math.exp(β0+β1))+pm.math.log(1-k)+pm.Poisson.logp(notes[Rc==1],pm.math.exp(β0))).sum() )\n",
    "    like = pm.Potential('like', δ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cats:\n",
    "    trace_c = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_c)\n",
    "dfi.export(tmp, 'catspost.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila - our estimates include the true probabilty of cat presence (0.7)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
