{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Lecture 2 - Hierarchical Models II\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=n2aJYtuGu54&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=14\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/pymc-devs/resources/tree/master/Rethinking\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import arviz as az\n",
    "from matplotlib import pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical models\n",
    "\n",
    "We're going to visit some other features of hierarhical models by looking again at the chimps data for prosocial behaviour, back to Week 6.\n",
    "\n",
    "Let's import that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = pd.read_csv('chimpanzees.csv', sep=\";\")\n",
    "# Actor index\n",
    "Ia = cdata.actor.values-1\n",
    "# Treatment variable\n",
    "cdata['treatment'] = 1 + cdata.prosoc_left + 2*cdata.condition\n",
    "# Grab data\n",
    "# Left pull - response\n",
    "L = cdata.pulled_left.values\n",
    "# Individual chimps\n",
    "Actor,Ia = indexall(cdata.actor.values)\n",
    "Chimp = ['Chimp '+str(a) for a in Actor]\n",
    "nchimps = len(Actor)\n",
    "# Treatment\n",
    "Treatment,It = indexall(cdata.treatment.values)\n",
    "Treatment = ['R/N','L/N','R/P','L/P']\n",
    "ntreat = len(Treatment)\n",
    "# Block\n",
    "Block,Ib = indexall(cdata.block.values.astype(str))\n",
    "nblock = len(Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly for this example, we have, in addition to the actors (chimps) and treatments, an additional covariate `block` that represents observations from the same day. This is a kind of **nussiance parameter** something we're not actually interested in but should account for as perhaps there was that Wednesday they played sad music all day in the chimp prison. We can accomodate this new additional factor using it's own *random effect*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(cdata.head(), 'cdata.jpg')\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Chimps model\n",
    "with pm.Model(coords={'Chimp':Chimp, 'Treat': Treatment}) as Chimps:\n",
    "    # Average chimp\n",
    "    γ0 = pm.Normal('Actor_mean', 0, 1.5)\n",
    "    σ_γ = pm.Exponential('sigma_actor', 1.)\n",
    "    \n",
    "    # Individual intercepts\n",
    "    β0 = pm.Normal('Actor', γ0, σ_γ, dims='Chimp')\n",
    "    \n",
    "    # Treatment effects\n",
    "    β1 = pm.Normal('Treatment', 0, 0.5, dims='Treat')\n",
    "\n",
    "    # Linear model\n",
    "    p = pm.invlogit(β0[Ia]+β1[It])\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Binomial('Yi', 1, p,observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Chimps model with random effects for block\n",
    "with pm.Model(coords={'Chimp':Chimp, 'Treat': Treatment, 'Block_':Block}) as ChimpsB:\n",
    "    # Average chimp\n",
    "    γ0 = pm.Normal('Actor_mean', 0, 1.5)\n",
    "    σ_γ = pm.Exponential('sigma_actor', 1.)\n",
    "    \n",
    "    # Individual intercepts\n",
    "    β0 = pm.Normal('Actor', γ0, σ_γ, dims='Chimp')\n",
    "    \n",
    "    # Treatment effects\n",
    "    β1 = pm.Normal('Treatment', 0, 0.5, dims='Treat')\n",
    "    \n",
    "    # Block effects\n",
    "    σ_β2 = pm.Exponential('sigma_block', 1.)\n",
    "    β2 = pm.Normal('Block', 0, σ_β2, dims='Block_')\n",
    "\n",
    "    # Linear model\n",
    "    p = pm.invlogit(β0[Ia]+β1[It]+β2[Ib])\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Binomial('Yi', 1, p,observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Chimps:\n",
    "    trace_c = pm.sample(1000)\n",
    "with ChimpsB:\n",
    "    trace_b = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these models in hand, we can take a look at plots of the posterior effects to see how they compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest([trace_c, trace_b], model_names=[\"No Block\", \"Block\"], var_names=['Actor','Treatment','Block'], figsize=(9,9))\n",
    "plt.axvline(0,linestyle=\":\",c='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('chimpforest.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "az.plot_kde(trace_b.posterior['Actor'].values, ax=ax)\n",
    "az.plot_kde(trace_b.posterior['Block'].values, ax=ax, plot_kwargs={'color':'C1'})\n",
    "ax.text(2, 0.75, \"actor\", color='C0')\n",
    "ax.text(0.5, 2, \"block\", color='C1')\n",
    "ax.set_xlabel('sigma')\n",
    "ax.set_ylabel('density')\n",
    "ax.set_xlim(-0.1, 4.1)\n",
    "ax.set_ylim(-0.05, 3.5)\n",
    "plt.savefig('chimpvar.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's clear from the forrest plot and the density plot above is that the inter-actor effects are way more influential than any day to day variation. Looking at WAIC suggestst they're roughly equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Chimps:\n",
    "    pm.compute_log_likelihood(trace_c)\n",
    "with ChimpsB:\n",
    "    pm.compute_log_likelihood(trace_b)\n",
    "comp_df = az.compare({'No Block':trace_c, 'Block':trace_b}, ic='waic', scale='deviance')\n",
    "dfi.export(comp_df.style.background_gradient(), 'chimpwaic.jpg')\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what should we do? Nothing - in fitting both models we've learned something about the data, that block has almost no effect and the block parameters being near zero tell us why. Each model makes nearly identical out of sample predictions. While model selection has value in looking at the conditional independices of different causal (or mechanistic) models, these are experiments so there's nothing to select.\n",
    "\n",
    "\n",
    "While the addition of block represents one kind of additional factor, we can go all in and at the partial pooling hierhiarcy so that we can estimate partial pooling effects of actor and treatment as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Chimp':Chimp, 'Treat': Treatment, 'Block_':Block}) as ChimpsT:\n",
    "    # Average chimp\n",
    "    γ0 = pm.Normal('Actor_mean', 0, 1.5)\n",
    "    σ_γ = pm.Exponential('sigma_actor', 1.)\n",
    "    # Individual intercepts\n",
    "    β0 = pm.Normal('Actor', γ0, σ_γ, dims='Chimp')\n",
    "    \n",
    "    # Treatment effects\n",
    "    σ_β1 = pm.Exponential('sigma_treat', 1.)\n",
    "    β1 = pm.Normal('Treatment', 0, σ_β1, dims='Treat')\n",
    "    \n",
    "    # Block effects\n",
    "    σ_β2 = pm.Exponential('sigma_block', 1.)\n",
    "    β2 = pm.Normal('Block', 0, σ_β2, dims='Block_')\n",
    "\n",
    "    # Linear model\n",
    "    p = pm.invlogit(β0[Ia]+β1[It]+β2[Ib])\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Binomial('Yi', 1, p,observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ChimpsT:\n",
    "    trace_t = pm.sample(1000)\n",
    "with ChimpsT:\n",
    "    pm.compute_log_likelihood(trace_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest([trace_c, trace_b, trace_t], model_names=[\"No Block\", \"Block\", \"Full\"], var_names=['Actor','Treatment','Block'], figsize=(9,9))\n",
    "plt.axvline(0,linestyle=\":\",c='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig('chimpforest2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These resuls are very similar but now we have estimates of how variable things are among actors, treatments, and blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "az.plot_kde(trace_t.posterior['sigma_actor'].values, ax=ax)\n",
    "az.plot_kde(trace_t.posterior['sigma_treat'].values, ax=ax, plot_kwargs={'color':'C2'})\n",
    "az.plot_kde(trace_t.posterior['sigma_block'].values, ax=ax, plot_kwargs={'color':'C1'})\n",
    "ax.text(2, 0.75, \"actor\", color='C0')\n",
    "ax.text(0.75, 1.3, \"treatment\", color='C2')\n",
    "ax.text(0.5, 2, \"block\", color='C1')\n",
    "ax.set_xlabel('sigma')\n",
    "ax.set_ylabel('density')\n",
    "ax.set_xlim(-0.1, 4.1)\n",
    "ax.set_ylim(-0.05, 3.5)\n",
    "plt.savefig('chimpvar2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = az.compare({'No Block':trace_c, 'Block':trace_b, 'FullH':trace_t}, ic='waic', scale='deviance')\n",
    "dfi.export(comp_df.style.background_gradient(), 'chimpwaic2.jpg')\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergent transitions\n",
    "\n",
    "Common particularly in hierarhical models, divergent transitions mean our HMC algorithm isn't exploring the parameter space properly. The details (and what to do about them) were worked out by [Michael Betancourt and Mark Girolami](https://arxiv.org/pdf/1312.0906). To understand what's happening let's take a look at a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as devil:\n",
    "    v = pm.Normal('v',0,3)\n",
    "    x = pm.Normal('x',0,pm.math.exp(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with devil:\n",
    "    trace_d = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here, well as written, at very low values of $v$ the distribution of x contracts around zero (the steep slopes in the likelihood surface) and this creates a specific problem: steep surfaces are hard to simulate using the discrete, leapfrog steps inherent in our HMC algorithm (remember back to week 5, lecture 2 where we outlined the number of leapfrog steps and the step sizes). What happens is that if the steps are too big (and they will be in the narrow funnel area, but not elsewhere) the simulation will somtimes fly wildly far off at one of the steps and the total energy at the start and end of the HMC step are not equal. In other words, a divergent transition. \n",
    "\n",
    "<img src=\"funnell.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "So what can we do? Well what's crazy is that we can make a small multiplicative adjustment to move our $v$ parameter out from the definition of $x$, using a standard normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as devilC:\n",
    "    v = pm.Normal('v',0,3)\n",
    "    z = pm.Normal('z',0,1)\n",
    "    x = z*pm.math.exp(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with devilC:\n",
    "    trace_c = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila - divergences solved. \n",
    "\n",
    "\n",
    "<img src=\"zfunnel.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "But what the heck just happened? Well, much like the z-scoring that we've been using throughout the course, in this new **non-centred** parameterization we're now sampling from the z-score $N(0,1)$ scale, rather than the $N(0,e^{v})$ scale, and then just reversing the z-scoring to get x. Recall\n",
    "\n",
    "$$\n",
    "z = \\frac{x-\\bar{x}}{SD(x)}\n",
    "$$\n",
    "\n",
    "so to solve for x:\n",
    "\n",
    "\n",
    "$$\n",
    "zSD(x) = x-\\bar{x} = x-0 = x\n",
    "$$\n",
    "\n",
    "If the mean wasn't 0 for x we could add that in here too. In essence, by z-scoring we have fattened out the funnel so that the algorithm keeps sampling properly. \n",
    "\n",
    "Let's apply this lesson to the Chimps example above and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Chimp':Chimp, 'Treat': Treatment, 'Block_':Block}) as ChimpsNC:\n",
    "    # Average chimp\n",
    "    γ0 = pm.Normal('Actor_mean', 0, 1.5)\n",
    "    σ_γ = pm.Exponential('sigma_actor', 1.)\n",
    "    za = pm.Normal('z_actor',0,1, dims='Chimp')\n",
    "    \n",
    "    # Old parameterization\n",
    "    #β0 = pm.Normal('Actor', γ0, σ_γ, dims='Chimp')\n",
    "    # Individual intercepts - non-centred\n",
    "    β0 = pm.Deterministic('Actor', γ0+za*σ_γ, dims='Chimp')\n",
    "    \n",
    "    # Treatment effects\n",
    "    β1 = pm.Normal('Treatment', 0, 0.5, dims='Treat')\n",
    "    \n",
    "    # Block effects - non-centred\n",
    "    σ_β2 = pm.Exponential('sigma_block', 1.)\n",
    "    zb = pm.Normal('z_block',0, 1, dims='Block_')\n",
    "    β2 = pm.Deterministic('Block', zb*σ_β2, dims='Block_')\n",
    "\n",
    "    # Linear model\n",
    "    p = pm.invlogit(β0[Ia]+β1[It]+β2[Ib])\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Binomial('Yi', 1, p,observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ChimpsNC:\n",
    "    trace_nc = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_divergence(trace, basevar, targetvar, ax=None, divergence=True, color='C3', divergence_color='C2'):\n",
    "    #theta = trace.get_values(varname=basevar, combine=True)[:, 0]\n",
    "    theta = trace.posterior[basevar].values.flatten()\n",
    "    logtau = trace.posterior[targetvar].values.flatten()\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(theta, logtau, 'o', color=color, alpha=.5)\n",
    "    if divergence:\n",
    "        divergent = trace.sample_stats.diverging.values.flatten()\n",
    "        ax.plot(theta[divergent], logtau[divergent], 'o', color=divergence_color)\n",
    "    ax.set_xlabel(basevar)\n",
    "    ax.set_ylabel(targetvar)\n",
    "    ax.set_title('scatter plot between log('+targetvar+') and '+basevar);\n",
    "    return ax\n",
    "\n",
    "# A small wrapper function for displaying the MCMC sampler diagnostics as above\n",
    "def report_trace(trace,basevar,targetvar,logscale=False):\n",
    "    # plot the trace of log(tau)\n",
    "    pm.plot_trace({targetvar: trace.posterior[targetvar].values.flatten()});\n",
    "\n",
    "    # plot the estimate for the mean of log(τ) cumulating mean\n",
    "    if logscale:\n",
    "        logtau = np.log(trace.posterior[targetvar].values.flatten())\n",
    "    else:\n",
    "        logtau = trace.posterior[targetvar].values.flatten()\n",
    "    mlogtau = [np.mean(logtau[:i]) for i in np.arange(1, len(logtau))]\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    #plt.axhline(0.7657852, lw=2.5, color='gray')\n",
    "    plt.plot(mlogtau, lw=2.5)\n",
    "    plt.ylim(0, 2)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('MCMC mean of log('+targetvar+')')\n",
    "    plt.title('MCMC estimation of log('+targetvar+')')\n",
    "    plt.show()\n",
    "\n",
    "    # display the total number and percentage of divergent\n",
    "    divergent = trace.sample_stats.diverging.values.flatten()\n",
    "    print('Number of Divergent %d' % divergent.nonzero()[0].size)\n",
    "    divperc = divergent.nonzero()[0].size / len(trace) * 100\n",
    "    print('Percentage of Divergent %.1f' % divperc)\n",
    "\n",
    "    # scatter plot between log(tau) and theta[0]\n",
    "    # for the identifcation of the problematic neighborhoods in parameter space\n",
    "    pairplot_divergence(trace,basevar,targetvar);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_trace(trace_b,'Actor_mean','sigma_block')\n",
    "plt.savefig('ppd1.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of these figure functions aren't so important but you can see in the plot above that the green dots (the divergences) are clustered along the bottom, showing that they happen when values of `sigma_block` are small - classic divergence. Somtetimes divergences just happen too, so having a look is helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_trace(trace_nc,'Actor_mean','sigma_block')\n",
    "plt.savefig('ppd2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As here they're not concentrated anywhere in particular, so are less of a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
