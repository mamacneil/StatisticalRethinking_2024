{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7 Lecture 3 - Ordered logits\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=p7g-CgGCS34\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/statrethinking_winter2019\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "Dustin Stansbury has some lovely PyMC Code available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import patsy\n",
    "import arviz as az\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "import pytensor as pyt\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered categorical variables\n",
    "\n",
    "Among the most initially challenging kinds of models to get your head around are those for ordered categorical variables - both the response and covariates can be this way. Things are scaled in order, but the divisions between them may or may not be additive. Wacky stuff.\n",
    "\n",
    "To get into this, we're going to have to cover a set of experiments used by moral philosophers to evaluate a set of unconcious principles that explain why people vary in judgements (relevant for things like juries):\n",
    "\n",
    "1. **The action principle** - harm caused by action is morally worse than the same harm caused by omission\n",
    "2. **The intention principle** - harm indended as the means to an end is morally worse than the same harm forseen as a side effect of a end\n",
    "3. **The contact principle** - harm caused by physical contact is morally worse than the same harm without physical contact\n",
    "\n",
    "These things may seem abstract, but think about something like the US drone program for item (3), where families of terrorists are killed without physical contact. Would it be worse if individual soldiers went into a targeted house and killed everyone? Philosophy has real-world consequences.\n",
    "\n",
    "Experiments are designed as stories with one or more of these principles and we'll look at data comparing two stories, from Dennis and Evan.\n",
    "\n",
    "Dennis' story:\n",
    "\n",
    "> Standing by the railroad tracks, Dennis sees an empty, out-of-control boxcar about to hit five people. Next to Dennis is a lever that can be pulled, sending the boxcar down a side track and away from the five people. But pulling the lever will also lower the railing on a footbridge spanning the side track, causing one person to fall off the footbridge and onto the side track, where he will be hit by the boxcar. If Dennis pulls the lever the boxcar will switch tracks and not hit the five people, and the one person to fall and be hit by the boxcar. If Dennis does not pull the lever the boxcar will continue down the tracks and hit five people, and the one person will remain safe above the side track.\n",
    "\n",
    "\n",
    "Evan's story:\n",
    "\n",
    "> Standing by the railroad tracks, Evan sees an empty, out-of-control boxcar about to hit five people. Next to Evan is a lever that can be pulled, lowering the railing on a footbridge that spans the main track, and causing one person to fall off the footbridge and onto the main track, where he will be hit by the boxcar. The boxcar will slow down because of the one person, therefore preventing the five from being hit. If Evan pulls the lever the one person will fall and be hit by the boxcar, and therefore the boxcar will slow down and not hit the five people. If Evan does not pull the lever the boxcar will continue down the tracks and hit the five people, and the one person will remain safe above the main track.\n",
    "\n",
    "Reading through these, you may have a different sense of what you would do in each case, or not, depending on how the action principle (both stories) and intention principle (Evan's story) weigh on your concience. \n",
    "\n",
    "We have data relating to these stories, so let's load it in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import trolley data\n",
    "tdata = pd.read_csv('Trolley.csv',sep=';')\n",
    "dfi.export(tdata.head(), 'tdata.jpg')\n",
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data include responses from 331 individuals (`id`) to various trolley problems (`case`). We're interested in their `response`, which is an ordered integer from 1 (never permissible) to 7 (always permissible) in reference to if the actor in the story should intervene or not.\n",
    "\n",
    "A first question is how should we even display such data? Here are three alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(17, 6))\n",
    "\n",
    "# Pandas plotting goodness for a histogram\n",
    "tdata.response.value_counts().sort_index().plot(kind='bar',ax=ax0)\n",
    "ax0.set_xlabel(\"\", fontsize=15);\n",
    "ax0.set_ylabel(\"Frequency\", fontsize=15);\n",
    "\n",
    "# Pandas plotting goodness for a cumulative proportion\n",
    "tdata.response.value_counts().sort_index().cumsum().div(tdata.shape[0]).plot(marker='o',ax=ax1)\n",
    "ax1.set_xlim(0.9, 7.1);\n",
    "ax1.set_xlabel(\"Response\", fontsize=15)\n",
    "ax1.set_ylabel(\"Cumulative proportion\", fontsize=15)\n",
    "\n",
    "# Pandas plotting goodness for log-odds\n",
    "tdata.response.value_counts().sort_index().cumsum().iloc[:-1].div(tdata.shape[0]).apply(lambda p:np.log(p/(1.-p))).plot(marker='o',ax=ax2)\n",
    "ax2.set_xlim(0.9, 7);\n",
    "ax2.set_xlabel(\"\", fontsize=15)\n",
    "ax2.set_ylabel(\"log(Cumulative-odds)\", fontsize=15)\n",
    "plt.savefig('cum-odds.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From left to right we have: a histogram of the responses, the cumulative proportion of responses from each category, and the log of the cumulative odds of each category. I'll walk you through the panels from left to right, using pandas notation. The first quantity is the count of each response category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts per response\n",
    "tdata.response.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas indexing is on the left, keeping track of which response category these totals represent. Next we want to order them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered counts per response\n",
    "tdata.response.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the left hand panel of the figure above. Next we want to calculate the cumulative proportions for each response - this is the ordering that we're concerned about. The first step there is to calculate the cumulative sum of each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum of responses (in order)\n",
    "tdata.response.value_counts().sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then calculate the proportion represented by each sum by dividing by the total number of responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative proportion of responses (in order)\n",
    "tdata.response.value_counts().sort_index().cumsum().div(tdata.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the middle panel of the figure above. Lastly we want to plot the log(cumulative-odds) of each category. Why? Well because it is the cumulative analogue of the log-odds we used in the binomial model. Remember that the logit is the log-odds and so the cumulative logit is the log(cumulative-odds); these transformations are what constrain our model to the probability scale when it comes to the response.\n",
    "\n",
    "To go from cumulative proportions to cumulative logit, we need the cumulative proportion for the first six responses - remember, these all have to sum to one so we get one of the ordered responses for free (typically the last is omitted). So first we can get the cumulative proportions for the first six categories by using the `.iloc` operator to remove the last category (`-1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative proportion of first six responses (in order)\n",
    "tdata.response.value_counts().sort_index().cumsum().iloc[:-1].div(tdata.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is calculate the log of the cumulative odds. Recall the equation for the logit:\n",
    "\n",
    "$$\n",
    "logit(p) = log(\\frac{p}{1-p})\n",
    "$$\n",
    "\n",
    "The equation for the cumulative logit is exactly this, but where $p=P(y_i<k)$\n",
    "\n",
    "$$\n",
    "logit(P(y_i<k)) = log(\\frac{P(y_i<k)}{1-P(y_i<k)})\n",
    "$$\n",
    "\n",
    "We can use the `.apply` function in pandas to apply this logit to the cumulative probabilites we calculated in the step above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata.response.value_counts().sort_index().cumsum().iloc[:-1].div(tdata.shape[0]).apply(lambda p:np.log(p/(1.-p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get by doing this? Well we've now reversed the cumulative probabilities onto the log-odds scale and - this is the miracle here - in doing so we can get the probabilty of each category by subtracting the cumulative probabilty in the previous category. Incidentally the genius behind [this idea](http://www.stat.uchicago.edu/~pmcc/pubs/paper2.pdf) is [Peter McCullagh](http://www.stat.uchicago.edu/~pmcc/) - who came up with it when he was 28...\n",
    "\n",
    "To further see this, we can plot how substracting these cumulative probabilies stack up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cumulative proportions\n",
    "tmp = tdata.response.value_counts().sort_index().cumsum().div(tdata.shape[0])\n",
    "# Grab cumulative proportion values\n",
    "cumsum = tmp.values\n",
    "ncat = len(cumsum)\n",
    "# Pandas plotting goodness\n",
    "tmp.plot(marker='o', label='P(y<=k)')\n",
    "# Draw horizontal lines at cumulative proportions\n",
    "[plt.axhline(p,c='grey',alpha=0.5) for p in cumsum]\n",
    "# Draw vertical lines from previous probability to current probabilty\n",
    "[plt.plot((i+1,i+1),(cumsum[i-1],cumsum[i]), c='red', zorder=0) for i in range(1,ncat)]\n",
    "# Draw first line\n",
    "plt.plot((1,1), (0,cumsum[0]), c='red', zorder=0, label='P(y=k)')\n",
    "# Label length of red lines\n",
    "[plt.text(i+1.1, cumsum[i-1]+0.03, str(round(cumsum[i]-cumsum[i-1],2)), c='red') for i in range(1,ncat)]\n",
    "# Label length of first red line\n",
    "plt.text(1.1, 0.03, str(round(cumsum[0],2)), c='red')\n",
    "plt.legend()\n",
    "plt.xlim(0.9, 7.1)\n",
    "plt.ylim(0., 1.01)\n",
    "plt.xlabel(\"Response\", fontsize=15)\n",
    "plt.ylabel(\"Cumulative proportion\", fontsize=15)\n",
    "plt.savefig('cum-props.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the cumulative logit is hopefully clear, we can develop a statistical model to estimate these ordered probabilities. While PyMC3 and Stan both have Ordered-logit distributions available, it's important to recognize what's going on under the hood. What's happening is really this\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R_i \\sim & Cat(p)\\\\\n",
    "p_1 = & q_1 \\\\\n",
    "p_k = & q_k-q_{k-1} \\text{   for K>k>1} \\\\\n",
    "p_K = & 1-q_{k-1} \\\\\n",
    "logit(q_k) = & \\kappa_k - \\phi_i \\\\\n",
    "\\phi_i = & \\text{  linear model} \\\\\n",
    "\\kappa_k \\sim & N(0, 1.5)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What this really says is that the probabilities for each category ($p_k$) come via subtraction from the cumulative probabilities ($q_k$), and that the link to our linear model is just the logit for that cumulative probability. Interestingly each category has it's own itercept, $\\kappa_k$ *from which* we subtract the rest of our linear model. Why by subtraction? Because adjustments made by the linear model should occur relative to the maximum - what is referred to as the **cutpoint**, which is represented by the $\\kappa_k$'s. So **if we decrease the log(cumulative-odds) of every outcome k below the maximum, this shifts probability toward higher outcome values**. What this will do is give us higher values when the parameters in $\\phi_i$ are positive. Re-read that bold sentence a few times until it makes sense. It's a pivotal idea.\n",
    "\n",
    "Ok, much easier than doing all this by hand is to use the Ordered-logit in PyMC3, which for a model with no covariates is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses (with -1 because Python)\n",
    "R = tdata.response.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique classes\n",
    "nK = len(np.unique(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDS = {'responses':np.unique(R).astype(str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=COORDS) as OLo:\n",
    "    # Cutpoints - note the use of transform here, and the need to initialize with some ordered values\n",
    "    κ = pm.Normal('Cutpoint', 0, 1.5, \n",
    "                  transform=pm.distributions.transforms.ordered,\n",
    "                  initval=np.arange(nK)-2.5,\n",
    "                  dims='responses')\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.OrderedLogistic('Yi', 0, κ, observed=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with OLo:\n",
    "    trace_o = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_o)\n",
    "dfi.export(tmp.head(), 'cutpoints.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voila! We have cutpoints! So what does this mean really? Well to have a look at these we need to convert these back to the cumulative probability scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invlogit(x):\n",
    "    return np.exp(x)/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_prob = invlogit(trace_o.posterior['Cutpoint'].mean(('draw','chain')))\n",
    "cum_prob_l95 = invlogit(np.quantile(trace_o.posterior['Cutpoint'], 0.025, axis=0))\n",
    "cum_prob_u95 = invlogit(np.quantile(trace_o.posterior['Cutpoint'], 0.975, axis=0))\n",
    "cum_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same as the cumulative proportions we calcualted from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cumulative proportions\n",
    "tmp = tdata.response.value_counts().sort_index().cumsum().div(tdata.shape[0])\n",
    "# Grab cumulative proportion values\n",
    "cumsum = tmp.values\n",
    "ncat = len(cumsum)\n",
    "# Pandas plotting goodness\n",
    "tmp.plot(marker='o', label='P(y<=k)')\n",
    "# Draw 80% HPD\n",
    "[plt.plot((i+1.1,i+1.1),(cum_prob_l95[i],cum_prob_u95[i]), c='black', zorder=0) for i in range(ncat-1)]\n",
    "# Draw horizontal lines at cumulative proportions\n",
    "[plt.axhline(p,c='grey',alpha=0.5) for p in cumsum]\n",
    "# Draw vertical lines from previous probability to current probabilty\n",
    "[plt.plot((i+1,i+1),(cumsum[i-1],cumsum[i]), c='red', zorder=0) for i in range(1,ncat)]\n",
    "# Draw first line\n",
    "plt.plot((1,1), (0,cumsum[0]), c='red', zorder=0, label='P(y=k)')\n",
    "# Label length of red lines\n",
    "[plt.text(i+1.1, cumsum[i-1]+0.03, str(round(cumsum[i]-cumsum[i-1],2)), c='red') for i in range(1,ncat)]\n",
    "# Label length of first red line\n",
    "plt.text(1.1, 0.03, str(round(cumsum[0],2)), c='red')\n",
    "plt.legend()\n",
    "plt.xlim(0.9, 7.1)\n",
    "plt.ylim(0., 1.01)\n",
    "plt.xlabel(\"Response\", fontsize=15)\n",
    "plt.ylabel(\"Cumulative proportion\", fontsize=15)\n",
    "plt.savefig('cum-props_post.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but now with associated uncertainties around the probabilities. Incidentally this gives some pseudo-statsitical people the shits, I once had a haughty reviewer state '*you can't have uncertainty about probabilities because they're probabilities*'. They didn't win the day.\n",
    "\n",
    "With this in hand, we can - by subtraction - get the estimated probabilities for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_prob = np.array([cum_prob[0]]+[cum_prob[i]-cum_prob[i-1] for i in range(1,ncat-1)]+[1-cum_prob[-1]])\n",
    "k_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is all fine and dandy, but how can we add covariates to these kinds of models, to make inferences about things we really care about? In the case of these trolley experiments, there are stories and response scores, and each story contains one or two of the three priciples: Action, Intention, and Contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw the covariate model before in Bayes notation, now in PyMC form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theano wrapper for later use in plotting - transforms variables into Theano tensor objects/\n",
    "from pytensor import shared\n",
    "\n",
    "# Covariates\n",
    "C = shared(tdata.contact.values)\n",
    "I = shared(tdata.intention.values)\n",
    "A = shared(tdata.action.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as OLo_c:\n",
    "    # Cutpoints - note the use of transform here, and the need to initialize with some ordered values\n",
    "    κ = pm.Normal('Cutpoint', 0, 1.5, \n",
    "                  transform=pm.distributions.transforms.ordered, \n",
    "                  shape=nK-1, \n",
    "                  initval=np.arange(nK-1)-2.5)\n",
    "    \n",
    "    # Elements present in story\n",
    "    βC = pm.Normal('C', 0, 1.5)\n",
    "    βA = pm.Normal('A', 0, 1.5)\n",
    "    βI = pm.Normal('I', 0, 1.5)\n",
    "    βIC = pm.Normal('IC', 0, 1.5)\n",
    "    βIA = pm.Normal('IA', 0, 1.5)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = βC*C+βA*A+βI*I+βIC*I*C+βIA*I*A\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.OrderedLogistic('Yi', μ, κ, observed=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with OLo_c:\n",
    "    trace_c = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot covariate effects\n",
    "pm.plot_forest(trace_c,var_names=['C','I','A','IC','IA'])\n",
    "plt.axvline(0)\n",
    "plt.savefig('cov_post.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are all negative - so less permissible. We **subtract** from the cutpoints, so the more negative things are the more they shift density into lower categories. To see this we can look at the effect of the biggest covariate, `IC`, on the transformed probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = trace_c.posterior\n",
    "cum_prob_C = invlogit(TP['Cutpoint'].mean(('chain','draw'))-(TP['C'].mean(('chain','draw'))))\n",
    "cum_prob_IC = (invlogit(TP['Cutpoint'].mean(('chain','draw'))-(TP['I'].mean(('chain','draw'))\n",
    "                +TP['C'].mean(('chain','draw'))+TP['IC'].mean(('chain','draw')))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab cumulative proportion values\n",
    "tmp2 = pd.Series(np.append(cum_prob_C,1))\n",
    "tmp = pd.Series(np.append(cum_prob_IC,1))\n",
    "cumsum = cum_prob_IC\n",
    "ncat = len(cumsum)\n",
    "# Pandas plotting goodness\n",
    "tmp.plot(marker='o', label='P(y<=k) IC')\n",
    "tmp2.plot(marker='o', label='P(y<=k) C')\n",
    "# Draw horizontal lines at cumulative proportions\n",
    "[plt.axhline(p,c='grey',alpha=0.5) for p in cumsum]\n",
    "# Draw vertical lines from previous probability to current probabilty\n",
    "[plt.plot((i,i),(cumsum[i-1],cumsum[i]), c='red', zorder=0) for i in range(1,ncat)]\n",
    "# Draw first line\n",
    "plt.plot((0.01,0.01), (0,cumsum[0]), c='red', zorder=0, label='P(y=k)')\n",
    "# Draw last line\n",
    "plt.plot((ncat,ncat), (cumsum[ncat-1],1), c='red', zorder=0)\n",
    "# Label length of red lines\n",
    "[plt.text(i, cumsum[i-1]+0.03, str(np.round(cumsum[2]-cumsum[2-1],2).values), c='red') for i in range(1,ncat)]\n",
    "plt.text(ncat, cumsum[ncat-1]+0.03, str(np.round(1-cumsum[ncat-1],2).values), c='red')\n",
    "# Label length of first red line\n",
    "plt.text(.1, 0.03, str(np.round(cumsum[0],2).values), c='red')\n",
    "plt.legend()\n",
    "plt.xlim(0, 8)\n",
    "plt.ylim(0., 1.01)\n",
    "plt.xlabel(\"Response\", fontsize=15)\n",
    "plt.ylabel(\"Cumulative proportion\", fontsize=15)\n",
    "plt.savefig('covar_sum.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I had to-reinitialize the jupyter notebook at this stage to run this model - I have no idea why but will try to sort it out and re-post.\n",
    "\n",
    "# Ordered categorical covariates\n",
    "\n",
    "While ordered categories can be handled on the response side, they can also occur in potential covariates. In the trolley data, we see there is an ordered list of educational attainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the `edu` column. Looking at the column we can see the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata.edu.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which need to be ordered in some way. We can do this using list indexing in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education order\n",
    "Edu = ['Elementary School','Middle School','Some High School','High School Graduate',\n",
    "       'Some College',\"Bachelor's Degree\",\"Master's Degree\",'Graduate Degree']\n",
    "# Number of education categories\n",
    "nedu = len(Edu)\n",
    "# Create new column with integer order\n",
    "tdata['edu_score'] = [Edu.index(e) for e in tdata.edu.values]\n",
    "# Take a look\n",
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `edu_score` variable is now in order, and what we want to estimate is the incremental effect of each step in the education ladder on the morality of the various stories. The way to do this is to make additive effects building on previous education effects. So with elementary corresponding to the intercept, we can show for the next three levels of education:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_1 = & \\delta_1 \\\\\n",
    "\\phi_2 = & \\delta_1 + \\delta_2\\\\\n",
    "\\phi_3 = & \\delta_1 + \\delta_2 + \\delta_3\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The shorthand notation for this is\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum^{7}_{j=1}\\delta_j\n",
    "$$\n",
    "\n",
    "Now the real trick in all this is to have these $\\delta$ parameters be some fraction of a maximum education effect ($\\beta_E$), which we can write as\n",
    "\n",
    "$$\n",
    "\\phi_i = \\beta_E\\sum^{E_i-1}_{j=0}\\delta_j\n",
    "$$\n",
    "\n",
    "Which with $j=0$ makes the first category $\\beta_E \\delta_0=0$. The last bit here is to define priors, which is the easiest part: we get to use a Dirichlet, which is a multivariate extension of the Beta distribtuion. The Beta distribution is a distribution for two probabilities that sum to one and the Dirichlet is the same, but generalized to $n$ probabilities.  So to write out our whole model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R_i = & OrderedLogit(\\phi_i, \\kappa_k) \\\\\n",
    "\\phi_i = & \\beta_E\\sum^{E_i-1}_{j=0}\\delta_j + \\beta_A A + \\beta_I I + \\beta_C C\\\\\n",
    "\\kappa_k \\sim & N(0, 1.5)\\\\\n",
    "\\beta_A,\\beta_I, \\beta_C, \\beta_E \\sim & N(0, 1)\\\\\n",
    "\\delta \\sim & Dirichlet(\\alpha) \\\\\n",
    "\\alpha = & [2,2,2,...,2]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And we can implement all this in PyMC as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education attainment\n",
    "E = shared(tdata.edu_score.values)\n",
    "nedu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edu[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'EDU':Edu}) as OLo_oc:\n",
    "    # Cutpoints - note the use of transform here, and the need to initialize with some ordered values\n",
    "    κ = pm.Normal('Cutpoint', 0, 1.5, \n",
    "                  transform=pm.distributions.transforms.ordered, \n",
    "                  shape=nK-1, \n",
    "                  initval=np.arange(nK-1)-2.5)\n",
    "\n",
    "    # Education mixture - weak flat priors\n",
    "    δ = pm.Dirichlet('edu_mix',[2]*(nedu-1))\n",
    "    # Cumulative\n",
    "    δ0 = [0.0]\n",
    "    δedu = pm.Deterministic('δedu', pm.math.concatenate([δ0, δ]),dims='EDU')\n",
    "    # For each level of education, the cumulative delta_E, phi_i\n",
    "    cδedu = pm.Deterministic('δedu_cumsum', δedu.cumsum(), dims='EDU')\n",
    "    \n",
    "    # Maximum education effect\n",
    "    βE = pm.Normal('E', 0, 1)\n",
    "    \n",
    "    # Elements present in story\n",
    "    βC = pm.Normal('C', 0, 1)\n",
    "    βA = pm.Normal('A', 0, 1)\n",
    "    βI = pm.Normal('I', 0, 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = βE*cδedu[E]+βC*C+βA*A+βI*I\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.OrderedLogistic('Yi', μ, κ, observed=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLo_oc.point_logps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with OLo_oc:\n",
    "    trace_oc = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot covariate effects\n",
    "pm.plot_forest(trace_oc,var_names=['C','I','A','E', 'δedu_cumsum'])\n",
    "plt.axvline(0)\n",
    "plt.savefig('cutco_post.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
