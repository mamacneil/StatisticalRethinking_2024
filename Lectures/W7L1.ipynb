{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Lecture 1 - Le Poisson\n",
    "\n",
    "McElreath's lecture for today: https://www.youtube.com/watch?v=YrwL6t0kW2I&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=11\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import dataframe_image as dfi\n",
    "import networkx as nx\n",
    "import patsy\n",
    "import arviz as az\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oceanic tool complexity\n",
    "\n",
    "A fascinating set of data from [Michelle Kline](https://www.michelleakline.com/), whereby she collected information about the size and complexity of tools among polyensian and melanesian societies. The idea is that larger, more connected populations should have more complex sets of tools. So let's take a look and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab tool data\n",
    "kdata = pd.read_csv('Kline.csv', sep=';')\n",
    "dfi.export(kdata, 'kline.png')\n",
    "kdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the entire dataset - this should give us pause about sample sizes and why McElreath is so focused on regularization, but also give us hope that we can make inferences from data in a Bayesian context by thinking carefully about what we're doing. \n",
    "\n",
    "Here we want to know: does the number of tools increase with log(population) size and the contact rate among islands? To do this, we can build a Poisson model for tool counts:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_i \\sim & Poisson(\\lambda_i) \\\\\n",
    "log(\\lambda_i) = & \\beta_c + \\beta_p log(P)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where there are distinct interepts $\\beta_c$ and population size effects $\\beta_b$ for high and low contact islands. Given how small this dataset is, priors become very important. Let's simulate first some intercepts to see what the Poisson and log link do to our numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "# Plot priors from N(0, 10)\n",
    "ax = az.plot_kde(np.random.lognormal(0,10,1000), label=\"Bc ~ LN(0, 10)\")\n",
    "ax.set_xlabel(\"mean number of tools\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"exp(Bc)\")\n",
    "plt.savefig('ln10.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, those numbers get big fast - how big? Well the mean value is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(np.random.normal(0,10,10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a boatload. Picking something sensible can be hard. If we eyeball the total tools column stuff is in the 10's (between 13 and 71), so let's try some alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(np.random.normal(2,2,10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, let's see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "# Plot priors from N(2, 2)\n",
    "ax = az.plot_kde(np.random.lognormal(2,2,10000), label=\"Bc ~ LN(2, 2)\")\n",
    "ax.set_xlabel(\"mean number of tools\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"exp(Bc)\")\n",
    "plt.savefig('ln2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hummm...not ideal - 1000's of tools isn't on the radar, maybe tighten up the variance a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(np.random.normal(2,.5,10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems low, maybe increase the mean a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.exp(np.random.normal(3,.5,10000))), np.exp(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "# Plot priors from N(3, .5)\n",
    "ax = az.plot_kde(np.random.lognormal(3,0.5,10000), label=\"Bc ~ LN(3, 0.5)\")\n",
    "ax.set_xlabel(\"mean number of tools\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"exp(Bc)\")\n",
    "plt.savefig('ln05.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that seems reasonable for the intercept, but what about the slope? We'll likely use standardized log-population size, so let's see what that looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N = 100\n",
    "# Intercept prior\n",
    "b0 = np.random.normal(3, 0.5, N)\n",
    "# log-Population size prior\n",
    "b1 = np.random.normal(0, 0.5, N)\n",
    "# log-population (std)\n",
    "xnew = stdize(np.log(np.linspace(100,200_000,1000)))\n",
    "# Plot predicted lines\n",
    "[plt.plot(xnew, np.exp(b0_+b1_*xnew)) for b0_,b1_ in zip(b0,b1)]\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(-1,1)\n",
    "plt.axvline(0,c='black')\n",
    "plt.xlabel('log(population) (std)')\n",
    "plt.ylabel('total tools')\n",
    "plt.title(\"Prior predictive lines\")\n",
    "plt.savefig('ppl.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable, so let's grab the data and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kline data\n",
    "\n",
    "# Total tools - response\n",
    "T = kdata.total_tools.values\n",
    "\n",
    "# log-Population size (std)\n",
    "P = stdize(np.log(kdata.population.values))\n",
    "# Dummy for high-contact\n",
    "C,Ic = indexall(kdata.contact.values)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Contact':C}) as Tools:\n",
    "    # Data\n",
    "    T_ = pm.Data('Tools', T, mutable=True)\n",
    "    P_ = pm.Data('zPopulation', P, mutable=True)\n",
    "    Ic_ = pm.Data('Ic', Ic, mutable=True)\n",
    "    \n",
    "    # Contact intercepts\n",
    "    β0 = pm.Normal('Intercept', 3, 0.5, dims='Contact')\n",
    "    # Contact effects of log-population size\n",
    "    β1 = pm.Normal('logPop', 0, 0.2, dims='Contact')\n",
    "    \n",
    "    # Linear model\n",
    "    λ = pm.math.exp(β0[Ic_]+β1[Ic_]*P_)\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Poisson('TotalTools', λ, observed=T_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tools:\n",
    "    trace_t = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_t)\n",
    "#dfi.export(tmp.style.background_gradient(), 'df_m1.png')\n",
    "dfi.export(tmp, 'df_m1.png')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like log(population) size matters for low contact places, but not for high.\n",
    "\n",
    "To illustrate something key about parameters and overfitting in the Poisson context, let's also run an intercept-only model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Tools_null:\n",
    "    # Data\n",
    "    T_ = pm.Data('Tools', T, mutable=True)\n",
    "    P_ = pm.Data('zPopulation', P, mutable=True)\n",
    "    Ic_ = pm.Data('Ic', Ic, mutable=True)\n",
    "    \n",
    "    # Contact intercepts\n",
    "    β0 = pm.Normal('Intercept', 3, 0.5)\n",
    "\n",
    "    # Linear model\n",
    "    λ = pm.math.exp(β0)\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Poisson('TotalTools', λ, observed=T_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tools_null:\n",
    "    trace_n = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_n)\n",
    "#dfi.export(tmp.style.background_gradient(), 'df_m1.png')\n",
    "dfi.export(tmp, 'df_m0.png')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tools loo\n",
    "with Tools_null:\n",
    "    pm.compute_log_likelihood(trace_n)\n",
    "with Tools:\n",
    "    pm.compute_log_likelihood(trace_t)\n",
    "tools_loo = pm.loo(trace_t, pointwise=True)\n",
    "tools_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = az.compare({\"Tools - logPop\": trace_t, \"Tools - null\": trace_n})\n",
    "dfi.export(tmp, 'lootable.png')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the log-population model does better than the null - but also that there are loo-based warnings. Where are thse coming from? We can plot the data with our model, scaling the point sizes by their Pareto-k values to see where the issues are (remember the chimps?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicted values from the model using PyMC\n",
    "\n",
    "# Number of values\n",
    "N = 10\n",
    "# Range of standardized population\n",
    "xnew = np.linspace(min(P),max(P),N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tools:\n",
    "    # Low contact predictions\n",
    "    pm.set_data({'Ic':np.zeros(N).astype(int), 'zPopulation':xnew})\n",
    "    ynew0_ = pm.sample_posterior_predictive(trace_t, var_names=[\"TotalTools\"], predictions=True)\n",
    "    # High contact predictions\n",
    "    pm.set_data({'Ic':np.ones(N).astype(int), 'zPopulation':xnew})\n",
    "    ynew1_ = pm.sample_posterior_predictive(trace_t, var_names=[\"TotalTools\"], predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab intervals\n",
    "ynew0 = ynew0_.predictions.TotalTools\n",
    "ynew1 = ynew1_.predictions.TotalTools\n",
    "# Grab trendlines\n",
    "ymu0 = ynew0.values.mean(1)[0]\n",
    "ymu1 = ynew1.values.mean(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab pointwise k values\n",
    "k = tools_loo.pareto_k.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot model fits with pareto k values\n",
    "# Code from https://github.com/AlexAndorra\n",
    "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Point size scaled to pareto k \n",
    "psize = k/k.max()*250\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Plot standardized scale\n",
    "# Plot low-connected fit\n",
    "az.plot_hdi(xnew, ynew0, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(xnew, ymu0, \"--\", color=\"b\", alpha=0.7, label=\"Low contact expected\")\n",
    "\n",
    "# Plot highly-connected fit\n",
    "az.plot_hdi(xnew, ynew1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax0)\n",
    "ax0.plot(xnew, ymu1,  color=\"b\", alpha=0.7, label=\"High contact expected\")\n",
    "\n",
    "# Plot names and k values\n",
    "mask = k>0.5\n",
    "labels = kdata.culture.values[mask]\n",
    "[ax0.text(P[mask][i]-0.2, T[mask][i]+4, f\"{text}({np.round(k[mask][i], 2)})\",fontsize=8,) for i, text in enumerate(labels)]\n",
    "\n",
    "# Plot highly-connected data\n",
    "indx = Ic == C.index('low')\n",
    "ax0.scatter(P[indx], T[indx],s=psize[indx],facecolors=\"none\",edgecolors=\"k\",alpha=0.8,lw=1,label=\"low contact\")\n",
    "# Plot low-connected data\n",
    "ax0.scatter(P[~indx], T[~indx],s=psize[~indx],alpha=0.8,lw=1,label=\"high contact\")\n",
    "\n",
    "# Make pretty\n",
    "ax0.set_xlabel(\"log(Population) (std)\")\n",
    "ax0.set_ylabel(\"Total tools\")\n",
    "ax0.legend(fontsize=8, ncol=2)\n",
    "ax0.set_ylim(0,130)\n",
    "\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Plot natural scale\n",
    "# Undstandardize prediction scale\n",
    "tmp = np.exp(xnew*np.log(kdata.population.values).std()+np.log(kdata.population.values).mean())\n",
    "xnew2 = az.from_dict(posterior={\"T\": T}, constant_data={\"P\":tmp}).constant_data.P\n",
    "\n",
    "# Plot low-connected fit\n",
    "az.plot_hdi(xnew2, ynew0, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2.values, ymu0, \"--\", color=\"b\", alpha=0.7, label=\"Low contact mean\")\n",
    "\n",
    "# Plot highly-connected fit\n",
    "az.plot_hdi(xnew2, ynew1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2.values, ymu1,  color=\"b\", alpha=0.7, label=\"High contact mean\")\n",
    "\n",
    "# Plot low-connected data\n",
    "ax1.scatter(kdata.population[indx], T[indx],s=psize[indx],facecolors=\"none\",edgecolors=\"k\",alpha=0.8,lw=1,label=\"low contact\")\n",
    "# Plot high-connected data\n",
    "ax1.scatter(kdata.population[~indx], T[~indx],s=psize[~indx],alpha=0.8,lw=1,label=\"high contact\")\n",
    "ax1.set_xlim(0,300000)\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlabel(\"Population\")\n",
    "ax1.set_ylabel(\"Total tools\")\n",
    "ax1.set_ylim(0,130)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('modelfits.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a $N(0, 0.2)$ prior for the slopes, we can see that the high contact mean trend falls below that of Hawaii - this is a problem because low-contact Hawaii has lots of tools due to high population, so we'd expect the counterfactual high-contact Hawaii to have even more tools, but the prediction says less. This is due, in part to the fact that our standardized model isn't anchored on zero for total tools at zero total population size. We can do better than this.\n",
    "\n",
    "*Scientific model*\n",
    "\n",
    "Based on domain knowledge - i.e. our understanding of the system under study - it should make sense that innovation in tool development increases with population size but with diminishing returns; eventually each additional person will add less innovation. Also cultures tend to discard tools over time, replacing them with new ones. These two processes can be represented by\n",
    "\n",
    "$$\n",
    "\\Delta T = \\alpha P^{\\beta} - \\gamma T\n",
    "$$\n",
    "\n",
    "where the change in tool number per time step ($\\Delta T$) is equal to some increase ($\\alpha$) proportional to population size, with diminishing returns ($\\beta$), and the per-unit-time rate of tool loss ($\\gamma$). If we then set $\\Delta T=0$, we'll get the equilibrium tool set size\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{T} = \\frac{\\alpha P^{\\beta}}{\\gamma}.\n",
    "$$\n",
    "\n",
    "Which we can encode this into a statistical model as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_i \\sim & Poisson(\\lambda_i) \\\\\n",
    "\\lambda_i = & \\frac{\\alpha P^{\\beta}}{\\gamma}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the link function is gone now, because we have a mechanistic model (science!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population size actual counts\n",
    "p = kdata.population.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Contact':C}) as SciTools:\n",
    "    # Data\n",
    "    T_ = pm.Data('Tools', T, mutable=True)\n",
    "    P_ = pm.Data('Population', p, mutable=True)\n",
    "    Ic_ = pm.Data('Ic', Ic, mutable=True)\n",
    "    \n",
    "    # Innovation increase with population\n",
    "    α = pm.Normal('innovation', .5, 1, dims='Contact')\n",
    "    # Diminishing returns\n",
    "    β = pm.Exponential('dim_returns', 2, dims='Contact')\n",
    "    # Tool loss rate\n",
    "    γ = pm.Exponential('tool_loss', .5)\n",
    "    \n",
    "    # Scientific model\n",
    "    λ = (pm.math.exp(α[Ic_])*(P_**β[Ic_]))/γ\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Poisson('TotalTools', λ, observed=T_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SciTools:\n",
    "    trace_st = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_st, compact=True)\n",
    "plt.savefig('sciencetrace.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, things looking good here on the estimation side - how about plotting our model against our observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicted values from the model using PyMC\n",
    "\n",
    "# Number of values\n",
    "N = 10\n",
    "# Range of positive standardized population\n",
    "xnew = np.linspace(min(p),max(p),N, dtype='int64')\n",
    "\n",
    "with SciTools:\n",
    "    # Low contact predictions\n",
    "    pm.set_data({'Ic':np.array([0]*N), 'Population':xnew})\n",
    "    ynew0_ = pm.sample_posterior_predictive(trace_st)\n",
    "    # High contact predictions\n",
    "    pm.set_data({'Ic':np.array([1]*N), 'Population':xnew})\n",
    "    ynew1_ = pm.sample_posterior_predictive(trace_st)\n",
    "\n",
    "    \n",
    "# Grab intervals\n",
    "ynew0 = ynew0_.posterior_predictive.TotalTools\n",
    "ynew1 = ynew1_.posterior_predictive.TotalTools\n",
    "# Grab trendlines\n",
    "ymu0 = ynew0.values.mean(1)[0]\n",
    "ymu1 = ynew1.values.mean(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot model fits with pareto k values\n",
    "# Code from https://github.com/AlexAndorra\n",
    "_, (ax1) = plt.subplots(1, 1, figsize=(7, 6))\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Point size scaled to pareto k \n",
    "psize = k/k.max()*250\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Plot natural scale\n",
    "# Undstandardize prediction scale\n",
    "xnew2 = xnew\n",
    "\n",
    "# Plot low-connected fit\n",
    "az.plot_hdi(xnew2, ynew0, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2, ymu0, \"--\", color=\"b\", alpha=0.7, label=\"Low contact mean\")\n",
    "\n",
    "# Plot highly-connected fit\n",
    "az.plot_hdi(xnew2, ynew1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2, ymu1,  color=\"b\", alpha=0.7, label=\"High contact mean\")\n",
    "\n",
    "# Plot low-connected data\n",
    "ax1.scatter(kdata.population[indx], T[indx],s=psize[indx],facecolors=\"none\",edgecolors=\"k\",alpha=0.8,lw=1,label=\"low contact\")\n",
    "# Plot high-connected data\n",
    "ax1.scatter(kdata.population[~indx], T[~indx],s=psize[~indx],alpha=0.8,lw=1,label=\"high contact\")\n",
    "\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlabel(\"Population\")\n",
    "ax1.set_ylabel(\"Total tools\")\n",
    "ax1.set_xlim((-10000, 300000))\n",
    "ax1.set_ylim(0,130)\n",
    "ax1.legend(fontsize=8, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sciencefits.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma-Poisson mixture (aka Negative Binomial)\n",
    "\n",
    "While Poisson distributions abound in nature, the hard scaling of the mean and variance being equal to $\\lambda$ means that it can fail in cases where the variance is either greater or less than we'd expect. In these cases we can turn to the [Negative Binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution), which can be thought of as (and often arises from) a mixture of Poisson distribtuions whose rates are [Gamma distributed](https://en.wikipedia.org/wiki/Gamma_distribution). This is, in my view, a much more intuitive representation than the Negative Binomial's namesake definition, which relates to the expected number of times you something fails until you see a success.\n",
    "\n",
    "In the case of our scientific model above, we can use the Negative Binomial in place of the Poisson to allow for *overdispersion* - the rate at which the variance departs from the mean as it increases, namely:\n",
    "\n",
    "$$\n",
    "var_{NB} = \\lambda+\\lambda^{2}/\\phi\n",
    "$$\n",
    "\n",
    "where $\\phi$ conrols the rate of departure from the $mean=variance$ relationship of the Poisson. As $\\phi$ increases, the NB converges toward a Poisson. \n",
    "\n",
    "Let's apply this model to the Oceania data and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Contact':C}) as SciTools_NB:\n",
    "    # Data\n",
    "    T_ = pm.Data('Tools', T, mutable=True)\n",
    "    P_ = pm.Data('Population', p, mutable=True)\n",
    "    Ic_ = pm.Data('Ic', Ic, mutable=True)\n",
    "    \n",
    "    # Innovation increase with population\n",
    "    α = pm.Normal('iRate', 1, 1, dims='Contact')\n",
    "    # Diminishing returns\n",
    "    β = pm.Exponential('dReturns', 2, dims='Contact')\n",
    "    # Tool loss rate\n",
    "    γ = pm.Exponential('lRate', .5)\n",
    "    \n",
    "    # Scientific model\n",
    "    λ = (pm.math.exp(α[Ic_])*P_**β[Ic_])/γ\n",
    "\n",
    "    # Gamma mixture parameter\n",
    "    φ = pm.Gamma('phi', 0.01, 0.01)\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.NegativeBinomial('TotalTools', λ, φ, observed=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SciTools_NB:\n",
    "    trace_stnb = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_stnb, compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicted values from the model using PyMC3\n",
    "\n",
    "# Number of values\n",
    "N = 10\n",
    "\n",
    "with SciTools_NB:\n",
    "    # Low contact predictions\n",
    "    pm.set_data({'Ic':np.array([0]*N), 'Population':xnew})\n",
    "    ynew0_ = pm.sample_posterior_predictive(trace_stnb)\n",
    "    # High contact predictions\n",
    "    pm.set_data({'Ic':np.array([1]*N), 'Population':xnew})\n",
    "    ynew1_ = pm.sample_posterior_predictive(trace_stnb)\n",
    "    \n",
    "# Grab intervals\n",
    "ynew0 = ynew0_.posterior_predictive.TotalTools\n",
    "ynew1 = ynew1_.posterior_predictive.TotalTools\n",
    "# Grab trendlines\n",
    "ymu0 = ynew0.values.mean(1)[0]\n",
    "ymu1 = ynew1.values.mean(1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot model fits with pareto k values\n",
    "# Code from https://github.com/AlexAndorra\n",
    "_, (ax1) = plt.subplots(1, 1, figsize=(7, 6))\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Point size scaled to pareto k \n",
    "psize = k/k.max()*250\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Plot natural scale\n",
    "# Undstandardize prediction scale\n",
    "xnew2 = xnew\n",
    "\n",
    "# Plot low-connected fit\n",
    "az.plot_hdi(xnew2, ynew0, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2, ymu0, \"--\", color=\"b\", alpha=0.7, label=\"Low contact mean\")\n",
    "\n",
    "# Plot highly-connected fit\n",
    "az.plot_hdi(xnew2, ynew1, color=\"b\", fill_kwargs={\"alpha\": 0.2}, ax=ax1)\n",
    "ax1.plot(xnew2, ymu1,  color=\"b\", alpha=0.7, label=\"High contact mean\")\n",
    "\n",
    "# Plot low-connected data\n",
    "ax1.scatter(kdata.population[indx], T[indx],s=psize[indx],facecolors=\"none\",edgecolors=\"k\",alpha=0.8,lw=1,label=\"low contact\")\n",
    "# Plot high-connected data\n",
    "ax1.scatter(kdata.population[~indx], T[~indx],s=psize[~indx],alpha=0.8,lw=1,label=\"high contact\")\n",
    "\n",
    "plt.setp(ax1.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax1.set_xlabel(\"Population\")\n",
    "ax1.set_ylabel(\"Total tools\")\n",
    "ax1.set_xlim((-10000, 300000))\n",
    "ax1.set_ylim(0,130)\n",
    "ax1.legend(fontsize=8, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sciencefits_nb.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson exposure rates\n",
    "\n",
    "A key aspect of Poisson (or NB) mean parameters is that they can be thought of as rates - which means that they can be thought of as the count of events per unit exposure. Exposure can be any kind of standardization - per capita, per unit area, per kg, whatever - and to represent it, we can add a simple offset to our model, namely:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i \\sim & Poisson(\\lambda_i) \\\\\n",
    "log(\\lambda_i) = & log(\\frac{\\mu_i}{\\tau_i}) = \\beta_0 + \\beta_1x_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "is equivent to \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i \\sim & Poisson(\\mu_i) \\\\\n",
    "log(\\mu_i) = & log(\\tau_i) + \\beta_0 + \\beta_1x_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mu_i$ is the expected count and $\\tau_i$ is the exposure. This often comes up so it is important to know about."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
