{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7 Lecture 2 - Monsters and mixtures\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=p7g-CgGCS34\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://xcelab.net/rm/statistical-rethinking/\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023/tree/main\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import patsy\n",
    "import arviz as az\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Analysis\n",
    "\n",
    "A key relationship between count models like the Poisson and the data they represent is that the underlying parameters are about rates - how long do we wait (or how wide an area do we search) on average for an event to take place? Therefore another option for quantifying these things lies in modeling times to events - with the trick being that we have to also pay attention to the *censoring* of data. \n",
    "\n",
    "1. Left-censored: when you don't know when time started relative to events\n",
    "2. Right-censored: ending the observation period before the next event occurs\n",
    "3. Dual-censored: both problems at once\n",
    "\n",
    "Ignoring the censoring process will also lead to inferrential mistakes, so each survival analysis needs to keep track of their potential effects.\n",
    "\n",
    "By way of example, let's have a look at some cat data from Austin Texas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cat data\n",
    "cdata = pd.read_csv('AustinCats.csv')\n",
    "dfi.export(cdata.head(), 'catdata.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here the data has dates in and out for individual cats, as well as things about their breed etc. Let's say we're interested in the average time to adoption, a useful metric if you're running a cat shelter. If we were modelling just the days to adoption ($D$) and ignoring the censoring, the model would simply be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_i \\sim & Exponential(\\lambda_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the underlying rate of adoptions. Let's run that model and see what happens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adoption days\n",
    "D = cdata.days_to_event.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as adopt:\n",
    "    # Adoption rate\n",
    "    λ = pm.Exponential('Adoption rate', 1)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Exponential('Yi', λ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with adopt:\n",
    "    trace_a = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace_a)\n",
    "plt.savefig('rate.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adoption rate is 0.04 - what does that mean? Well the mean of an exponetial is $1/\\lambda$, so this means that the average days to adoption is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_density(1/trace_a.posterior['Adoption rate'],hdi_prob=0.999)\n",
    "plt.axvline(np.mean(D),c='black')\n",
    "plt.xlabel('Average days to adoption')\n",
    "plt.savefig('meandays.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in our dataset we stopped recording the data at some point, with newly-arrived cats that hadn't yet had a chance to be adopted. So how do we deal with this right-hand censoring? Well for the cats that are adopted we know their number of days to adoption, so no issues there. But for the cats in the data that have yet to adopted, all we know is how long they've been in jail. This second set of cats implies a different quantity, namely the number of days in captivity without yet having been adpoted, which is complement of the exponential (i.e. the complementary cumulative distribution). We can add this into our model as a special component:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(c=1,D) = & \\lambda e^{-\\lambda D} \\\\\n",
    "f(c=0,D) = & e^{-\\lambda D}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The top line is just the Exponential PDF. The bottom line is the censored PDF, which simply has the leading $\\lambda$ parameter removed. This because the uncensorted cumulative distribution function is\n",
    "\n",
    "$$\n",
    "1-e^{-\\lambda D}\n",
    "$$\n",
    "\n",
    "which is the probability of being adopted given time D. So to get the censored bit, this is simply the probability of *not* being adopted by time D, which is\n",
    "\n",
    "$$\n",
    "e^{-\\lambda D}.\n",
    "$$\n",
    "\n",
    "Because of this small difference, we can get the log probability for both kinds of data (censored and not) by adding an indicator variable (c) to show that the cat has or has not been adopted\n",
    "\n",
    "\n",
    "$$\n",
    "log(\\lambda e^{-\\lambda D})-log(e^{-\\lambda D})c\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "log(\\lambda)c -\\lambda D\n",
    "$$\n",
    "\n",
    "cool huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adoped or not\n",
    "A = cdata.out_event.values == 'Adoption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as adoptC:\n",
    "    # Adoption rate\n",
    "    λ = pm.Exponential('Adoption rate', 1)\n",
    "    \n",
    "    # Censored/non log-probability\n",
    "    def CE_logp(value, λ, cens):\n",
    "        return pm.math.sum(cens*pm.math.log(λ)-λ*value)\n",
    "\n",
    "    # Censored likelihood\n",
    "    ExSurv = pm.DensityDist('ExSurv', λ, A, logp=CE_logp, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with adoptC:\n",
    "    trace_aC = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_aC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(1/trace_aC.posterior['Adoption rate'].values[0].T,label='Censored',density=True)\n",
    "plt.hist(1/trace_a.posterior['Adoption rate'].values[0].T,label='Naive',density=True)\n",
    "plt.xlabel('Average days to adoption')\n",
    "plt.legend()\n",
    "plt.savefig('censor.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has made a massive difference, with censoring accounting for a 50% decrease in apparent adoption times in the previous model. \n",
    "\n",
    "\n",
    "The next question is, are black cats adopted at a lower rate relative to other colours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , Ic = indexall(cdata.color.values=='Black')\n",
    "Colour = ['Other','Black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Colour':Colour}) as adoptCb:\n",
    "    # Adoption rate\n",
    "    λ = pm.Exponential('Adoption rate', 1, dims='Colour')\n",
    "    \n",
    "    # Censored/non log-probability\n",
    "    def CE_logp(value, λ, cens):\n",
    "        return pm.math.sum(cens*pm.math.log(λ[Ic])-λ[Ic]*value)\n",
    "\n",
    "    # Censored likelihood\n",
    "    ExSurv = pm.DensityDist('ExSurv', λ, A, logp=CE_logp, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with adoptCb:\n",
    "    trace_aCb = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace_aCb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = 1/trace_aCb.posterior['Adoption rate'].values[0].T[0]\n",
    "black = 1/trace_aCb.posterior['Adoption rate'].values[0].T[1]\n",
    "\n",
    "plt.hist(other, label='Other',density=True)\n",
    "plt.hist(black, color='black', label='Black',density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Days to adoption')\n",
    "plt.savefig('black.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly yes, with the average difference being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(black-other, color='purple',density=True)\n",
    "plt.xlabel('Black cat extra days')\n",
    "plt.savefig('blackother.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about 9 additional days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drunken monks\n",
    "\n",
    "Mixture models have some incredible properties and the ability to distinguish between different sources of zeros is one of them. What do we mean here? Well often processes aren't distinct in how data can arise - many times multiple things happen that can lead to the same observation. Zeros are a classic example - in fisheries for example, zeros can arise because fish weren't there, or because a boat failed to catch fish that were. These kinds of situations arise all the time and as a result a series of **zero inflated** models have been developed to address them. \n",
    "\n",
    "By way of example, imagine a monestary where monks are busy copying manuscripts (important work for a monk) and each day a large number of monks finish copying a small number of manuscripts. This kind of thing could easliy be represented as a Poisson process, whereby manuscripts are churned out at a constant underlying rate ($\\lambda$), leading to a Poisson distribution that has some proportion of zeros.\n",
    "\n",
    "But this being a Catholic monestary the monks on occasion turn to drink, upon which days they produce nothing - an additional source of zeros that is unrelated to the typical rate of production. How should we handle this? Well we can model both processes and allow the model to sort out which zeros are likely to arise from which processes. How? Well let's simulate this process for a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of days\n",
    "N = 365\n",
    "\n",
    "# Probability of drinking\n",
    "p_drink = 0.2\n",
    "# Rate of manuscript production\n",
    "work_rate = 1.3\n",
    "\n",
    "# Simulate work or drink for each day of the year\n",
    "work = np.random.binomial(1,1-p_drink,N)\n",
    "\n",
    "# Simulate production of manuscripts\n",
    "M_ = np.random.poisson(work_rate,N)\n",
    "M = work*M_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuscripts on work days only\n",
    "plt.hist(M_, bins=30)\n",
    "plt.ylim(0,170)\n",
    "plt.savefig('work.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed manuscript production\n",
    "plt.hist(M, bins=30)\n",
    "plt.ylim(0,170)\n",
    "plt.savefig('manuscripts.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZI manuscript production\n",
    "z_work = (M_==0).sum()\n",
    "z_total = sum(M==0)\n",
    "\n",
    "plt.hist(M, bins=30)\n",
    "plt.plot((0.09,0.09),(z_work, z_total),linewidth=10, c='red')\n",
    "plt.ylim(0,170)\n",
    "plt.savefig('zimonk.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with our simulated data in place, we need to develop a likelihood that reflects both processes, a zero-inflated Poisson. There is a ZIP model built into both Stan and PyMC, but it's fairly simple to define so let's do that for our model here as well, just to get a bit of practice and the knowledge that **we're allowed** to build whatever model we like - there are no rules beyond coherence.\n",
    "\n",
    "From our simulation we have the elements we'll need, a binomial for the workin' vs drinkin' days, and a Poisson for the manuscript production rate. First the bionomial pmf (probability **mass** function) for 1/0 data is a Bernoulli\n",
    "\n",
    "$$\n",
    "p^{k}(1-p)^{1-k}\n",
    "$$\n",
    "\n",
    "and the pmf for a Poisson is\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda^{k}e^{-\\lambda}}{k!}\n",
    "$$\n",
    "\n",
    "So the probability of a zero is\n",
    "\n",
    "$$\n",
    "p + (1-p)\\frac{\\lambda^{0}e^{-\\lambda}}{0!} = p + (1-p)e^{-\\lambda}\n",
    "$$\n",
    "\n",
    "And the probabilty of a count (i.e. when $k\\neq0$) is\n",
    "\n",
    "$$\n",
    "(1-p)\\frac{\\lambda^{k}e^{-\\lambda}}{k!}\n",
    "$$\n",
    "\n",
    "the Poisson multiplied by the probability of working. Fortunately for us this is an available likelihood in PyMC3 - note however that the zero-inflation is the probability of counts (not zeros):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Monks:\n",
    "    # Manuscript log-scale rate\n",
    "    γ = pm.Normal('logMrate', 2, .5)\n",
    "    \n",
    "    # log-odds of drinking\n",
    "    μ = pm.Normal('loDrink',-1.5,1)\n",
    "    \n",
    "    # link functions\n",
    "    λ = pm.Deterministic('Mrate', pm.math.exp(γ))\n",
    "    p = pm.Deterministic('pDrink', pm.invlogit(μ))\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.ZeroInflatedPoisson('Yi', 1-p, λ, observed=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Monks:\n",
    "    trace_m = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_m)\n",
    "dfi.export(tmp, 'ziptable.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(trace_m.posterior['pDrink'].values.flatten(), density=True)\n",
    "plt.axvline(p_drink,c='red',lw=4)\n",
    "plt.xlabel('P(drinking)')\n",
    "plt.savefig('Pdrinking.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we were able to recapture the simulated data and see that monks drink about 20% of the time. A question is - what would we have estimated if we'd just used the Poisson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as PMonks:\n",
    "    # Manuscript log-scale rate\n",
    "    γ = pm.Normal('logMrate', 1, 0.5)\n",
    "\n",
    "    # link function\n",
    "    λ = pm.Deterministic('Mrate', pm.math.exp(γ))\n",
    "    \n",
    "    # Likelihood\n",
    "    Yi = pm.Poisson('Yi', λ, observed=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PMonks:\n",
    "    trace_pm = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest([trace_m, trace_pm], model_names=[\"ZIP\", \"Poisson\"], var_names=[\"Mrate\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig('monksmodels.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, all those drinking zeros get sunk into the Poisson density and what we estimate is the Monk's overall (both drinking and working days) rate of manuscript production. What would WAIC say comparing these two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAIC-based model comparison\n",
    "with Monks:\n",
    "    pm.compute_log_likelihood(trace_m)\n",
    "with PMonks:\n",
    "    pm.compute_log_likelihood(trace_pm)\n",
    "comp_WAIC = pm.compare({'Poisson': trace_pm, 'ZIP': trace_m}, ic='waic')\n",
    "dfi.export(comp_WAIC, 'comp_WAIC.jpg')\n",
    "comp_WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the model weight favours zero-inflation. Note that depending on your simulation above, you may randomly get a dataset that doesn't conform quite as well. Re-run the simulation and see what happens. None of this stuff is easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial and categorical models\n",
    "\n",
    "While binomials are commonplace for looking at 0/1 data, successes in trials, categorical data can apply to mutliple outcomes, where quantities end up on distinct bins. \n",
    "\n",
    "Let's look at this kind of data through a simulated example, whereby we simulate various career choices based on expected income. We can port over McElreath's code from Chatper 11 to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate career choices for 500 people\n",
    "N = 500\n",
    "# Expected income for each career\n",
    "income = np.array([1,2,5])\n",
    "# Score for each career based on income\n",
    "bx = 0.4\n",
    "score = 0.4*income\n",
    "\n",
    "# Convert scores to probabilities\n",
    "p = sp.special.softmax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate choices based on underlying income\n",
    "career = np.random.multinomial(1,p,N)\n",
    "# Put choice into categorical array\n",
    "career = np.where(career==1)[1]\n",
    "career"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with these in place, we can develop a regression model to figure out what the effect of expected income is on career choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Careerz:\n",
    "    # Intercepts\n",
    "    a = pm.Normal('a', 0, 1, shape=2)\n",
    "    # Income slope\n",
    "    b = pm.Lognormal('b',0, 0.5)\n",
    "    \n",
    "    # Linear model\n",
    "    s0 = a[0]+b*income[0]\n",
    "    s1 = a[1]+b*income[1]\n",
    "    s = pm.Deterministic('s', pm.math.stack([s0,s1,0]))\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Categorical('Yi', pm.math.softmax(s), observed=career)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Careerz:\n",
    "    trace_c = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_c)\n",
    "dfi.export(tmp, 'multinom.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - we have results, but unfortunately the nature of multinomial regression is that effect sizes are really difficult to interpret, because they're relative to the baseline category used (the 'pivot'). So instead let's look at the posterior predicted probabilities for each category, relative to our known values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_c.posterior['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated probabilities\n",
    "smax  = np.array([sp.special.softmax(x) for x in trace_c.posterior['s'].values[0]])\n",
    "sp.special.softmax(trace_c.posterior['s'].values[0].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(s, density=True) for s in smax.T]\n",
    "[plt.axvline(x,c='black',lw=1,ls=':') for x in p]\n",
    "plt.xlabel('P(career)')\n",
    "plt.savefig('Pcareer.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look great - we've captured the probabilities of choosing each career. But that's a bit unsatisfying, as what we really want to know is the relationship between incomes and choosing each careear. To gain some insight to this, the thing to do is simulate what would happen to these probabilites if, say, the income for career=1 doubles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab posteriors\n",
    "a_post = trace_c.posterior['a'].values[0].T\n",
    "s_post = trace_c.posterior['s'].values[0].T\n",
    "b_post = trace_c.posterior['b'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(b_post)\n",
    "plt.axvline(bx,c='red',lw=4)\n",
    "plt.xlabel('Income effect on career')\n",
    "plt.savefig('Pcareerb.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logit scores\n",
    "s0x = a_post[0]+b_post*income[0]\n",
    "s1x = a_post[1]+b_post*income[1]\n",
    "s1x_new = a_post[1]+b_post*income[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilites from original income and new income\n",
    "p_orig = np.array([sp.special.softmax(x) for x in np.array([s0x,s1x,np.zeros(len(s1x))]).T])\n",
    "p_new = np.array([sp.special.softmax(x) for x in np.array([s0x,s1x_new,np.zeros(len(s1x))]).T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So plotting our model posteriors against our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(p_orig.T[i], density=True, label=str(i)) for i in range(len(a_post)+1)]\n",
    "plt.legend()\n",
    "plt.savefig('Pcareer1.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.hist(s, density=True) for s in p_new.T]\n",
    "plt.savefig('Pcareer2.jpg',dpi=300);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
