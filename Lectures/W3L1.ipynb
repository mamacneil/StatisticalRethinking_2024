{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Lecture 1 - Spurious Waffle Houses\n",
    "\n",
    "McElreath's lectures for the whole book are available here:https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here:  https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waffle Houses\n",
    "\n",
    "Let's import the Waffe House devorce data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "ddata = pd.read_csv('WaffleDivorce.csv',sep=';')\n",
    "# Display top 5 rows\n",
    "ddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "ddata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not unintense data to look at, but let's start with divorce and Waffle Houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(ddata.WaffleHouses,ddata.Divorce)\n",
    "[plt.annotate(txt, (ddata.WaffleHouses[i], ddata.Divorce[i])) for i, txt in enumerate(ddata.Location)]\n",
    "b1,b0 = np.polyfit(ddata.WaffleHouses,ddata.Divorce, 1)\n",
    "xnew = np.linspace(0,400,100)\n",
    "plt.plot(xnew,b0+b1*xnew,c='black')\n",
    "plt.xlabel('Number of Waffle Houses', fontsize=17)\n",
    "plt.ylabel('Divorce rate', fontsize=17)\n",
    "plt.savefig('WaffleDivorce.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or is divorce rate a product of marriage rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(ddata.Marriage,ddata.Divorce)\n",
    "[plt.annotate(txt, (ddata.Marriage[i], ddata.Divorce[i])) for i, txt in enumerate(ddata.Location)]\n",
    "b1,b0 = np.polyfit(ddata.Marriage,ddata.Divorce, 1)\n",
    "xnew = np.linspace(13,32,100)\n",
    "plt.plot(xnew,b0+b1*xnew,c='black')\n",
    "plt.xlabel('Marriage rate', fontsize=17)\n",
    "plt.ylabel('Divorce rate', fontsize=17)\n",
    "plt.savefig('WaffleMarriage.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or age at marriage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(ddata.MedianAgeMarriage,ddata.Divorce)\n",
    "[plt.annotate(txt, (ddata.MedianAgeMarriage[i], ddata.Divorce[i])) for i, txt in enumerate(ddata.Location)]\n",
    "b1,b0 = np.polyfit(ddata.MedianAgeMarriage,ddata.Divorce, 1)\n",
    "xnew = np.linspace(23,32,100)\n",
    "plt.plot(xnew,b0+b1*xnew,c='black')\n",
    "plt.xlabel('Median marriage age', fontsize=17)\n",
    "plt.ylabel('Divorce rate', fontsize=17)\n",
    "plt.savefig('WaffleAge.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what does the South have to do with all this? Well with the assertion of a causal model we can take a look and see. For example, if we assert:\n",
    "\n",
    "A->M->D\n",
    "A->D\n",
    "\n",
    "Then we can look and see what the affect of marriage rate (M) is on divorce (D), given that we know the median age (A). To do this we need a statistcal model to help evaluate this DAG.\n",
    "\n",
    "$$\n",
    "D_i \\sim N(\\mu_i,\\sigma)\\\\\n",
    "\\mu_i = \\beta_0+\\beta_M M_i+\\beta_A A_i\n",
    "$$\n",
    "\n",
    "There is nothing magic here - we've all done multiple regression before - but what is new is our causal assertion. Weird, that what we assert and assume changes things eh? But you should get very comfortable with this idea because it turns out it lies at the core of scientific enquiry - as Popper argued, causality is built consenually. \n",
    "\n",
    "First we should standardize variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = stdize(ddata.MedianAgeMarriage.values)\n",
    "M = stdize(ddata.Marriage.values)\n",
    "D = stdize(ddata.Divorce.values)\n",
    "\n",
    "State = ddata.Location.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With covariates in hand we can do some prior predictive simulation to see what priors might look like in terms of possible lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "nsamp = 100\n",
    "# Intercept\n",
    "β0_ = np.random.normal(0, .2, nsamp)\n",
    "# Marriage rate slope\n",
    "βm_ = np.random.normal(0, .5, nsamp)\n",
    "# Marriage age slope\n",
    "βa_ = np.random.normal(0, .5, nsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Grab range of marriage ages to plot over\n",
    "A_ = np.linspace(min(A),max(A),50)\n",
    "# Plot resulting lines given sample values for β0 and βa, using a list comprehension\n",
    "[ax[0].plot(A_, b0+b1*A_, c='black', alpha=0.1) for b0,b1 in zip(β0_,βa_)]\n",
    "# Make it look nice\n",
    "ax[0].set_xlabel('Median marriage age (std)', fontsize=17)\n",
    "ax[0].set_ylabel('Divorce rate (std)', fontsize=17)\n",
    "\n",
    "\n",
    "\n",
    "# Grab range of marriage rates to plot over\n",
    "M_ = np.linspace(min(M),max(M),50)\n",
    "# Plot resulting lines given sample values for β0 and βm, using a list comprehension\n",
    "[ax[1].plot(M_, b0+b1*M_, c='black', alpha=0.1) for b0,b1 in zip(β0_,βm_)]\n",
    "# Make it look nice\n",
    "ax[1].set_xlabel('Marriage rate (std)', fontsize=17)\n",
    "ax[1].set_ylabel('Divorce rate (std)', fontsize=17)\n",
    "plt.tight_layout()\n",
    "plt.savefig('n0xstd.jpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can bulid a NUTS model in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal model\n",
    "\n",
    "# Bayesian PyMC\n",
    "with pm.Model(coords={'State':State}) as divorce:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    βa = pm.Normal('Marriage age', 0, .5)\n",
    "    βm = pm.Normal('Marriage rate', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = pm.Deterministic('Mu', β0+βa*A+βm*M, dims='State')\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampler\n",
    "with divorce:\n",
    "    trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace, var_names=['Intercept', 'Marriage age', 'Marriage rate', 'Sigma'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('posterior.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_forest(trace, var_names=['Intercept', 'Marriage age', 'Marriage rate', 'Sigma'], ridgeplot_overlap=3)\n",
    "plt.axvline(0,linestyle=':')\n",
    "plt.xlabel('Effect size (std)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('forest.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC\n",
    "with pm.Model(coords={'State':State}) as divorce_m:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    βm = pm.Normal('Marriage rate', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = pm.Deterministic('Mu', β0+βm*M, dims='State')\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC\n",
    "with pm.Model(coords={'State':State}) as divorce_a:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    βa = pm.Normal('Marriage age', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = pm.Deterministic('Mu', β0+βa*A, dims='State')\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run samplers\n",
    "with divorce_m:\n",
    "    trace_m = pm.sample(1000)\n",
    "with divorce_a:\n",
    "    trace_a = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "v_ = 'Marriage age'\n",
    "ax_ = 0\n",
    "az.plot_dist(trace_a.posterior[v_],ax=ax[ax_], label=v_)\n",
    "ax[ax_].set_xlim(-1,1)\n",
    "ax[ax_].axvline(0,c='black',linestyle=\":\")\n",
    "\n",
    "v_ = 'Marriage rate'\n",
    "ax_ = 1\n",
    "az.plot_dist(trace_m.posterior[v_],ax=ax[ax_], label=v_)\n",
    "ax[ax_].set_xlim(-1,1)\n",
    "ax[ax_].axvline(0,c='black',linestyle=\":\")\n",
    "plt.savefig('singles.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "v_ = 'Marriage age'\n",
    "ax_ = 0\n",
    "az.plot_dist(trace.posterior[v_],ax=ax[ax_], label=v_+' (full)',color='red')\n",
    "az.plot_dist(trace_a.posterior[v_],ax=ax[ax_], label=v_+' (only)')\n",
    "ax[ax_].axvline(0,c='black',linestyle=\":\")\n",
    "\n",
    "\n",
    "v_ = 'Marriage rate'\n",
    "ax_ = 1\n",
    "az.plot_dist(trace.posterior[v_],ax=ax[ax_], label=v_+' (full)', color='red')\n",
    "az.plot_dist(trace_m.posterior[v_],ax=ax[ax_], label=v_+' (only)')\n",
    "ax[ax_].axvline(0,c='black',linestyle=\":\")\n",
    "\n",
    "plt.savefig('conditionals.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Among the most - I'll say **the most** - important checks on your models is to plot the model and the data together. It is critical that you see things the way the model sees things, otherwise it is difficult to know how well you're doing in fitting these things. Three options are:\n",
    "\n",
    "    1. Predictor residual plots\n",
    "    2. Posterior prediction plots\n",
    "    3. Counterfactual plots\n",
    "    \n",
    "Each has their own value and can tell us something about how our model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predictor residual plots\n",
    "\n",
    "There's an awful legacy in Biology of modelling the residuals of another model. It's awful because it's wrong, and you should never do it. It's wrong because it doesn't get the unceratinties right, prioritizing variation in the first analysis and hiding it in the second. This leads to biased estiamtes, possibly for both models, but certainly for the second. But there is some utility in seeing what information remains in one predictor when you already have information about the other (which is what multiple regression does). \n",
    "\n",
    "To do this we need to build individual models where we regress one predictor on the other, which will give us the marginal benefit of the other predictor conditional on knowing one of them.\n",
    "\n",
    "So for the divorce case, we have two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC\n",
    "with pm.Model() as m_a:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    βm = pm.Normal('Marriage rate', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = pm.Deterministic('Mu', β0+βm*M)\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC\n",
    "with pm.Model() as a_m:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    βm = pm.Normal('Marriage age', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = pm.Deterministic('Mu', β0+βm*A)\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run samplers\n",
    "with a_m:\n",
    "    trace_m = pm.sample(1000)\n",
    "with m_a:\n",
    "    trace_a = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residuals for the other predictor - first set of chains (of 4)\n",
    "m_pred = trace_m.posterior['Mu'].values[0,].mean(0)\n",
    "residuals_m = M - m_pred\n",
    "\n",
    "a_pred = trace_a.posterior['Mu'].values[0,].mean(0)\n",
    "residuals_a = A - a_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "xnew = np.linspace(-2,3,100)\n",
    "\n",
    "v_ = 'Marriage rate'\n",
    "ax_ = 0\n",
    "ax[ax_].scatter(M,A)\n",
    "ax[ax_].plot(xnew,np.median(trace_a.posterior['Intercept'])+np.median(trace_a.posterior[v_])*xnew)\n",
    "ax[ax_].set_ylabel('Marriage age', fontsize=17,c='red')\n",
    "ax[ax_].set_xlabel(v_, fontsize=17)\n",
    "ax[ax_].vlines(M, a_pred, a_pred + residuals_a, colors='grey')\n",
    "\n",
    "v_ = 'Marriage age'\n",
    "ax_ = 1\n",
    "ax[ax_].scatter(A,M)\n",
    "ax[ax_].plot(xnew,np.median(trace_m.posterior['Intercept'])+np.median(trace_m.posterior[v_])*xnew)\n",
    "ax[ax_].set_ylabel('Marriage rate', fontsize=17)\n",
    "ax[ax_].set_xlabel(v_, fontsize=17,c='red')\n",
    "ax[ax_].vlines(A, m_pred, m_pred + residuals_m, colors='grey')\n",
    "plt.tight_layout()\n",
    "plt.savefig('residualplots.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's seemingly bonkers, is that we now have the residuals for each parameter, we can plot them against divorce to see how the **full model** actually sees these things inside their guts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "xnew = np.linspace(-2,3,100)\n",
    "\n",
    "v_ = 'Marriage age'\n",
    "ax_ = 0\n",
    "ax[0,ax_].scatter(residuals_a,D)\n",
    "coef = np.polyfit(residuals_a,D,1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "ax[0,ax_].plot(residuals_a, poly1d_fn(residuals_a), 'red', label='Local fit')\n",
    "ax[0,ax_].plot(xnew,np.median(trace.posterior['Intercept'].values[0,])+np.median(trace.posterior[v_].values[0,])*xnew,label='Model slope')\n",
    "m,b = np.polyfit(residuals_a,D, 1)\n",
    "[ax[0,ax_].plot(xnew, trace.posterior['Intercept'].values[0,][i]+trace.posterior[v_].values[0,][i]*xnew, alpha=0.05, c='black') for i in range(100)]\n",
    "ax[0,ax_].set_ylabel('Divorce rate')\n",
    "ax[0,ax_].set_xlabel(v_+' residuals (M->A)')\n",
    "ax[0,ax_].legend()\n",
    "\n",
    "v_ = 'Marriage rate'\n",
    "ax_ = 1\n",
    "ax[0,ax_].scatter(residuals_m,D)\n",
    "coef = np.polyfit(residuals_m,D,1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "ax[0,ax_].plot(residuals_m, poly1d_fn(residuals_m), 'red', label='Local fit')\n",
    "ax[0,ax_].plot(xnew,np.median(trace.posterior['Intercept'].values[0,])+np.median(trace.posterior[v_].values[0,])*xnew,label='Model slope')\n",
    "[ax[0,ax_].plot(xnew, trace.posterior['Intercept'].values[0,][i]+trace.posterior[v_].values[0,][i]*xnew, alpha=0.05, c='black') for i in range(100)]\n",
    "ax[0,ax_].set_ylabel('Divorce rate')\n",
    "ax[0,ax_].set_xlabel(v_+' residuals (A->M)')\n",
    "\n",
    "v_ = 'Marriage age'\n",
    "ax_ = 0\n",
    "ax[1, ax_].scatter(A,D)\n",
    "coef = np.polyfit(A,D,1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "ax[1,ax_].plot(A, poly1d_fn(A), 'red', label='Local fit')\n",
    "ax[1,ax_].plot(xnew,np.median(trace.posterior['Intercept'].values[0,])+np.median(trace.posterior[v_].values[0,])*xnew,label='Model slope')\n",
    "[ax[1,ax_].plot(xnew, trace.posterior['Intercept'].values[0,][i]+trace.posterior[v_].values[0,][i]*xnew, alpha=0.05, c='black') for i in range(100)]\n",
    "ax[1,ax_].set_ylabel('Divorce rate')\n",
    "ax[1,ax_].set_xlabel(v_)\n",
    "\n",
    "v_ = 'Marriage rate'\n",
    "ax_ = 1\n",
    "ax[1, ax_].scatter(M,D)\n",
    "coef = np.polyfit(M,D,1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "ax[1,ax_].plot(M, poly1d_fn(M), 'red', label='Local fit')\n",
    "ax[1,ax_].plot(xnew,np.median(trace.posterior['Intercept'])+np.median(trace.posterior[v_])*xnew,label='Model slope')\n",
    "[ax[1,ax_].plot(xnew, trace.posterior['Intercept'].values[0,][i]+trace.posterior[v_].values[0,][i]*xnew, alpha=0.05, c='black') for i in range(100)]\n",
    "ax[1,ax_].set_ylabel('Divorce rate')\n",
    "ax[1,ax_].set_xlabel(v_)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('machine.jpg');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So conditional on knowing marriage rate, marriage age still tells us something useful about divorce, but conditional on knowing marriage age, marriage rate tells us very little. Hence the difference in parameter estimates, with marriage age having a way bigger effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, while we have these residuals, let's take a look at their distribtuion and what they mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "tmp = ax[0].hist(residuals_a, label='Resid (A)')\n",
    "ax[0].plot(xnew,sp.stats.norm.pdf(xnew,0,trace_m.posterior['Sigma'].mean())*max(tmp[0]),label='Sigma (A)')\n",
    "ax[0].set_xlabel('A residuals')\n",
    "ax[0].legend()\n",
    "\n",
    "tmp = ax[1].hist(residuals_m, label='Resid (M)')\n",
    "ax[1].plot(xnew,sp.stats.norm.pdf(xnew,0,trace_a.posterior['Sigma'].mean())*max(tmp[0]),label='Sigma (M)')\n",
    "ax[1].set_xlabel('M residuals')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('residplot.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the residuals is the error distribution (`Sigma`) for the linear model - i.e. `Sigma` describes the magnitude of the deviations from the regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Posterior prediction plots\n",
    "\n",
    "Another important question is - how well is our model capturing the observed data? Are our predictions about each observation any good? Having used MCMC for our inference (and stored the values using a `pm.Determinisitc` node), we can just grab the observed and expected values and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of transposed traces for each state observation\n",
    "PostObs = pd.DataFrame(trace.posterior['Mu'].values[0,], columns=ddata.Location)\n",
    "PostObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected y values and UI's \n",
    "y = PostObs.median(0).values\n",
    "y_l95 = np.percentile(PostObs,2.5,axis=0)\n",
    "y_u95 = np.percentile(PostObs,97.5,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot expected vs observed\n",
    "plt.vlines(D, y_l95, y_u95, colors='grey')\n",
    "plt.scatter(D,y)\n",
    "plt.plot((-2,2),(-2,2),linestyle=\":\")\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Posterior')\n",
    "plt.tight_layout()\n",
    "plt.savefig('obs_ex.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that our model underpredicts high divorce rates (right side) and overpredicts low devorce rates (left side) but that is to be expected, it is a normal model after all an predictions tend to shrink toward the overall average. \n",
    "\n",
    "But it does look like there are some outlying values, let's label a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot expected vs observed\n",
    "plt.vlines(D, y_l95, y_u95, colors='grey')\n",
    "plt.scatter(D,y)\n",
    "plt.plot((-2,2),(-2,2),linestyle=\":\");\n",
    "\n",
    "\n",
    "# Label states that are >x SD off\n",
    "x = 1.3\n",
    "[plt.text(D[i],y[i],ddata.Location.values[i]) for i in np.arange(0,len(ddata.Location.values))[abs(D-y)>x]]\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Posterior')\n",
    "plt.tight_layout()\n",
    "plt.savefig('obs_ex2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counterfactual plots\n",
    "\n",
    "Counterfactuals are frequently brought up in statistical circles, and especially in economics, as a device to imagine what would happen if something else had happened in our data. In the case of counterfactual plots, they show us what happens if we manipulate one variable while keeping the others constant. \n",
    "\n",
    "If we return to the causal model where median marriage age influences divorce rate both directly and indirectly via marriage rate, we can develop a counterfactual plot by simulating from our `divorce` and `a_m` models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divorce model trace\n",
    "pm.summary(trace,var_names=['Intercept','Marriage rate','Marriage age','Sigma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_m model trace\n",
    "pm.summary(trace_m, var_names=['Intercept','Marriage age','Sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these values in place, we can see what the predicted change in divorce rate is across the full range of changes in median marriage age. To do this, we first choose the range of marriage ages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marriage age prediction range\n",
    "nsim = 100\n",
    "A_new = np.linspace(min(A),max(A),nsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the expected effect of marriage age on marriage rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marriage rates given marriage age range\n",
    "M_new = trace_m.posterior['Intercept'].values[0,]+trace_m.posterior['Marriage age'].values[0,]*A_new[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we simulate from the full `divorce` model, given our new (counterfactual) covariate values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divorce rates given marriage age range and manipulated marriage rate\n",
    "D_new = trace.posterior['Intercept'].values[0,]+trace.posterior['Marriage age'].values[0,]*A_new[:,None]+trace.posterior['Marriage rate'].values[0,]*M_new.mean(1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "ax[0].plot(A_new,M_new.mean(1))\n",
    "# Uncertainty intervals\n",
    "ax[0].plot(A_new,np.quantile(M_new,0.95,1),linestyle=\":\", c='blue')\n",
    "ax[0].plot(A_new,np.quantile(M_new,0.05,1),linestyle=\":\",  c='blue')\n",
    "ax[0].set_xlabel('Marriage age range')\n",
    "ax[0].set_ylabel('Manipulated marriage rate')\n",
    "ax[0].set_ylim(-3,2)\n",
    "\n",
    "\n",
    "# Expected trend\n",
    "ax[1].plot(A_new,D_new.mean(1))\n",
    "# Uncertainty intervals\n",
    "ax[1].plot(A_new,np.quantile(D_new,0.95,1),linestyle=\":\", c='blue')\n",
    "ax[1].plot(A_new,np.quantile(D_new,0.05,1),linestyle=\":\",  c='blue')\n",
    "\n",
    "ax[1].set_xlabel('Marriage age range')\n",
    "ax[1].set_ylabel('Counterfactual divorce rate')\n",
    "ax[1].set_ylim(-3,2)\n",
    "\n",
    "plt.savefig('counterfactual.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked relationships\n",
    "\n",
    "One of the many (many, many,...) pitfalls of statistical models is the presence of masked relationships - variables that counteract each other so they each appear to have no particular relationship. The primate milk data has just such a case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "mdata = pd.read_csv('milk.csv', sep=';')\n",
    "# Drop rows where neocortex percent is nan\n",
    "mdata = mdata[mdata['neocortex.perc'].notna()]\n",
    "# Add log(mass) column\n",
    "mdata['log(mass)'] = np.log(mdata.mass.values)\n",
    "mdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the bivariate relationships among variables, it seems there's not too much going on beyond the relationship between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(mdata, vars=['kcal.per.g','log(mass)','neocortex.perc'])\n",
    "g.map_upper(sns.scatterplot, s=15)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet if we run the full model for the relationship between log(mass) and neocortex.conc on kcal.per.g, we get a surprise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab variables of interest\n",
    "logMass = stdize(mdata['log(mass)'].values)\n",
    "neocorp = stdize(mdata['neocortex.perc'].values)\n",
    "kcal = stdize(mdata['kcal.per.g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian PyMC\n",
    "with pm.Model() as milker:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Intercept', 0, .2)\n",
    "    β1 = pm.Normal('log(mass)', 0, .5)\n",
    "    β2 = pm.Normal('neocortex_perc', 0, .5)\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Linear model\n",
    "    μ_ = β0+β1*logMass+β2*neocorp\n",
    "    \n",
    "    # Link function\n",
    "    μ = μ_*1\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kcal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with milker:\n",
    "    trace_milk = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_milk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this now? Both log(mass) and percent neocortex do not span zero, meaning they have strong relationships in the data. This can happen and is due to some unknown variable having synnergistic effects on both variables, but in different directions. Because they both happen they appear to not have any effect in a bivariate plot, but when both are present, their actual effects are revealed. Knowing that it can happen is half the battle. But it still sucks that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
