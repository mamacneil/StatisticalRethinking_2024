{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Lecture 1 - Conditional Manatees\n",
    "\n",
    "McElreath's lecture for today: https://www.youtube.com/watch?v=QhHfo6-Bx8o\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/statrethinking_winter2019\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information-based model comparsion\n",
    "\n",
    "With the information criteria outlined last week, we can now see how to use this information in practice to help diagnose problems in model specification. This is a really powerful result, so we'll take our time to step through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monkies\n",
    "\n",
    "We'll illustrate how to do this using the primate data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = pd.read_csv('Primates301.csv')\n",
    "mdata['Spp'] = (mdata.genus+' '+mdata.species).values\n",
    "mdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows missing longevity, brain size, or body mass\n",
    "mdata.dropna(subset=['longevity', 'brain', 'body'], inplace=True)\n",
    "mdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation we want to bulid a model looking at the influence of body mass and brain size on lifespan. We can represent the causal model using the `networkx` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Monkey DAG\n",
    "mDAG = nx.DiGraph()\n",
    "#mDAG.add_edges_from([(\"M\", \"L\"), (\"M\", \"B\"), (\"B\", \"L\"), (\"U\", \"M\"), (\"U\", \"L\")])\n",
    "mDAG.add_edges_from([(\"M\", \"L\"), (\"M\", \"B\"), (\"B\", \"L\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DAG\n",
    "nx.draw_networkx(mDAG, arrows=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('primateDAG.jpg',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're asserting that longer lifespans are caused by being bigger and having bigger brains (clever monkies). We can run three models to see which has the most WAIC-based support:\n",
    "\n",
    "$$\n",
    "M_{MB}: \\enspace \\enspace L \\sim N(\\beta_0+\\beta_M M+\\beta_B B, \\sigma) \\enspace \\enspace\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{M}: \\enspace \\enspace   L \\sim N(\\beta_0+\\beta_M M, \\sigma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{B}: \\enspace \\enspace  L \\sim N(\\beta_0+\\beta_B B, \\sigma)\n",
    "$$\n",
    "\n",
    "Looking piecewise at how much better things are when adding each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data\n",
    "\n",
    "# Body mass\n",
    "M = stdize(np.log(mdata.body.values))\n",
    "# Brain size\n",
    "B = stdize(np.log(mdata.brain.values))\n",
    "# Lifespan\n",
    "L = stdize(np.log(mdata.longevity.values))\n",
    "# Ratio of brain size to body size\n",
    "R = B/M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata.body.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full monkey\n",
    "with pm.Model() as M_MB:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Body mass effect\n",
    "    β1 = pm.Normal('M', 0, 0.5)\n",
    "    # Brain size\n",
    "    β2 = pm.Normal('B', 0, 0.5)\n",
    "    # Linear model\n",
    "    #μ = pm.Deterministic('mu',β0+β1*M+β2*B)\n",
    "    μ = β0+β1*M+β2*B\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampler\n",
    "with M_MB:\n",
    "    trace_mb = pm.sample(1000, idata_kwargs={'log_likelihood':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big monkey\n",
    "with pm.Model() as M_M:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Body mass effect\n",
    "    β1 = pm.Normal('M', 0, 0.5)\n",
    "    # Linear model\n",
    "    #μ = pm.Deterministic('mu',β0+β1*M)\n",
    "    μ = β0+β1*M\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampler\n",
    "with M_M:\n",
    "    trace_m = pm.sample(1000, idata_kwargs={'log_likelihood':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart monkey\n",
    "with pm.Model() as M_B:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Brain size\n",
    "    β1 = pm.Normal('B', 0, 0.5)\n",
    "    # Linear model\n",
    "    #μ = pm.Deterministic('mu',β0+β1*B)\n",
    "    μ = β0+β1*B\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sampler\n",
    "with M_B:\n",
    "    trace_b = pm.sample(1000, idata_kwargs={'log_likelihood':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAIC_compare = az.compare({'Full' : trace_mb,'Big' : trace_m,'Smart' : trace_b}, ic='waic', method='pseudo-BMA', scale='deviance')\n",
    "WAIC_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table gives lots of useful information for comparision - but also relative WAIC based model weight - which has the interpretation of the WAIC-based probability that Full is the best model (lowest KL-divergence) in the set of models compared. The calcualtion for this is\n",
    "\n",
    "$$\n",
    "weight_{i} = \\frac{exp(-\\frac{1}{2}\\Delta_{i})}{\\sum_{i=1}^{K}-\\frac{1}{2}\\Delta_{k})}\n",
    "$$\n",
    "\n",
    "where $\\Delta_{i}$ is the `elpd_diff` above, the difference in WAIC units between each model and the lowest WAIC model. From the table above we can use the `elpd_diff` values to do the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.exp(-WAIC_compare.elpd_diff)/sum(np.exp(-WAIC_compare.elpd_diff))).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not identical as Arviz uses some other calculation but close enough..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(WAIC_compare)\n",
    "plt.savefig('waiccompare.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we look at the WAIC model results, we get evidence that the full model (M_MB) and the smart (M_B) model have equal support, while the body mass model is far worse. What's going on? Well to figure it out let's look at the posteriors from all three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest([trace_mb, trace_m, trace_b], model_names=['Full','Big','Smart'], figsize=(10, 5))\n",
    "plt.axvline(0)\n",
    "plt.savefig('forestape.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see is that the effect of body size that is so strong in model `M_M` ('Big') gets negative in the full model (`M_MB`), while the effect of brain size in model `M_B` ('Smart') remains positive (although more uncertain). So why does body mass go negative in the joint model? Well to figure that out, we can look at the pointwise WAIC predictions between the Smart model and the joint model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model pointwise WAIC estimates\n",
    "pWAIC_mb = pm.waic(trace_mb, scale='deviance', pointwise=True)\n",
    "# Brain size model pointwise WAIC estimates\n",
    "pWAIC_b = pm.waic(trace_b, scale='deviance', pointwise=True)\n",
    "# Difference between them\n",
    "dWAIC = pWAIC_mb.waic_i.values-pWAIC_b.waic_i.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rplot = R\n",
    "Rplot[Rplot<0] = min(Rplot[Rplot>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Select species to label\n",
    "sppx = ['Cebus albifrons','Cebus capucinus', 'Cebus olivaceus',\n",
    "       'Gorilla gorilla', 'Lepilemur leucopus', 'Cacajao melanocephalus']\n",
    "sindx = [list(mdata.Spp.values).index(s) for s in sppx]\n",
    "\n",
    "[plt.scatter(d,l,s=50+r*30, facecolor='blue', alpha=0.3, edgecolor='black') for r,d,l in zip(Rplot,dWAIC,L)];\n",
    "[plt.text(d,l,s,) for l,d,s in zip(L[sindx],dWAIC[sindx],mdata.Spp.values[sindx])]\n",
    "plt.text(-0.01,-2.9,'<-- Full model better',horizontalalignment='right', weight='bold')\n",
    "plt.text(0.01,-2.9,'Smart model better -->', weight='bold')\n",
    "plt.xlabel('Pointwise difference in WAIC', fontsize=17)\n",
    "plt.ylabel('log(longevity) (std)', fontsize=17)\n",
    "plt.axvline(0,linestyle=':',c='black')\n",
    "plt.axhline(0,linestyle=':',c='black')\n",
    "plt.savefig('waicprime.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditionaily\n",
    "\n",
    "A couple of classic examples of conditioning on a collider can be seen in thinking about manatees\n",
    "\n",
    "\n",
    "<img src=\"manatees.jpg\" width=\"600\">\n",
    "\n",
    "Because of the observation of scars on so many manatees, and so many dying from boat strikes, Florida passed laws requring cowlings over the propellers of speadboats to save manatees. However the problem with this - other than helping with nasty scars - is that it doesn't deal with the cause of death in manatees, which is idiots running them over in shallow water and causing massive internal trauma. In other words \n",
    "\n",
    "\n",
    "A second favourite example comes from WWII, where [Abraham Wald](https://en.wikipedia.org/wiki/Abraham_Wald) was tasked with figuring out where to place the limited armour that could be added to bombers and other allied airplanes\n",
    "\n",
    "\n",
    "<img src=\"mkV.jpg\" width=\"600\">\n",
    "\n",
    "In [a series of memos](http://people.ucsc.edu/~msmangel/Wald.pdf) Wald figured out that rather than putting armour on areas of returning planes that had lots of bullet holes, it was correct to put the armour on areas that did not. \n",
    "\n",
    "\n",
    "<img src=\"holes.png\" width=\"600\">\n",
    "\n",
    "As for the manatees, by conditioning on survivors, the cause of death becomes obscured. \n",
    "\n",
    "\n",
    "# Interactions\n",
    "\n",
    "Interactions measure the influence of various predictors conditional on the other predictors in a model. Interactions can happen with the slope or the intercept or both. \n",
    "\n",
    "\n",
    "Let's start with a discrete interaction on a continuous variable. By way of example, let's have a look at the ruggedness of African nations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = pd.read_csv('rugged.csv', sep=\";\")\n",
    "adata.dropna(subset=['rgdppc_2000'], inplace=True)\n",
    "adata['Continent'] = np.array(['Non-African','African'])[adata.cont_africa.values]\n",
    "adata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the economic output of each country (measured by GDP, which is problematic, but anyhow...), the relationship between the ruggedness of the terrain and GDP is positive among African nations while being negative everywhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDP ratio relative to mean\n",
    "GDP = np.log(adata.rgdppc_2000.values)/np.mean(np.log(adata.rgdppc_2000.values))\n",
    "# Ruggedness\n",
    "RUG = (adata.rugged/max(adata.rugged)).values\n",
    "meanRUG = np.mean(RUG)\n",
    "# Africa index - equivlant to indexall(adata['Continent'])\n",
    "Ia, Continent = pd.factorize(adata['Continent'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab function to plot a line\n",
    "from numpy.polynomial.polynomial import polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(0,1,30)\n",
    "# Fit with polyfit\n",
    "b0, b1 = polyfit(RUG[Ia==0], GDP[Ia==0], 1)\n",
    "\n",
    "ax[0].scatter(RUG[Ia==0],GDP[Ia==0])\n",
    "ax[0].plot(xnew,b0+b1*xnew)\n",
    "ax[0].set_ylim(0.7, 1.35)\n",
    "ax[0].set_title('African nations', fontsize=16)\n",
    "ax[0].set_ylabel('log(GDP) (ratio to mean)', fontsize=16)\n",
    "ax[0].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "\n",
    "\n",
    "# Fit with polyfit\n",
    "b0, b1 = polyfit(RUG[Ia==1], GDP[Ia==1], 1)\n",
    "ax[1].scatter(RUG[Ia==1],GDP[Ia==1])\n",
    "ax[1].plot(xnew,b0+b1*xnew)\n",
    "ax[1].set_ylim(0.7, 1.35)\n",
    "ax[1].set_title('Non-African nations', fontsize=16)\n",
    "ax[1].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "plt.savefig('rugged.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to put together an interaction model we can start by guessing at some relatively benign priors and adding them to a model that allows for different intercepts and slopes for African/non-African nations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruggedness model\n",
    "with pm.Model(coords = {\"Cont\": Continent}) as rugged:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 1, 1, dims='Cont')\n",
    "    # Ruggedness effect\n",
    "    β1 = pm.Normal('Ruggedness', 0, 1, dims='Cont')\n",
    "    # Linear model\n",
    "    μ = β0[Ia]+β1[Ia]*RUG-meanRUG\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distribution\n",
    "ppd_ = pm.sample_prior_predictive(200, model=rugged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(0,1,30)\n",
    "\n",
    "ax[0].scatter(RUG[Ia==0],GDP[Ia==0])\n",
    "\n",
    "# Sample from prior predictive\n",
    "[ax[0].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[1], ppd_.prior['Ruggedness'][0].values.T[1])]\n",
    "\n",
    "ax[0].set_ylim(0.7, 1.35)\n",
    "ax[0].set_title('African nations', fontsize=16)\n",
    "ax[0].set_ylabel('log(GDP) (ratio to mean)', fontsize=16)\n",
    "ax[0].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "\n",
    "\n",
    "ax[1].scatter(RUG[Ia==1],GDP[Ia==1])\n",
    "# Sample from prior predictive\n",
    "[ax[1].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[0], ppd_.prior['Ruggedness'][0].values.T[0])]\n",
    "ax[1].set_ylim(0.7, 1.35)\n",
    "ax[1].set_title('Non-African nations', fontsize=16)\n",
    "ax[1].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "plt.savefig('rugged_ppd.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is a mess, let's try something tighter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruggedness model\n",
    "with pm.Model(coords = {\"Continent\": Continent}) as rugged:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 1, .1, dims='Continent')\n",
    "    # Ruggedness effect\n",
    "    β1 = pm.Normal('Ruggedness', 0, .1, dims='Continent')\n",
    "    # Linear model\n",
    "    μ = β0[Ia]+β1[Ia]*RUG-meanRUG\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distribution\n",
    "ppd_ = pm.sample_prior_predictive(200,model=rugged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(0,1,30)\n",
    "\n",
    "ax[0].scatter(RUG[Ia==0],GDP[Ia==0])\n",
    "\n",
    "# Sample from prior predictive\n",
    "[ax[0].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[1], ppd_.prior['Ruggedness'][0].values.T[1])]\n",
    "\n",
    "ax[0].set_ylim(0.7, 1.35)\n",
    "ax[0].set_title('African nations', fontsize=16)\n",
    "ax[0].set_ylabel('log(GDP) (ratio to mean)', fontsize=16)\n",
    "ax[0].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "\n",
    "\n",
    "ax[1].scatter(RUG[Ia==1],GDP[Ia==1])\n",
    "# Sample from prior predictive\n",
    "[ax[1].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[0], ppd_.prior['Ruggedness'][0].values.T[0])]\n",
    "ax[1].set_ylim(0.7, 1.35)\n",
    "ax[1].set_title('Non-African nations', fontsize=16)\n",
    "ax[1].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "plt.savefig('rugged_ppd2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humm, not bad, maybe a little too tight (missing stuff at the bottom), one more try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruggedness model\n",
    "with pm.Model(coords = {\"Continent\": Continent}) as rugged:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 1, 0.1, dims='Continent')\n",
    "    # Ruggedness effect\n",
    "    β1 = pm.Normal('Ruggedness', 0, 0.3, dims='Continent')\n",
    "    # Linear model\n",
    "    μ = β0[Ia]+β1[Ia]*RUG\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distribution\n",
    "ppd_ = pm.sample_prior_predictive(200,model=rugged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(0,1,30)\n",
    "\n",
    "ax[0].scatter(RUG[Ia==0],GDP[Ia==0])\n",
    "\n",
    "# Sample from prior predictive\n",
    "[ax[0].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[1], ppd_.prior['Ruggedness'][0].values.T[1])]\n",
    "\n",
    "ax[0].set_ylim(0.7, 1.35)\n",
    "ax[0].set_title('African nations', fontsize=16)\n",
    "ax[0].set_ylabel('log(GDP) (ratio to mean)', fontsize=16)\n",
    "ax[0].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "\n",
    "\n",
    "ax[1].scatter(RUG[Ia==1],GDP[Ia==1])\n",
    "# Sample from prior predictive\n",
    "[ax[1].plot(xnew,b0+b1*xnew, alpha=0.3, c='black') for b0,b1 in zip(ppd_.prior['Intercept'][0].values.T[0], ppd_.prior['Ruggedness'][0].values.T[0])]\n",
    "ax[1].set_ylim(0.7, 1.35)\n",
    "ax[1].set_title('Non-African nations', fontsize=16)\n",
    "ax[1].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "plt.savefig('rugged_ppd3.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, seems to get the balance of just a bit wider than is sensible. So let's put some data in and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugged:\n",
    "    trace_r = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_r)\n",
    "plt.savefig('rugged_posterior.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(0,1,30)\n",
    "\n",
    "ax[0].scatter(RUG[Ia==0],GDP[Ia==0])\n",
    "\n",
    "# Sample from prior predictive\n",
    "[ax[0].plot(xnew,b0+b1*xnew, alpha=0.01, c='black') for b0,b1 in zip(trace_r.posterior['Intercept'][0].values.T[0], trace_r.posterior['Ruggedness'][0].values.T[0])]\n",
    "\n",
    "ax[0].set_ylim(0.7, 1.35)\n",
    "ax[0].set_title('African nations', fontsize=16)\n",
    "ax[0].set_ylabel('log(GDP) (ratio to mean)', fontsize=16)\n",
    "ax[0].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "\n",
    "\n",
    "ax[1].scatter(RUG[Ia==1],GDP[Ia==1])\n",
    "# Sample from prior predictive\n",
    "[ax[1].plot(xnew,b0+b1*xnew, alpha=0.01, c='black') for b0,b1 in zip(trace_r.posterior['Intercept'][0].values.T[1], trace_r.posterior['Ruggedness'][0].values.T[1])]\n",
    "ax[1].set_ylim(0.7, 1.35)\n",
    "ax[1].set_title('Non-African nations', fontsize=16)\n",
    "ax[1].set_xlabel('Ruggedness (ratio)', fontsize=16)\n",
    "plt.savefig('rugged_fits.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net difference in ruggedness effect\n",
    "Net_rug = trace_r.posterior['Ruggedness'][0].values.T[0]-trace_r.posterior['Ruggedness'][0].values.T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Net_rug)\n",
    "plt.xlabel('Non-africa θ - Africa θ')\n",
    "plt.savefig('rugged_net.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-sum(Net_rug<0)/len(Net_rug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what this says is a couple of things:\n",
    "\n",
    "1. In flat places, GDP among African nations is a fraction (0.86) of flat Non-African (1.1) nations\n",
    "2. As ruggedness increases, African nations become wealthier while Non-African nations become poorer\n",
    "3. There is strong evidence of ruggedness having a more positive slope in African nations\n",
    "\n",
    "Point (2) bears thinking about. Why does this happen? Well it has a very dark answer that you'll explore in the homework this week.\n",
    "\n",
    "An important point about interactions is that they are ALWAYS difficult to interpret. One of the most basic problems is that they are statistically but not congnitively symmetric: \n",
    "\n",
    "    - *The effect of ruggedness on a nation's GDP depends on what continent it is from*\n",
    "    - *The effect of continent depends on ruggedness*\n",
    "    \n",
    "If you read these carefully, one will make complete sense and the other is nonsense. This is your causal brain understanding that you can't move nations among continents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloomin' tulips\n",
    "\n",
    "For a more comprehensive data, let's have a look at a continuous interaction, including combinations of water and shade on tulip blooming. \n",
    "\n",
    "First, let's import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata = pd.read_csv('tulips.csv',sep=';')\n",
    "bdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And knowing our flower husbandry, we can readily imagine that the quantities of water and light, as well as their combination, influencing the blooming of flowers. First we standardize the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response scaled to proportion of max\n",
    "B = bdata.blooms.values/max(bdata.blooms.values)\n",
    "# Water covariate - mean centred\n",
    "W = bdata.water.values-np.mean(bdata.water.values)\n",
    "# Shade covariate - mean centred\n",
    "S = bdata.shade.values-np.mean(bdata.shade.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need some priors - what would be sensible? Well now that blooms (B) has been scaled to be between 0 and 1, the intercept should be mostly in this range. To figure out what's within that range, we can first take a stab that the intercept will be at roughly 0.5 when water (W) and shade (S) are at their mean values. For the standard deviation we can then figure out what the cumulative probability is for 0 and 1, given a $N(0.5, \\sigma)$ distribtuion, with $\\sigma$ found by guessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with Normal(0.5, 1) at zero\n",
    "sp.stats.norm.cdf(0, 0.5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is too big, more than 30% of the prior would be below zero, maybe we should aim for the conventional 5% (which means 2.5% should be below zero and the other 2.5% above 1). How about $N(0.5, 0.25)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal(0.5, 0.25) at zero\n",
    "sp.stats.norm.cdf(0, 0.5, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal(0.5, 0.25) at one\n",
    "1-sp.stats.norm.cdf(1, 0.5, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good - ok onto the slopes. What's reasonable here? Well despite knowing there's likely an interaction effect, our ignorant, slightly wide prior should allow for either water or shade to account for the full 0 to 1 range in blooms. So to do this we can see how wide the range is for each (using the funny `np.ptp()` function in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak to peak range, the name of which comes from min/max calculations of waveforms\n",
    "np.ptp(W),np.ptp(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So over a range of 2 units, we need a prior that can span the full range from 0 to 1. This would imply a slope of -0.5 or 0.5 (i.e. $2*0.5=1$). So a sensible prior for $2SD=0.5$ would seem to be 0.25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal(0, 0.25) at -0.5\n",
    "sp.stats.norm.cdf(-0.5, 0, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal(0, 0.25) at 0.5\n",
    "1-sp.stats.norm.cdf(0.5, 0, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now putting this all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic bloom model\n",
    "with pm.Model() as bloom:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 0.5, 0.25)\n",
    "    # Water effect\n",
    "    β1 = pm.Normal('Water', 0, 0.25)\n",
    "    # Shade effect\n",
    "    β2 = pm.Normal('Shade', 0, 0.25)\n",
    "    # Linear model\n",
    "    μ = β0+β1*W+β2*S\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction bloom model\n",
    "with pm.Model() as ibloom:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 0.5, 0.25)\n",
    "    # Water effect\n",
    "    β1 = pm.Normal('Water', 0, 0.25)\n",
    "    # Shade effect\n",
    "    β2 = pm.Normal('Shade', 0, 0.25)\n",
    "    # Interaction effect\n",
    "    β3 = pm.Normal('ShadeWater', 0, 0.25)\n",
    "    # Linear model\n",
    "    μ = β0+β1*W+β2*S+β3*S*W\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distributions\n",
    "ppd_ib = pm.sample_prior_predictive(200,model=ibloom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_ib.prior['Water'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,3, figsize=(15,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(-1,1,30)\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 0\n",
    "sx = -1\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 1\n",
    "sx = 0\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 2\n",
    "sx = 1\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well these look like shit, maybe something a bit smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction bloom model\n",
    "with pm.Model() as ibloom:\n",
    "    # African/non intercepts\n",
    "    β0 = pm.Normal('Intercept', 0.5, 0.25)\n",
    "    # Water effect\n",
    "    β1 = pm.Normal('Water', 0, 0.1)\n",
    "    # Shade effect\n",
    "    β2 = pm.Normal('Shade', 0, 0.1)\n",
    "    # Interaction effect\n",
    "    β3 = pm.Normal('ShadeWater', 0, 0.1)\n",
    "    # Linear model\n",
    "    μ = β0+β1*W+β2*S+β3*S*W\n",
    "    # Error\n",
    "    σ = pm.Exponential('SD_obs', 1)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior predictive distributions\n",
    "ppd_ib = pm.sample_prior_predictive(200,model=ibloom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,3, figsize=(15,4))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(-1,1,30)\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 0\n",
    "sx = -1\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 1\n",
    "sx = 0\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 2\n",
    "sx = 1\n",
    "[ax[ix].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(ppd_ib.prior['Intercept'][0].values, ppd_ib.prior['Water'][0].values, ppd_ib.prior['Shade'][0].values, ppd_ib.prior['ShadeWater'][0].values)]\n",
    "ax[ix].set_ylim(0., 1.)\n",
    "ax[ix].set_title('Shade='+str(sx), fontsize=16)\n",
    "ax[ix].set_xlabel('Water', fontsize=16)\n",
    "ax[ix].set_ylabel('Blooms (scaled)', fontsize=16);\n",
    "plt.savefig('tulip_ppd.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem a bit better, let's run with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bloom:\n",
    "    trace_b = pm.sample(1000, idata_kwargs={'log_likelihood':True})\n",
    "with ibloom:\n",
    "    trace_ib = pm.sample(1000, idata_kwargs={'log_likelihood':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "# New data\n",
    "xnew = np.linspace(-1,1,30)\n",
    "# Number of samples to take\n",
    "ns = 20\n",
    "# Grab ns random samples\n",
    "Ix = np.random.uniform(0,len(trace_ib.posterior['Intercept'][0].values),ns).astype(int)\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 0\n",
    "iy = 0\n",
    "sx = -1\n",
    "\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx, alpha=0.3, c='black') for b0,b1,b2 in zip(trace_b.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Basic model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "\n",
    "# Sample from prior predictive\n",
    "iy = 1\n",
    "sx = 0\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx, alpha=0.3, c='black') for b0,b1,b2 in zip(trace_b.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Basic model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "\n",
    "# Sample from prior predictive\n",
    "iy = 2\n",
    "sx = 1\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx, alpha=0.3, c='black') for b0,b1,b2 in zip(trace_b.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Basic model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "\n",
    "\n",
    "\n",
    "# Sample from prior predictive\n",
    "ix = 1\n",
    "iy = 0\n",
    "sx = -1\n",
    "\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(trace_ib.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix], trace_ib.posterior['ShadeWater'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Interaction model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "\n",
    "\n",
    "# Sample from prior predictive\n",
    "iy = 1\n",
    "sx = 0\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(trace_ib.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix], trace_ib.posterior['ShadeWater'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Interaction model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "\n",
    "# Sample from prior predictive\n",
    "iy = 2\n",
    "sx = 1\n",
    "[ax[ix,iy].plot(xnew,b0+b1*xnew+b2*sx+b3*xnew*sx, alpha=0.3, c='black') for b0,b1,b2,b3 in zip(trace_ib.posterior['Intercept'][0].values[Ix], trace_ib.posterior['Water'][0].values[Ix], trace_ib.posterior['Shade'][0].values[Ix], trace_ib.posterior['ShadeWater'][0].values[Ix])]\n",
    "ax[ix,iy].set_ylim(0., 1.)\n",
    "ax[ix,iy].set_title('Interaction model: Shade='+str(sx), fontsize=16)\n",
    "ax[ix,iy].set_xlabel('Water', fontsize=16)\n",
    "ax[ix,iy].set_ylabel('Blooms (scaled)', fontsize=16)\n",
    "ax[ix,iy].scatter(W[S==sx],B[S==sx])\n",
    "plt.tight_layout()\n",
    "plt.savefig('tulip_fits.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with just one interaction we've got a heck of a lot going on. Nervous yet? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
