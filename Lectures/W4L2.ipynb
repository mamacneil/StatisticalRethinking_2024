{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Lecture 2 - Model comparison\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/statrethinking_winter2019\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From information theory to model comparsion\n",
    "\n",
    "The advances of Claude Shannon are huge, on par with those of Allen Turing or, some would argue, even Einstein, in that they came out of nowhere and were singular in their clear contribution to the modern world. A great article outlining his remarkable life can be found [here])https://spectrum.ieee.org/tech-history/cyberspace/claude-shannon-tinkerer-prankster-and-father-of-information-theory). You can grab the original paper [here](https://dl.acm.org/doi/pdf/10.1145/584091.584093?casa_token=0oyegsiCAF0AAAAA:xgmhPoVWitChIaYrP6ez36alK6jQpUCIBrScdLLSEk9brBveBYGl0B-pB8MByPz-DZSqM10Kw-_K). There is also a new documentary film: https://thebitplayer.com/\n",
    "\n",
    "![](shannon.jpg)\n",
    "\n",
    "\n",
    "However the path from information theory relating to how to encode things and send them over a wire and model comparison is a difficult one, so let's work carefully through how these things relate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information\n",
    "\n",
    "Defining information is an obscure concept, but here we'll look at Shannon's defnition and see how it plays out. First, the big insight is to ask *how much is our uncertainty reduced once we learn an outcome*. To figure this out we need to unpack this statement a bit:\n",
    "\n",
    "1. By *outcome* we mean some measurable thing that occurs as a result of some phenomenon\n",
    "\n",
    "2. By *uncertainty* we mean here the range of guesses or possibilities as to what the next outcome will be when it appears\n",
    "\n",
    "3. By *how much* we are asking for a measure or metric that quantifies a net change in uncertainty before and after an outcome is observed.\n",
    "\n",
    "Confused? Ok, well let's start with item `3`, which is Shannon's measure of *information entropy*, which states that **the uncertainty (H) contained in a probability distribution is the average log-probability (p) of an event,** which can be expressed as\n",
    "\n",
    "$$\n",
    "H(p) = -\\sum^{n}_{i=1}p_i log(p_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information entropy function\n",
    "def IE(p):\n",
    "    return -sum(p*np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to make this concrete, let's say we have a model for the weather, where the true probabilites are 0.3 for rain ($p_1$) and 0.7 for sun ($p_2$). So the total entropy (or uncertainty) in this situation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True probabilities of rain and sun\n",
    "p = np.array([0.3, 0.7])\n",
    "\n",
    "IE(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine however, a place where it either mostly rains (Glasgow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True probabilities of rain and sun\n",
    "p = np.array([0.9, 0.1])\n",
    "\n",
    "IE(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or where it is mostly sunny (LA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True probabilities of rain and sun\n",
    "p = np.array([0.1, 0.9])\n",
    "\n",
    "IE(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to the question of what kind of place has the greatest uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True probabilities of rain and sun\n",
    "p = np.array([0.5, 0.5])\n",
    "\n",
    "IE(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes total sense, if it mostly rains or is sunny, there is less uncertainty; if it is 50/50, then who knows? Probability is amazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence\n",
    "\n",
    "Now that we can measure the level of uncertainty inherent in a known probability distribution, the question is how can we use this to measure how well a model we might propose is from this truth (don't worry, we'll address the fact that we don't know the truth shortly)? Well, we can express the distance (in uncertainty units) between our model (q) and the true model (p) as *divergence*, expressed as the sum of the average distances between them\n",
    "\n",
    "$$\n",
    "D_{KL}(p,q) = -\\sum^{n}_{i=1}p_i(log(p_i)-log(q_i)).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler divergence\n",
    "def KLD(p,q):\n",
    "    return sum(p*(np.log(p)-np.log(q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measure is called the Kullback-Leibler divergence after [Solomon Kullback](https://en.wikipedia.org/wiki/Solomon_Kullback) and Richard Leibler, two crypto-analysts at the US National Security Agency who developed the measure in 1951. With the true model and our proposed model in hand, we can calculate the KL divergence for model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True probabilities of rain and sun\n",
    "p = np.array([0.3, 0.7])\n",
    "\n",
    "# Our ignorant guess as the true probabilites of rain and sun\n",
    "q1 = np.array([0.5, 0.5])\n",
    "\n",
    "# KL divergence\n",
    "KLD(p,q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best guess as the true probabilites of rain and sun\n",
    "q2 = np.array([0.2, 0.8])\n",
    "\n",
    "# KL divergence\n",
    "KLD(p,q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we propose a model that's closer to the truth than a coin flip, the KL divergence gets smaller. A key nuance in this calculation is that divergence is not symmetric (this is the mars-earth example in the book). If we reverse the true and proposed models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD(q2,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the level of surprise is lower. Why? Because with 0.2/0.8 as the 'true' model there is less 'surprise' ($D_{KL}=0.026$) in going from a less-certain set of conditions (earth, $q2=[0.2, 0.8]$) to a more-certain set of conditions (mars, $p=[0.3, 0.7]$) than there is ($D_{KL}=0.028$) in going from a more-certain set of conditions (mars, $p=[0.3, 0.7]$) to a less-certain set of conditions (earth, $q2=[0.2, 0.8]$). Nutty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative log-probability\n",
    "\n",
    "Ok, we now have a measure of the information distance between our model and the truth. Well big deal - we'll never know the truth ($p$), so what use is this? Well, while we'll never know the truth, we can calculate the KL divergence for a bunch of models. How? Well when comparing two models we can assume the truth is constant, meaning we can just sub in something sensible ($x_i$) for the $p_i$ values\n",
    "\n",
    "$$\n",
    "D_{KL}(p,q) = -\\sum^{n}_{i=1}x_i(log(x_i)-log(q_i)).\n",
    "$$\n",
    "\n",
    "Let's give this a try with our $q1$ and $q2$ models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "(KLD(x,q1)-KLD(x,q2))/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "(KLD(x,q1)-KLD(x,q2))/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 6\n",
    "(KLD(x,q1)-KLD(x,q2))/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference in log-probabilites\n",
    "abs(sum(np.log(q1))-sum(np.log(q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference in log-probabilites\n",
    "abs(sum(np.log(q2))-sum(np.log(q1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it really doesn't matter what the truth is, we can still compare the relative KL divergence of two models through their log-probabilities. So the log-probability score, or some variant of it, is the basis of *information criteria* used to compare model fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviance\n",
    "\n",
    "In a Bayesian context, with more complex models, things start to become more effort to keep track of because rather than a single point estimate of probability for each observation, we have a distribution of probability. But that aside, calculating the log-probabilities for each point can be done, through the *log-pointwise predictive density*:\n",
    "\n",
    "$$\n",
    "lppd(y|\\Theta) = \\sum^{}_{i}log\\frac{1}{S}\\sum_{s}p(y_i|\\Theta_s)\n",
    "$$\n",
    "\n",
    "which is simply calculating the log-probabilty of the data ($y$) given the set of parameters ($\\theta$) in the current iteration ($s$) of the sampler. We can do this for a set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "ddata = pd.read_csv('WaffleDivorce.csv',sep=\";\")\n",
    "# Display top 5 rows\n",
    "ddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize variables\n",
    "A = stdize(ddata.MedianAgeMarriage.values)\n",
    "M = stdize(ddata.Marriage.values)\n",
    "D = stdize(ddata.Divorce.values)\n",
    "W = stdize(ddata.WaffleHouses.values)\n",
    "S = stdize(ddata.South.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioning on Southerness\n",
    "with pm.Model() as Smod:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Waffle House effect\n",
    "    β1 = pm.Normal('WH', 0, 0.5)\n",
    "    # Southern\n",
    "    β2 = pm.Normal('S', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = pm.Deterministic('mu',β0+β1*W+β2*S)\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using A and M\n",
    "with pm.Model() as AMmod:\n",
    "    # Baseline intercept\n",
    "    β0 = pm.Normal('Intercept', 0, 0.2)\n",
    "    # Waffle House effect\n",
    "    β1 = pm.Normal('WH', 0, 0.5)\n",
    "    # Marriage age\n",
    "    β2 = pm.Normal('A', 0, 0.5)\n",
    "    # Marriage rate\n",
    "    β3 = pm.Normal('M', 0, 0.5)\n",
    "    # Linear model\n",
    "    μ = pm.Deterministic('mu',β0+β1*W+β2*A+β3*M)\n",
    "    # Error\n",
    "    σ = pm.Uniform('SD_obs', 0, 10)\n",
    "    # Likelihood\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Smod:\n",
    "    trace_s = pm.sample(1000, idata_kwargs={\"log_likelihood\": True})\n",
    "with AMmod:\n",
    "    trace_am = pm.sample(1000, idata_kwargs={\"log_likelihood\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `Deterministic` node, we're keeping track of mu for each observation, at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected value for mu at step 1\n",
    "trace_s.posterior['mu'][0][0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the current value for sigma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected value for sigma at step 1\n",
    "trace_s.posterior['SD_obs'][0].T[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then substitute these into the data likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.norm.pdf(D, trace_s.posterior['mu'][0][0].values, trace_s.posterior['SD_obs'][0].T[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the probability of each observation, given a normal distribution and the current expected values and sigma. In other words, the likelihood.\n",
    "\n",
    "Now the trick is to keep track of all these so that we can do the lppd calculation for each data point. First we iterate over the full trace to get these pointwise probabilities at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_s.posterior['SD_obs'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "nsamp = trace_s.posterior['SD_obs'].shape[1]\n",
    "# Number of datapoints\n",
    "nobs = len(D)\n",
    "# Empty matrix to hold values\n",
    "Slppd_ = np.zeros(shape=(nsamp,nobs))\n",
    "\n",
    "# Loop over iterations to grab pointwise likelihoods\n",
    "for i in range(nsamp):\n",
    "    Slppd_[i] = sp.stats.norm.pdf(D, trace_s.posterior['mu'][0][i], trace_s.posterior['SD_obs'][0].T[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do the calculation \n",
    "\n",
    "\n",
    "$$\n",
    "lppd(y|\\Theta) = \\sum^{}_{i}log\\frac{1}{S}\\sum_{s}p(y_i|\\Theta_s)\n",
    "$$\n",
    "\n",
    "using the values for each datapoint, and getting the log of their avergae for the Southern model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slppd = np.log(Slppd_.T.mean(1)).sum()\n",
    "Slppd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notation allows us to transpose Slppd, so that each row corresponds to the trace for each datapoint, then to take the mean of each row (the `1`) and sum them together. We can do the same process for the Marriage age/rate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty matrix to hold values\n",
    "AMlppd_ = np.zeros(shape=(nsamp,nobs))\n",
    "\n",
    "# Loop over iterations to grab pointwise likelihoods\n",
    "for i in range(nsamp):\n",
    "    AMlppd_[i] = sp.stats.norm.pdf(D, trace_am.posterior['mu'][0][i], trace_am.posterior['SD_obs'][0].T[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMlppd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMlppd = np.log(AMlppd_.T.mean(1)).sum()\n",
    "AMlppd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the marriage age and marriage rate model. Higher values here are better (they're more accurate), so the AM model has more support. Multiplying these values by -2 gives us model **deviance**, with smaller values (less deviant) being better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slppd*-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMlppd*-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIC\n",
    "\n",
    "AIC - that is Akaike's Information Criterion - has a long history in model comparision. It is defined as two times the number of parameters in the model minus the deviance:\n",
    "\n",
    "$$\n",
    "AIC = 2k-2log(\\hat{L})\n",
    "$$\n",
    "\n",
    "with $\\hat{L}$ being the maximum likelihood. In our case we can sub in the deviance value for the $-2log(\\hat{L})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC for Southern model\n",
    "2*4-2*Slppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC for marriage model\n",
    "2*5-2*AMlppd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's happened here? We need some sort of penalty for having added those extra parameters. The $2k$ penalty does this, telling us that the Marriage model has more support, given the data and considering the numbers of parameters used.\n",
    "\n",
    "All the various information criteria do some version of this, with various improvements over time. AIC was cutting edge 20 years ago, but has been completely replaced by WAIC (the widely-applicable information criteria), so called because it is more generalized. WAIC has an extra bit which is to use a penalty term proportional to the variance in the posterior predictions:\n",
    "\n",
    "\n",
    "$$\n",
    "WAIC = -2(lppd-\\sum_{i}var_\\theta log(y_i|\\theta))\n",
    "$$\n",
    "\n",
    "\n",
    "To do this by hand, we need to go back to the lppd matrix and store the variance of the posterior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance in log probabilites for each observation\n",
    "Slppd_sd = sum(np.log(Slppd_.T).std(1)**2)\n",
    "AMlppd_sd = sum(np.log(AMlppd_.T).std(1)**2)\n",
    "Slppd_sd,AMlppd_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the other bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Southern model WAIC\n",
    "WAICs = -2*(Slppd-Slppd_sd)\n",
    "# Marriage model WAIC\n",
    "WAICam = -2*(AMlppd-AMlppd_sd)\n",
    "\n",
    "WAICs,WAICam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows again that the marriage model has more support, given the data. \n",
    "\n",
    "Incidentally, PyMC does this calculation for you, using the `pm.waic()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.waic(trace_s, scale='deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.waic(trace_am, scale='deviance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loo-CV\n",
    "\n",
    "The information based criteria above are but one choice for assessment of relative model fits by scoring their overfitting risk. Another is cross-validation, the omission of one (or more) datapoints that are iteratively compared with their predicted values given a particular model. The average out of sample performace is, as it turns out, a good representation of the log-score of a model. What does this look like? Well similar to the lppd calculation above, it is the deviation between the single dropped observation ($y_i$) and the parameters estimated from the data that excluded $y_i$:\n",
    "\n",
    "$$\n",
    "lppd_{CV} = \\sum^{}_{i}log\\frac{1}{S}\\sum_{s}p(y_i|\\Theta_{-i,s})\n",
    "$$\n",
    "\n",
    "However this is computationally expensive to calculate - the number of datapoints times the number of iterations - so [Aki Vehtari](https://users.aalto.fi/~ave/) came up with Pareto-smoothed importance sampling (PSIS) as a very good approximation. It weights each sample by the inverse probabilty of the omitted observation, then takes their normalized sum as a new value, $lppd_{IS}$. As it turns out, the distribution of the largest weights calculated for each $y_i$ should have a [Pareto distribution](https://en.wikipedia.org/wiki/Pareto_distribution), with each $y_i$ having an estimated Pareto $k$ parameter. For observations with $k>0.7$, there is evidence the Pareto distribution is failing and that the observation is highly inflential, given the proposed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.loo(trace_s, scale='deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.loo(trace_am, scale='deviance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, model comparison...\n",
    "\n",
    "So now we understand the derivation of information criteria - what should we do with it? Model selection! Many people use information criteria for this but hold on - use of information criteria depends on your objectives. Information criteria is a measure of model fit - it has nothing to do with causal inference. Let's look back at our Waffle House models, the results of which are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_s,var_names=['WH','S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm.summary(trace_am,var_names=['WH','A','M'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing that the Marriage model still sees it as plausible that Waffle Houses have a positive effect on divorce rates, which is silly, and yet if we look at WAIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAICs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAICam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would greatly favour the Marriage model. Simple eh?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent time to outline this level of detail, please watch McElreath's lecture to see these calculations in context: https://www.youtube.com/watch?v=gjrsYDJbRh0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
