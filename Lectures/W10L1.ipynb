{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 Lecture 1 - Gaussian Processes\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=PIuqxOBJqLU&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=16\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "Dustin Stansbury has some lovely PyMC Code available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "def invlogit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "def Gauss2d(mu, cov, ci, ax=None, ec='k'):\n",
    "    \"\"\"Copied from statsmodel\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    v_, w = np.linalg.eigh(cov)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1]/u[0])\n",
    "    angle = 180 * angle / np.pi # convert to degrees\n",
    "\n",
    "    for level in ci:\n",
    "        v = 2 * np.sqrt(v_ * chi2.ppf(level, 2)) #get size corresponding to level\n",
    "        ell = Ellipse(mu[:2], v[0], v[1], 180 + angle, facecolor='None',\n",
    "                      edgecolor=ec,\n",
    "                      alpha=(1-level)*.5,\n",
    "                      lw=1.5)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes\n",
    "\n",
    "While linear models have got us a long way, and are typically fit for purpose in many contexts, we might also profitably bulid models that need a bit more wiggle to them. This can be in terms of both the mean function and aspects related to covariances. We saw how splines were built early on, but in terms of non-linearities we might be able to do a bit better, and estimate regularized functions of the data that don't break down into linear model components with arbitrary numbers of knots and basis functions. \n",
    "\n",
    "One particularly Bayesian way to do this is through the use of [Gaussian Processes](https://blog.dominodatalab.com/fitting-gaussian-process-models-python/) (GP's), which are in essence, distributions of functions. This might seem weird, but all we're doing in putting a GP prior in place is drawing the values for function f(x) from a multivarite normal distribution. PyMC is particuarly good for fitting GP means or covariances, with [great built in functions](https://www.pymc.io/projects/docs/en/stable/api/gp.html).\n",
    "\n",
    "So what are we talking about, well stealing [this example from the PyMC-learn site](https://pymc-learn.readthedocs.io/en/latest/notebooks/GaussianProcessRegression.html), we can simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 150 # The number of data points\n",
    "X = np.linspace(start = 0, stop = 10, num = n)[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "length_scale_true = 1.0\n",
    "signal_variance_true = 3.0\n",
    "cov_func = signal_variance_true**2 * pm.gp.cov.ExpQuad(1, length_scale_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of aseara\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(),\n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of Gaussian distributed noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "noise_variance_true = 2.0\n",
    "y = f_true + noise_variance_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend()\n",
    "plt.savefig('simgp.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty wiggly generating function, whose shape we could use a spline to estimate. But let's see what a GP would do with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as gp_fit:\n",
    "    # Matern function parameters for covariance of adjacent points\n",
    "    ρ = pm.Gamma('ρ', 1, 1)\n",
    "    η = pm.Gamma('η', 1, 1)\n",
    "    K = η * pm.gp.cov.Matern32(1, ρ)\n",
    "    \n",
    "    # Mean function - in this case zero, but could be a linear model etc\n",
    "    M = pm.gp.mean.Zero()\n",
    "    \n",
    "    # Gaussian process prior - M can be omitted because default is zero, and not much gained by adding non-zero fn\n",
    "    gp = pm.gp.Marginal(mean_func=M, cov_func=K)\n",
    "\n",
    "    \n",
    "    # Data likelihood\n",
    "    σ = pm.HalfCauchy('σ', 2.5)\n",
    "    y_obs = gp.marginal_likelihood('y_obs', X=X, y=y, noise=σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here? Well the Matern function characterizes how quickly correlations between adjacent observations decline, and that gets put into something called a GP Marginal distribution. So what's that? Well it's the prior distribtuion on the wiggly functions. Let's fit it to see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_fit:\n",
    "    trace = pm.sample(2000, n_init=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace, var_names=['ρ', 'σ', 'η']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.linspace(-2, 12, 100).reshape(-1, 1)\n",
    "with gp_fit:\n",
    "    fp = gp.conditional('fp', Z)\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=['fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Z, ppc.posterior_predictive['fp'].values[0].T, c='grey', alpha=0.1)\n",
    "plt.scatter(X, y, c='red')\n",
    "plt.xlim(-2,12)\n",
    "plt.savefig('gps.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at draws from the posterior, these are draws from a population of **functions**, rather than from parameters. It's insane, but dammit it works well. Let's have a look then at two examples from Rethinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oceanic tools again\n",
    "\n",
    "The oceanic tools example has, sitting behind it, the sea-going distances between islands, meaning that tool complexity is very likely impacted by how far each society is from other societies, implying a spatial dependence. There are many ways to deal with spatial strucuture but GP's are an increasingly common way to handle them, primarily because we can allow for spaital decay in relatedness among locations with functions on their covariances. Let's see how to do this with the `Kline.csv` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Oceanic Tools data\n",
    "kdata = pd.read_csv('Kline2.csv')\n",
    "dfi.export(kdata, 'kdata.jpg')\n",
    "kdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Distance matrix for Islands\n",
    "Dmat = pd.read_csv('islandsDistMatrix.csv', index_col=0)\n",
    "# Grab island names\n",
    "SocNames = Dmat.columns.values\n",
    "# Take a look\n",
    "tmp = Dmat.round(1)\n",
    "dfi.export(tmp, 'dmat.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools data itself we've already seen, however we now also have a distance matrix (in 1000's of km's) of how far each island is from the others. This is what we'll use for the covariance part of our variance-covariance (aka covariance) matrix.\n",
    "\n",
    "The model that incorporates this spaital structure builds on the 'scientific model' from Week 7, but now with the multivariate normal structure in there to include the distance matrix information. Repeated here from Week 8:\n",
    "\n",
    "*Scientific model*\n",
    "\n",
    "Based on domain knowledge - i.e. our understanding of the system under study - it should make sense that innovation in tool development increases with population size but with diminishing returns; eventually each additional person will add less innovation. Also cultures tend to discard tools over time, replacing them with new ones. These two processes can be represented by\n",
    "\n",
    "$$\n",
    "\\Delta T = \\alpha P^{\\beta} - \\gamma T\n",
    "$$\n",
    "\n",
    "where the change in tool number per time step ($\\Delta T$) is equal to some increase ($\\alpha$) proportional to population size, with diminishing returns ($\\beta$), and the per-unit-time rate of tool loss ($\\gamma$). If we then set $\\Delta T=0$, we'll get the equilibrium tool set size\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{T} = \\frac{\\alpha P^{\\beta}}{\\gamma}.\n",
    "$$\n",
    "\n",
    "Which we can encode this into a statistical model along with a new intercept for each island $k_i$, which is estimated from the distance matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_i \\sim & Poisson(\\lambda_i) \\\\\n",
    "\\lambda_i = & e^{k_i}\\frac{\\alpha P^{\\beta}}{\\gamma} \\\\\n",
    "\\left[\\begin{array}{c}\n",
    "k_{1} \\\\\n",
    "k_{2} \\\\\n",
    "... \\\\\n",
    "k_{10}\n",
    "\\end{array}\\right] = & MvN \\left(\\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "... \\\\\n",
    "0 \n",
    "\\end{array}\\right], K \\right) \\\\\n",
    "K_{ij} = & \\eta^2 e^{-\\rho^2 D^{2}_{ij}} + \\delta_{ij}\\sigma^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The critical developments come at the bottom, where average values for $k$ are $e^0=1$, meaning nothing changes in the the scientific model function at the average. Where $k<0$, $\\lambda$ decreases and where $k>0$, $\\lambda$ increases. \n",
    "\n",
    "Values for $k$ on each island are then altered by the covariance matrix $K$, which itself is defined by two additive parts. The second part $\\delta_{ij}\\sigma^2$ is just the variance within each society along the matrix diagonal, and zero elsewhere. The first bit however is the covariance part, with correlation $\\rho$ and distance matrix $D$, which says that the covariance between any two places, $i$ and $j$, declines exponentially with the square of the distance between them. Why the square? Because it makes a nice shape in terms of decay that allows for more rapid decline as distance increases than would be the linear case. If you think about setting off in a boat, things get far more difficult quite quickly as you get further from your home island. You can see the difference for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "# Distance range\n",
    "xrange = np.linspace(0, 4, 100)\n",
    "# Plot linear exponential decline\n",
    "ax.plot(xrange, np.exp(-1*xrange), 'k--', label='Linear')\n",
    "# Plot squared exponential decline\n",
    "ax.plot(xrange, np.exp(-1*xrange**2), 'k', label='Squared')\n",
    "ax.set_xlabel('distance')\n",
    "ax.set_ylabel('correlation')\n",
    "plt.legend()\n",
    "plt.savefig('decay.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these elements in place, we can add our GP for exponential decline into our scientific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total tools - response\n",
    "T = kdata.total_tools.values\n",
    "# Number of islands\n",
    "nsoc = len(T)\n",
    "\n",
    "# log-Population size\n",
    "P = kdata.logpop.values\n",
    "# Dummy for high-contact\n",
    "C,Ic = indexall(kdata.contact.values)\n",
    "\n",
    "# Distance matrix\n",
    "Dmat_ = Dmat.values\n",
    "# Squared distance matrix\n",
    "Dmatsq = np.power(Dmat_, 2)\n",
    "\n",
    "# Society index\n",
    "Is = np.arange(nsoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'Contact':C}) as SciToolsGP:\n",
    "    # Innovation rate\n",
    "    α = pm.Exponential('iRate(α)', 1)\n",
    "    # Diminishing returns\n",
    "    β = pm.Exponential('dReturns(β)', 1)\n",
    "    # Tool loss rate\n",
    "    γ = pm.Exponential('lRate(γ)', 1)\n",
    "    \n",
    "    # Maximum covariance\n",
    "    etasq = pm.Exponential('etasq', 2)\n",
    "    # Correlation\n",
    "    rhosq = pm.Exponential('rhosq', 0.5)\n",
    "    # Variance-Covariance\n",
    "    Kij = etasq*(pm.math.exp(-rhosq*Dmatsq)+np.diag([.01]*nsoc))\n",
    "    \n",
    "    # Distance-based effects\n",
    "    k = pm.MvNormal('k', mu=np.zeros(nsoc), cov=Kij, shape=nsoc)\n",
    "    \n",
    "    # Scientific distance model\n",
    "    λ = pm.math.exp(k[Is])*(α*P**β/γ)\n",
    "\n",
    "    # Likelihood\n",
    "    Yi = pm.Poisson('TotalTools', λ, observed=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SciToolsGP:\n",
    "    trace_t = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_t)\n",
    "dfi.export(tmp.style.background_gradient(), 'scitoolsgp.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, things sampled ok (we can likely do better by non-centering k) but for now we'll run with it and take a look at our decay function and see how our priors and posteriors compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plot\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "xrange = np.linspace(0, 10, 200)\n",
    "\n",
    "# Plot priors\n",
    "ax[0].plot(xrange, 2 * np.exp(-.5 * xrange**2), 'k', c='red')\n",
    "ax[0].plot(xrange, (np.random.exponential(2,100)[:, None] * np.exp(-np.random.exponential(0.5,100)[:, None] * xrange**2)).T,'k', alpha=.1)\n",
    "ax[0].set_ylim(0, 2)\n",
    "ax[0].set_xlabel('Distance (1000 km)')\n",
    "ax[0].set_ylabel('Covariance')\n",
    "ax[0].set_title('Prior')\n",
    "\n",
    "\n",
    "# Grab posteriors\n",
    "post_etasq = trace_t.posterior['etasq'].values[0]\n",
    "post_rhosq = trace_t.posterior['rhosq'].values[0]\n",
    "\n",
    "# Plot posteriors\n",
    "ax[1].plot(xrange, np.median(post_etasq) * np.exp(-np.median(post_rhosq) * xrange**2), 'k', c='red')\n",
    "ax[1].plot(xrange, (post_etasq[:100][:, None] * np.exp(-post_rhosq[:100][:, None] * xrange**2)).T, 'k', alpha=.1)\n",
    "ax[1].set_ylim(0, 2)\n",
    "ax[1].set_xlabel('Distance (1000 km)')\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_title('Posterior')\n",
    "plt.savefig('postdecay.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from this that the level of prior covariance has shrunk considerably, but this is difficult to visualize. To make this more concrete we can re-constitute the correlation matrix among societies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior median covariance among islands\n",
    "K_post = np.median(post_etasq) * (np.exp(-np.median(post_rhosq)*Dmatsq) + np.diag([.01]*nsoc))\n",
    "# Variance\n",
    "sigma_post = np.sqrt(np.diag(K_post))\n",
    "# Correlation matrix\n",
    "Rho = pd.DataFrame(np.diag(sigma_post**-1).dot(K_post.dot(np.diag(sigma_post**-1))), index=SocNames, columns=SocNames)\n",
    "tmp = Rho.round(2)\n",
    "dfi.export(tmp.style.background_gradient(), 'rho.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the distance-based correlation matrix, and to see exactly how it is working, we can plot these correlations in space, to see how spatially correlated things are, and we can plot tools against population, to see how these correlations play out for the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale point size to logpop\n",
    "logpop = P.copy()\n",
    "logpop /= P.max()\n",
    "psize = np.exp(logpop*5.5)\n",
    "\n",
    "# Calculate posterior median relationship, ignoring distance\n",
    "Nsamp, Nbin = 1000, 30\n",
    "log_pop_seq = np.linspace(6, 14, Nbin)\n",
    "a_post = trace_t.posterior['iRate(α)'].values[0][:, None]\n",
    "b_post = trace_t.posterior['dReturns(β)'].values[0][:, None]\n",
    "g_post = trace_t.posterior['lRate(γ)'].values[0][:, None]\n",
    "lambda_post = a_post*log_pop_seq**b_post/g_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot\n",
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#### Plot societies in space\n",
    "ax[0].scatter(kdata['lon2'], kdata['lat'], psize)\n",
    "# Grab names\n",
    "labels = kdata['culture'].values\n",
    "# Iterate over islands\n",
    "for i, itext in enumerate(labels):\n",
    "    ax[0].text(kdata['lon2'][i]+1, kdata['lat'][i]+1, itext)\n",
    "\n",
    "# Add lines shaded by Rho\n",
    "for i in range(10):\n",
    "    for j in np.arange(i+1, 10):\n",
    "        ax[0].plot([kdata['lon2'][i],kdata['lon2'][j]],[kdata['lat'][i], kdata['lat'][j]],'k-', alpha=Rho.iloc[i,j]**2, lw=2.5)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "\n",
    "\n",
    "#### Plot tools against population\n",
    "# Posterior predictions\n",
    "ax[1].plot(log_pop_seq, np.median(lambda_post, axis=0), '--', color='k')\n",
    "cix = 0.8\n",
    "az.plot_hdi(log_pop_seq, lambda_post, hdi_prob=cix, color='k', fill_kwargs={'alpha':cix*.5}, ax=ax[1])\n",
    "\n",
    "# plot raw data and labels\n",
    "ax[1].scatter(P, kdata['total_tools'], psize)\n",
    "labels = kdata['culture'].values\n",
    "for i, itext in enumerate(labels):\n",
    "    ax[1].text(P[i]+.1, kdata['total_tools'][i]-2.5, itext)\n",
    "    \n",
    "# Add correlations\n",
    "for i in range(10):\n",
    "    for j in np.arange(i+1, 10):\n",
    "        ax[1].plot([P[i], P[j]],[kdata['total_tools'][i], kdata['total_tools'][j]], 'k-', alpha=Rho.iloc[i, j]**2, lw=2.5)\n",
    "        \n",
    "ax[1].set_xlabel('log(population)')\n",
    "ax[1].set_ylabel('Total tools')\n",
    "ax[1].set_xlim(6.8, 12.8)\n",
    "ax[1].set_ylim(10, 73)\n",
    "plt.savefig('map.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phylogenetic distance\n",
    "\n",
    "Among the many recent obsessions of a small corner of the ecological world is the relationship between phylogeny and morphology - how various body parts, brain sizes etc vary with phylogenetic relatedness. Phylogeny is essentially like a distance between islands, but rather through evolutionary distance, and so we can develop a similar covariance structure to add 'known' phylogenetic distances into a model. \n",
    "\n",
    "For this example we'll look at the causal influence of group size on brain size, plus phylogeny, on primates, outlined on p.477 in Rehinking. The difference between this model and the two other GP's above is that we'll have both a covariance matrix and a linear model. What we're looking for, in effect, is if phylogeny has much to say about brain size (B) beyond group size (G) and body size (M) among the various species. What's special is - you guessed it - that there is a phylogenetic distance matrix in there to account for correlations among species. In notation the model is \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "B_i \\sim & MvN(\\mu_i, K) \\\\\n",
    "\\mu_i = & \\beta_0 + \\beta_1G_i + \\beta_2M_i \\\\\n",
    "K_{ij} = & \\eta^2 exp({\\rho^2D^2_{ij}}) \\\\\n",
    "\\beta_0 \\sim & N(0, 1) \\\\\n",
    "\\beta_1,\\beta_2 \\sim & N(0, 0.5) \\\\\n",
    "\\eta^2 \\sim & Exp(1) \\\\\n",
    "\\rho^2 \\sim & HalfNorm(3, 0.25)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With nearly the same distance matrix function for $K_{ij}$ seen in the Oceanic tools example. Note the prior for $\\eta^2$ is different than in McElreath's text (p483); the $HalfNormal(1, 0.25)$ he uses is very strong.\n",
    "\n",
    "First let's import the various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab primates data\n",
    "Primates301 = pd.read_csv(\"Primates301.csv\", sep=\";\")\n",
    "# Drop missing stuff\n",
    "pdata = Primates301.dropna(subset=[\"group_size\", \"body\", \"brain\"])\n",
    "# Keep track of spp names\n",
    "spp_names = pdata[\"name\"]\n",
    "# Total number of species\n",
    "nspp = len(spp_names)\n",
    "# Take a look\n",
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain size\n",
    "B = stdize(np.log(pdata.brain)).values\n",
    "# Body mass\n",
    "M = stdize(np.log(pdata.body)).values\n",
    "# Group size\n",
    "G = stdize(np.log(pdata.group_size)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can import the matrix of phylogentic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import distance matrix\n",
    "Dmat = pd.read_csv(\"Primates301_distance_matrix.csv\", index_col=0)\n",
    "# Scale to 0-1 and reorder distance matrix to match that of the species names\n",
    "Dmat_ord = (Dmat.loc[spp_names, spp_names] / Dmat.loc[spp_names, spp_names].max()).values\n",
    "tmp = pd.DataFrame(Dmat_ord)\n",
    "dfi.export(tmp.head(10), 'D.jpg',max_cols=10,)\n",
    "Dmat_ord\n",
    "Dmatsq = np.power(Dmat_ord, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these elements in place, we can code this into PyMC3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Phylo:\n",
    "    # Linear model priors\n",
    "    β0 = pm.Normal(\"Intercept\", 0.0, 1.0)\n",
    "    β1 = pm.Normal(\"BodyMass\", 0.0, 0.5)\n",
    "    β2 = pm.Normal(\"GroupSize\", 0.0, 0.5)\n",
    "    \n",
    "    # Linear model\n",
    "    mu = β0+β1*M+β2*G\n",
    "\n",
    "    # OU process priors\n",
    "    η2 = pm.Normal(\"etasq\", 1, 0.25)\n",
    "    ρ2 = pm.Normal(\"rhosq\", 3.0, 0.25)\n",
    "    \n",
    "    # Covariance (OU) function\n",
    "    K = η2*(pm.math.exp(-ρ2*Dmatsq))\n",
    "            \n",
    "    mu = pm.MvNormal('g', mu=mu, cov=K, shape=nspp)\n",
    "\n",
    "    # GP\n",
    "    σ = pm.Exponential(\"sigma\", 1.0)\n",
    "    Yi = pm.Normal(\"Yi\", mu, σ, observed=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Phylo:\n",
    "    trace_p = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_p, var_names=['Intercept','BodyMass','GroupSize','etasq','rhosq'])\n",
    "dfi.export(tmp.style.background_gradient(), 'phylotable.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_seq = np.linspace(0, Dmat_ord.max(), 100)\n",
    "post = trace_p.posterior.stack(sample=(\"chain\", \"draw\"))\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "# prior mean and 89% interval\n",
    "eta = np.random.exponential(1.0, 1000)\n",
    "rho = np.random.normal(3.0, 0.25, 1000)\n",
    "Kx = []\n",
    "for dx in d_seq:\n",
    "    Kx.append(eta * np.exp(-rho * dx))\n",
    "Kx = np.asarray(Kx)\n",
    "\n",
    "ax.plot(d_seq, Kx.mean(1), \"k\", alpha=.8, lw=2, label='Prior')\n",
    "az.plot_hdi(d_seq, Kx.T, color=\"k\", fill_kwargs={\"alpha\": .3}, ax=ax)\n",
    "\n",
    "# posterior\n",
    "#indx = np.random.choice(1000, 50)\n",
    "#post_etasq = post[\"etasq\"].values[indx][:, None]\n",
    "#post_rhosq = post[\"rhosq\"].values[indx][:, None]\n",
    "#ax.plot(d_seq, (post_etasq * np.exp(-post_rhosq * d_seq)).T, \"b\", alpha=0.1)\n",
    "#ax.plot(d_seq, (post[\"etasq\"].median().values * np.exp(-post[\"rhosq\"].median().values * d_seq)), c='b')\n",
    "Kp = []\n",
    "for d in d_seq:\n",
    "    Kp.append(post[\"etasq\"].values * np.exp(-post[\"rhosq\"].values * d))\n",
    "Kp = np.asarray(Kp)\n",
    "\n",
    "ax.plot(d_seq, Kp.mean(1), \"b\", alpha=.8, lw=2, label='Posterior')\n",
    "az.plot_hdi(d_seq, Kp.T, color=\"b\", fill_kwargs={\"alpha\": .3}, ax=ax)\n",
    "\n",
    "\n",
    "ax.set(xlabel=\"Phylogenetic distance\", ylabel=\"Covariance\")\n",
    "plt.legend()\n",
    "plt.savefig('PriorPost.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
