{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lecture 1 - Bayesian coding\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import random as rd\n",
    "import pymc as pm\n",
    "from matplotlib import cm\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalahari foragers example\n",
    "\n",
    "Let's import the Nancy Howell's data from the Kalahari people and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import data\n",
    "xdata = pd.read_csv('../Data/howell.csv')\n",
    "# Display top 5 rows\n",
    "xdata.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Table of descriptive statistics\n",
    "xdata.describe().T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Remove kids from dataframe\n",
    "kdata = xdata.loc[xdata.age >  17]\n",
    "kdata.describe().T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example, let's take a look at the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use pymc3 plotting features to plot the distribution of adult heights\n",
    "pm.plot_kde(kdata.height.values)\n",
    "plt.xlabel('Height (cm)', fontsize=17)\n",
    "plt.title('!Kung heights by Nancy Howell')\n",
    "plt.savefig('kungheight.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start our model with a basic (null) model, using a normal distribution to summarize the distribution of adult heights:\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "h_i \\sim Normal(\\mu,\\sigma)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "This describes the likelihood (or data distribution) part of the model, which being normal has two parameters that need priors:\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\begin{align*}\n",
    "h_i &\\sim Normal(\\mu,\\sigma) \\\\\n",
    "\\mu &\\sim Normal(178, 20) \\\\\n",
    "\\sigma &\\sim Uniform(0, 50)\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "or again, more succinctly,\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\begin{align*}\n",
    "h_i &\\sim N(\\mu,\\sigma) \\\\\n",
    "\\mu &\\sim N(178, 20) \\\\\n",
    "\\sigma &\\sim U(0, 50)\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "We can plot the distribution of these priors to see what they assume:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup multipanel figure\n",
    "nrows, ncols = 1, 2\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10,5))\n",
    "ax_ = axes.flatten()\n",
    "\n",
    "# Plot range of normal prior\n",
    "x = np.linspace(100, 250, 100)\n",
    "ax_[0].plot(x, sp.stats.norm.pdf(x, 178, 20))\n",
    "ax_[0].set_xlabel('μ', fontsize=17)\n",
    "ax_[0].set_title('μ~N(178,20)')\n",
    "\n",
    "# Plot range of sigma prior\n",
    "x = np.linspace(-10, 60, 100)\n",
    "ax_[1].plot(x, sp.stats.uniform.pdf(x, 0, 50))\n",
    "ax_[1].set_xlabel('σ', fontsize=17)\n",
    "ax_[1].set_title('σ~U(0, 50)')\n",
    "\n",
    "plt.savefig('kungpriors.jpg', dpi=200);\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a model - a likelihood and some priors - from which we can simulate, even though we have yet to see any data. To do this, we'll draw 1000 samples from our priors, then fire them into a normal and store the value at each iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number of samples\n",
    "nsamp = 1000\n",
    "# Grab samples from \n",
    "mu_ = np.random.normal(178, 20, nsamp)\n",
    "sigma_ = np.random.uniform(0, 50, nsamp)\n",
    "h_ = np.random.normal(mu_, sigma_)\n",
    "pm.plot_kde(h_)\n",
    "plt.xlabel('Height (cm)', fontsize=17)\n",
    "plt.yticks([])\n",
    "plt.title('hi~N(μ,σ) Prior predictive distribution')\n",
    "plt.savefig('kungpriorsim.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this gives us is some idea about how realistic our results are in terms of the **a priori** allowable height values for adults. Are these reasonable? Well we could add lines to indicate some known information - the tallest ever person ([Robert Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow)) who topped out at 272 cm:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number of samples\n",
    "nsamp = 1000\n",
    "# Grab samples from \n",
    "mu_ = np.random.normal(178, 20, nsamp)\n",
    "sigma_ = np.random.uniform(0, 50, nsamp)\n",
    "h_ = np.random.normal(mu_, sigma_)\n",
    "pm.plot_kde(h_)\n",
    "plt.xlabel('Height (cm)', fontsize=17)\n",
    "plt.axvline(x=272, c='black')\n",
    "#plt.yticks([])\n",
    "plt.text(275, 0.010, \"Robert \\n Wadlow \\n (8'11'')\")\n",
    "plt.title('hi~N( N(178, 20) , U(0, 50)) Prior predictive distribution')\n",
    "plt.text(275, 0.0011, \"P(>Wadlow) \\n =0.01\")\n",
    "plt.yticks([])\n",
    "plt.savefig('kungpriorsim2.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which amounts to 1.3% percent of our a priori people being taller than the tallest ever person:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sum(h_>272)/len(h_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about those wide 'uninformative' priors I've heard so much about? Well, this poses a problem for heights, which by definition can't go below zero. If we use an $N(178,1000)$ prior, this is what happens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number of samples\n",
    "nsamp = 1000\n",
    "# Grab samples from \n",
    "mu_ = np.random.normal(178, 1000, nsamp)\n",
    "sigma_ = np.random.uniform(0, 50, nsamp)\n",
    "h_ = np.random.normal(mu_, sigma_)\n",
    "pm.plot_kde(h_)\n",
    "plt.xlabel('heights', fontsize=17)\n",
    "plt.axvline(x=272, c='black')\n",
    "#plt.text(275, 0.010, \"Robert \\n Wadlow \\n (8'11'')\")\n",
    "plt.title('hi~N( N(178, 1000) , U(0, 50)) Prior predictive distribution')\n",
    "plt.text(300, 0.00, \"P(>Wadlow) \\n =0.5\")\n",
    "plt.yticks([])\n",
    "plt.savefig('kungpriorsim3.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sum(h_>272)/len(h_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So both the mean **and the variance** of our priors matter. We should be skeptical of them, and test their implications before we hit the inference button. Given our original $N(178,20)$ prior, let's use the grid approximation for one last time to see what our likelihood surface looks like. To do this we can use the handy `griddata()` function in the `scipy.interpolate` package. To take a look at what it does we can use a `?`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "?griddata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can then evaluate all the combinations of μ and σ across our entire grid:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bulid numpy grid from 100 to 260, against 4 to 9, in steps of 1\n",
    "pgrid = np.mgrid[100:260:1, 4:10:1].reshape(2,-1).T\n",
    "pgrid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Look at grid\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xticks(pgrid.T[0] , minor=True)\n",
    "ax.set_yticks(pgrid.T[1] , minor=True)\n",
    "ax.set_xlim(100,260)\n",
    "ax.set_ylim(4,9)\n",
    "ax.grid(True, which='both')\n",
    "plt.savefig('gridx.jpg', dpi=200);\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bulid numpy grid from 100 to 260, against 4 to 9, in steps of 1\n",
    "pgrid = np.mgrid[100:260:1, 4:10:1].reshape(2,-1).T\n",
    "pgrid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the priors and calcualte their likelihoods (confusing name alert), starting with the first two pairs of values on the grid"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate prior likelihood for first pair of values on the grid\n",
    "sp.stats.norm.logpdf(pgrid[0,0], loc=178, scale=20), sp.stats.uniform.logpdf(pgrid[0,1], loc=0, scale=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sp.stats.norm.logpdf(pgrid[0,0], loc=178, scale=20)+sp.stats.uniform.logpdf(pgrid[0,1], loc=0, scale=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate prior likelihood for second pair of values on the grid\n",
    "sp.stats.norm.logpdf(pgrid[1,0], loc=178, scale=20), sp.stats.uniform.logpdf(pgrid[1,1], loc=0, scale=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate prior likelihood for last pair of values on the grid\n",
    "sp.stats.norm.logpdf(pgrid[-1,0], loc=178, scale=20), sp.stats.uniform.logpdf(pgrid[-1,1], loc=0, scale=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sum\n",
    "sp.stats.norm.logpdf(pgrid[-1,0], loc=178, scale=20)+ sp.stats.uniform.logpdf(pgrid[-1,1], loc=0, scale=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Calculate priors\n",
    "prior_loglike = sp.stats.norm.logpdf(pgrid[:,0], loc=178, scale=20) + sp.stats.uniform.logpdf(pgrid[:,1], loc=0, scale=50)\n",
    "prior_loglike"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically call these values the **prior probabilty** (likelihood here though, as we haven't standardized), so what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grab range of values across full range of grid for mu and sigma\n",
    "xi = np.linspace(pgrid[:,0].min(), pgrid[:,0].max(), 100)\n",
    "yi = np.linspace(pgrid[:,1].min(), pgrid[:,1].max(), 100)\n",
    "# Interpolate prior values across grid given the ranges above\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Create a mu by sigma grid\n",
    "X = np.arange(100, 260, 1)\n",
    "Y = np.arange(4, 10, 1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "# Calculate prior likelihood on grid\n",
    "Z = sp.stats.norm.logpdf(X, loc=178, scale=20) + sp.stats.uniform.logpdf(Y, loc=0, scale=50)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Contour plot of results\n",
    "#plt.contour(xi, yi, zi)\n",
    "plt.xlabel('μ', fontsize=17)\n",
    "plt.ylabel('σ', fontsize=17)\n",
    "plt.title('Prior likelihood')\n",
    "plt.savefig('priors.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a flat top because in the σ dimension we have a uniform distribution; although odd, this is a 'heat' contour, so it's 'red hot' peak is at 178, regardless of the value of σ.\n",
    "\n",
    "Ok, we have the prior likelihood, now we need the data likelihood - substituting in the grid pair values for the mean and standard deviation for each recorded height. Let's start with the first pair on the grid and the first height value:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "xdata.height.values[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sp.stats.norm.logpdf(kdata.height.values[0], loc=pgrid[:,0][0], scale=pgrid[:,1][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate for first pair of values on the grid\n",
    "sum(sp.stats.norm.logpdf(kdata.height, loc=pgrid[:,0][0], scale=pgrid[:,1][0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate for last pair of values on the grid\n",
    "sum(sp.stats.norm.logpdf(kdata.height, loc=pgrid[:,0][-1], scale=pgrid[:,1][-1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate likelihood (data distribution) for a normal (on the log scale) for each point in the grid using the stats part of scipy, \n",
    "# which uses location for mean, and scale for SD (due to lots of ML people using scipy)\n",
    "log_likelihood = np.array([sum(sp.stats.norm.logpdf(kdata.height, loc=pgrid[:,0][i], scale=pgrid[:,1][i])) for i in range(len(pgrid))])\n",
    "log_likelihood"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can take a look at the **likelihood surface**, which is where frequentist analysis stops "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Grab range of values across full range of grid for mu and sigma\n",
    "xi = np.linspace(pgrid[:,0].min(), pgrid[:,0].max(), 100)\n",
    "yi = np.linspace(pgrid[:,1].min(), pgrid[:,1].max(), 100)\n",
    "\n",
    "# Calculate data likelihood on grid\n",
    "Zll = np.array([[sum(sp.stats.norm.logpdf(kdata.height, loc=x_, scale=y_)) for x_,y_ in zip(x,y)] for x,y in zip(X,Y)])\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Zll, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "plt.xlabel('μ', fontsize=17)\n",
    "plt.ylabel('σ', fontsize=17)\n",
    "plt.title('Data likelihood')\n",
    "plt.savefig('likelihood_surface.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimate\n",
    "pgrid[log_likelihood==max(log_likelihood)], max(log_likelihood)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have log-scale data likelhood and prior probability values, we can **add** them together to calculate the numerator of Bayes theorem:\n",
    "\n",
    "$$\n",
    "\\text{Posterior} = \\frac{\\text{likelihood x prior}}{\\text{normalizing constant}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Un-normalized posterior\n",
    "post_num = log_likelihood + prior_loglike"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Normalized posterior - remember we're doing log-probability calculations here, so don't foget to exponentiate!\n",
    "# Also in log-land, the division becomes subtraction, and the normalization is relative to the largst value (for plotting)\n",
    "posterior = np.exp(post_num - max(post_num))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can take a look at what the posterior surface looks like:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grab range of values across full range of grid for mu and sigma\n",
    "xi = np.linspace(pgrid[:,0].min(), pgrid[:,0].max(), 100)\n",
    "yi = np.linspace(pgrid[:,1].min(), pgrid[:,1].max(), 100)\n",
    "# Interpolate prior values across grid given the ranges above\n",
    "zi = griddata((pgrid[:,0], pgrid[:,1]), posterior, (xi[None,:], yi[:,None]))\n",
    "# Contour plot of results\n",
    "plt.contour(xi, yi, zi)\n",
    "plt.xlabel('μ', fontsize=17)\n",
    "plt.ylabel('σ', fontsize=17)\n",
    "plt.title('Posterior')\n",
    "plt.savefig('posterior.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grab range of values across full range of grid for mu and sigma\n",
    "xi = np.linspace(pgrid[:,0].min(), pgrid[:,0].max(), 100)\n",
    "yi = np.linspace(pgrid[:,1].min(), pgrid[:,1].max(), 100)\n",
    "# Interpolate prior values across grid given the ranges above\n",
    "zi = griddata((pgrid[:,0], pgrid[:,1]), posterior, (xi[None,:], yi[:,None]))\n",
    "# Contour plot of results\n",
    "plt.contour(xi, yi, zi)\n",
    "plt.xlabel('μ', fontsize=17)\n",
    "plt.ylabel('σ', fontsize=17)\n",
    "plt.xlim(150,160)\n",
    "plt.ylim(7,9)\n",
    "plt.title('Posterior')\n",
    "\n",
    "plt.savefig('posterior_zoom.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is what's known as the **joint posterior** density for μ and σ, evaluated using grid approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So congradulations - we've calcualted the mean and standard deviation of some data, but in a really important way (*i.e.* using Bayes theorem). It's important to keep in mind that while means and variacnes are things we can easily calculate using math (hence their widespread use), they still constitute a fully Bayesian model for the height data. **Plus** we get uncertainty estimates for μ and σ, which is important. \n",
    "\n",
    "Next we'll add some complexity to our model - a covariate of weight - to do what a more typical kind of linear regression. First, let's see what sort of relationship we have between these variables:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.scatter(kdata.weight, kdata.height)\n",
    "plt.xlabel('Weight (kg)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('scatter.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks linear enough. Now let's write out a linear model for this:\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\begin{align*}\n",
    "h_i &\\sim N(\\mu_i,\\sigma) \\\\\n",
    "\\mu_i &= \\beta_0 + \\beta_1(x_i - \\bar{x}) \\\\\n",
    "\\beta_0 &\\sim N(178,20) \\\\\n",
    "\\beta_1 &\\sim N(0,10) \\\\\n",
    "\\sigma &\\sim U(0, 50)\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "So, as before, let's simulate and see if we have reasonable priors. This time we are simulating possible lines to describe the relationship between weight and height."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Number of samples\n",
    "nsamp = 100\n",
    "# Intercept - estimated height at popultion average weight\n",
    "β0_ = np.random.normal(178, 20, nsamp)\n",
    "# Slope - relationship between weight and height\n",
    "β1_ = np.random.normal(0, 10, nsamp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grab range of weights to plot over\n",
    "meanweight = np.mean(kdata.weight)\n",
    "weights_ = np.linspace(min(kdata.weight),max(kdata.weight),50)-meanweight\n",
    "\n",
    "# Plot resulting lines given sample values for β0 and β1, using a list comprehension\n",
    "[plt.plot(weights_+meanweight, b0+b1*weights_, c='black', alpha=0.1) for b0,b1 in zip(β0_,β1_)]\n",
    "\n",
    "# Add min and max human heights\n",
    "plt.axhline(272)\n",
    "plt.axhline(0)\n",
    "\n",
    "# Make it look nice\n",
    "plt.ylim(-100,400)\n",
    "plt.xlabel('weight (kg)', fontsize=17)\n",
    "plt.ylabel('height (cm)', fontsize=17)\n",
    "plt.title('β1~N(0,10) Prior predictive simulation')\n",
    "plt.savefig('priorPlinear.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that people can't be less than zero or more than 272 cm tall, so our model is a priori quite wrong, and we have the opportunity to do a bit better, using our 'domain knowledge' (external information, expertise, common sense etc.) to do a bit better. Given that we know that the relationship between weight and height in people is positive, we can do a bit better. McElreath suggests a log-normal prior because it is by definition constrained to be above zero. Let's try a $log-Normal(0, 1)$ prior and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# New slope prior for the relationship between weight and height\n",
    "β1_ = np.random.lognormal(0, 1, nsamp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot new lines given sample values for β0 and β1, using a list comprehension\n",
    "[plt.plot(weights_+meanweight, b0+b1*weights_, c='black', alpha=0.1) for b0,b1 in zip(β0_,β1_)]\n",
    "\n",
    "# Add min and max human heights\n",
    "plt.axhline(272)\n",
    "plt.axhline(0)\n",
    "\n",
    "# Make it look nice\n",
    "plt.ylim(-100,400)\n",
    "plt.xlabel('weight (kg)', fontsize=17)\n",
    "plt.ylabel('height (cm)', fontsize=17)\n",
    "plt.title('β1~logN(0,1) Prior predictive simulation')\n",
    "plt.savefig('priorPlinear2.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while there are some extreme values still possible for max heights, the lower values are constrained to be positive, which is much better. Incidentally plotting a hundred or more lines from some distribtuion is a very Bayesian way to get a look at uncertainty - they tend to illustrate odd cases such as above and are **way** easier to plot than formal uncertainty intervals (credible intervals) around the mean trendline.\n",
    "\n",
    "With reasonable priors in hand we can now use the quadratic approximation to calculate our posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.scatter(kdata.weight.values-meanweight,kdata.height.values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sampling using MCMC in PyMC\n",
    "with pm.Model() as kalahari2:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Average_height', 178, 20)\n",
    "    β1 = pm.Lognormal('Weight_slope', 0, 1)\n",
    "    σ = pm.Uniform('Obs_sd', 0, 50)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0+β1*(kdata.weight.values-meanweight)\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kdata.height.values)\n",
    "    \n",
    "    trace_k = pm.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.hist(trace_k.posterior.Average_height.values.flatten());"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grab median value from the posterior of average height\n",
    "b0 = trace_k.posterior.Average_height.median().values\n",
    "# Grab median value from the posterior of the effect of weight on height\n",
    "b1 = trace_k.posterior.Weight_slope.median().values\n",
    "# Grab median value from the posterior of the variability around the line\n",
    "sig = trace_k.posterior.Obs_sd.median().values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have some parameter estimates, lets see how this fits to our data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.scatter(kdata.weight, kdata.height)\n",
    "plt.plot(weights_+meanweight, b0+b1*weights_, c='black')\n",
    "plt.xlabel('Weight (kg)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('fitline.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks not too bad, how about those uncertainty bounds?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.scatter(kdata.weight, kdata.height)\n",
    "y_ = b0+b1*weights_\n",
    "y_uu = y_+sig*2\n",
    "y_ul = y_-sig*2\n",
    "plt.plot(weights_+meanweight, y_, c='black')\n",
    "plt.plot(weights_+meanweight, y_uu, \":\", c='black')\n",
    "plt.plot(weights_+meanweight, y_ul, \":\", c='black')\n",
    "plt.xlabel('Weight (kg)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('fitlines.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do those uncertainty bounds represent?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create wireframe positions\n",
    "sx = weights_.size\n",
    "# Grab every 5th element in ynew\n",
    "yrangex = y_[::5]\n",
    "sy = yrangex.size\n",
    "xnew_w = np.tile(weights_, (sy, 1))\n",
    "ynew_w = np.tile(yrangex, (sx, 1)).T\n",
    "\n",
    "# Wireframe lines\n",
    "Z = sp.stats.norm.pdf(xnew_w,loc=0,scale=sig)\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "surf = ax.plot_wireframe(xnew_w+((ynew_w-b0)/b1)+meanweight, ynew_w, Z, cstride=1000)\n",
    "ax.scatter(kdata.weight, kdata.height)\n",
    "ax.plot(weights_+meanweight, y_, c='black')\n",
    "ax.set_xlabel('Weight (kg)')\n",
    "ax.set_ylabel('Height (cm)')\n",
    "plt.savefig('wireframe.jpg', dpi=200);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! We have fit our second Bayesian linear regression model (the first that looks like a line). You should feel proud, this is a big foundation on which to build your skilz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
