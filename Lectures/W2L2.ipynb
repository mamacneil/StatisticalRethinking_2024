{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Lecture 2 - Geocentric models and wiggly orbits\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import random as rd\n",
    "import pymc as pm\n",
    "import patsy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !Kung Kids\n",
    "\n",
    "Let's import the Nancy Howell's data from the Kalahari people and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "kdata = pd.read_csv('howell.csv')\n",
    "# Display top 5 rows\n",
    "kdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "kdata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pymc plotting features to plot the distribution of adult heights\n",
    "pm.plot_kde(kdata.height.values)\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.title('!Kung heights from Nancy Howell')\n",
    "plt.savefig('fullkung.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long tails here, what's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colours for kids\n",
    "xcol = np.array(['grey','dodgerblue'])\n",
    "# Dummy for kids\n",
    "Ik = (kdata.age>17)*1\n",
    "plt.scatter(kdata.weight, kdata.height, c=xcol[Ik])\n",
    "plt.xlabel('Weight (kg)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('heightweight.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, those pesky kids. So our glorious linearity among the adults is now gone. So how should we model this relationship?\n",
    "\n",
    "\n",
    "# Curves from 'linear models'\n",
    "\n",
    "### Link functions\n",
    "    - common\n",
    "    - ok\n",
    "\n",
    "### Polyomial regression\n",
    "    - common\n",
    "    - bad\n",
    "\n",
    "### Splines\n",
    "    - very flexible\n",
    "    - highly geocentric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link functions\n",
    "\n",
    "Link functions are present in every statistical model as they translate the model scale - which can be log-odds for example - onto the observation scale. A log-link or logit-link are common choices. Let's see what a log link does to our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colours for kids\n",
    "xcol = np.array(['grey','dodgerblue'])\n",
    "# Dummy for kids\n",
    "Ik = (kdata.age>17)*1\n",
    "plt.scatter(np.log(kdata.weight), kdata.height, c=xcol[Ik])\n",
    "plt.xlabel('log(Weight) (kg)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('logheightweight.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Near linearity - very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "If you care not about any sort of process but are confident in the relative shape of what you want, a polynomial can **sometimes** be worth a look. \n",
    "\n",
    "$1^{st}$ order (line): $\\mu_i = \\beta_0+\\beta_1 x_i$\n",
    "\n",
    "$2^{nd}$ order (parabola): $\\mu_i = \\beta_0+\\beta_1 x_i+\\beta_2 x^{2}_i$\n",
    "\n",
    "$3^{rd}$ order (cubic): $\\mu_i = \\beta_0+\\beta_1 x_i+\\beta_2 x^{2}_i+\\beta_3 x^{3}_i$\n",
    "\n",
    "$n^{th}$ order (insanity): $\\mu_i = \\beta_0+\\beta_1 x_i+\\beta_2 x^{2}_i+...+\\beta_n x^{n}_i$\n",
    "\n",
    "And it goes on...with increasingly uninterpretable results. Even the parabolic parameters are not individually interpretable, so be careful!!\n",
    "\n",
    "\n",
    "## Polynomial !Kungs\n",
    "\n",
    "So let's try fitting a paraboloa and see how well it does. As a first step we're going to standardize the data, meaning we'll subtract the mean (as always) and also divide by the standard deviation*. Why? Because it makes the intercept interpretable (as with zero-centreing) and because it places things on a small-ish scale near zero, where likelhoods don't explode. \n",
    "\n",
    "\\*Note I typically standardize by 2sd because it makes 0-1 (dummy) variables comparable with standardized variables, allowing us to look at **relative** effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize weights\n",
    "kweights = (kdata.weight.values-np.mean(kdata.weight.values))/np.std(kdata.weight.values)\n",
    "plt.hist(kweights)\n",
    "plt.xlabel('Height (z-score)')\n",
    "plt.title('!Kung heights from Nancy Howell')\n",
    "plt.savefig('zkung.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our model:\n",
    "\n",
    "$$\n",
    "\\large{\n",
    "\\begin{align*}\n",
    "z_i &= \\frac{x_i-\\bar{x}}{SD(x)} \\\\\n",
    "h_i &\\sim N(\\mu_i,\\sigma) \\\\\n",
    "\\mu_i &= \\beta_0 + \\beta_1z_i + \\beta_2z^{2}_i \\\\\n",
    "\\beta_0 &\\sim N(178,20) \\\\\n",
    "\\beta_1 &\\sim logN(0,1) \\\\\n",
    "\\beta_2 &\\sim N(0,1) \\\\\n",
    "\\sigma &\\sim U(0, 50)\n",
    "\\end{align*}}\n",
    "$$\n",
    "\n",
    "We've added the quadratic parameter $\\beta_2$, for which we have specified a $N(0,1)$ prior. Why? Weelll based on the recommendations of our modern-day Bayesian Yoda [Andrew Gelman](http://www.stat.columbia.edu/~gelman/), who very often standardizes covariates and uses $N(0,1)$ priors in his own work. He's done the legwork, so we take it and move on.\n",
    "\n",
    "Coding this into model, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in PyMC\n",
    "with pm.Model() as kung:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Average_height', 178, 20)\n",
    "    β1 = pm.Lognormal('Weight', 0, 1)\n",
    "    β2 = pm.Normal('Weight2', 0, 1)\n",
    "    σ = pm.Uniform('Obs_sd', 0, 50)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0+β1*kweights+β2*kweights**2\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kdata.height.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "with kung:\n",
    "    trace_p2 = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = trace_p2.posterior.Average_height.values.flatten()\n",
    "b1 = trace_p2.posterior.Weight.values.flatten()\n",
    "b2 = trace_p2.posterior.Weight2.values.flatten()\n",
    "sig = trace_p2.posterior.Obs_sd.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kweights, kdata.height)\n",
    "xnew = np.linspace(min(kweights),max(kweights),100)\n",
    "y_ = b0.mean()+b1.mean()*xnew+b2.mean()*xnew**2\n",
    "y_uu = y_+sig.mean()*2\n",
    "y_ul = y_-sig.mean()*2\n",
    "plt.plot(xnew, y_, c='black')\n",
    "plt.plot(xnew, y_uu, \":\", c='black')\n",
    "plt.plot(xnew, y_ul, \":\", c='black')\n",
    "plt.xlabel('Weight (z-score)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('2ndorder.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of plotting the intervals is to to make random draws from the posterior distribution, which contains a multitude of wiggly lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kweights, kdata.height.values)\n",
    "xnew = np.linspace(min(kweights),max(kweights),100)\n",
    "for i in range(100):\n",
    "    b0_ = np.random.choice(b0)\n",
    "    b1_ = np.random.choice(b1)\n",
    "    b2_ = np.random.choice(b2)\n",
    "    y_ = b0_+b1_*xnew+b2_*xnew**2\n",
    "    plt.plot(xnew, y_, c='black', alpha=0.1)\n",
    "plt.xlabel('Weight (z-score)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key part of polynomials is that they often do a horrific job outside of the data, flopping around everywhere. This is clear if we look at slightly heavier people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kweights, kdata.height.values)\n",
    "xnew = np.linspace(min(kweights),max(kweights)+0.5,100)\n",
    "for i in range(100):\n",
    "    b0_ = np.random.choice(b0)\n",
    "    b1_ = np.random.choice(b1)\n",
    "    b2_ = np.random.choice(b2)\n",
    "    y_ = b0_+b1_*xnew+b2_*xnew**2\n",
    "    plt.plot(xnew, y_, c='black', alpha=0.1)\n",
    "plt.xlabel('Weight (z-score)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('fat2ndorder.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(shrinking large people...)\n",
    "\n",
    "What about 3rd order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in PyMC\n",
    "with pm.Model() as kung3:\n",
    "    # Priors\n",
    "    β0 = pm.Normal('Average_height', 178, 20)\n",
    "    β1 = pm.Lognormal('Weight', 0, 1)\n",
    "    β2 = pm.Normal('Weight2', 0, 1)\n",
    "    β3 = pm.Normal('Weight3', 0, 1)\n",
    "    σ = pm.Uniform('Obs_sd', 0, 50)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0+β1*kweights+β2*kweights**2+β3*kweights**3\n",
    "    \n",
    "    # Likelihood\n",
    "    yi = pm.Normal('yi',μ, σ, observed=kdata.height.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "with kung3:\n",
    "    trace_p3 = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = trace_p3.posterior.Average_height.values.flatten()\n",
    "b1 = trace_p3.posterior.Weight.values.flatten()\n",
    "b2 = trace_p3.posterior.Weight2.values.flatten()\n",
    "b3 = trace_p3.posterior.Weight3.values.flatten()\n",
    "sig = trace_p3.posterior.Obs_sd.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kweights, kdata.height)\n",
    "y_ = y_ = b0.mean()+b1.mean()*xnew+b2.mean()*xnew**2+b3.mean()*xnew**3\n",
    "y_uu = y_+sig.mean()*2\n",
    "y_ul = y_-sig.mean()*2\n",
    "plt.plot(xnew, y_, c='black')\n",
    "plt.plot(xnew, y_uu, \":\", c='black')\n",
    "plt.plot(xnew, y_ul, \":\", c='black')\n",
    "plt.xlabel('Weight (z-score)', fontsize=17)\n",
    "plt.ylabel('Height (cm)', fontsize=17)\n",
    "plt.savefig('3rdorder.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splines\n",
    "\n",
    "![](splines.png)\n",
    "\n",
    "\n",
    " - Basis-Splines: wiggly function build from many local less wiggly functions\n",
    " - Basis function: a local function\n",
    " - Better than polynomials, but equally geocentric\n",
    " - Bayesian B-splines called *P-splines*\n",
    " \n",
    "So what does these B-splines look like? Well they're just linear models that have synthetic variables (B's):\n",
    "\n",
    "$$\n",
    "\\mu_i = \\beta_0 + w_1 B_{i,1}+ w_2 B_{i,2}+...++ w_n B_{i,n}\n",
    "$$\n",
    "\n",
    "w - are weights that are just like slopes, while the basis functions turn on these weights for specific regions of *x*. \n",
    "\n",
    "To make this clear, let's have a look at the Japanese cherry blossom data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = pd.read_csv('https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/cherry_blossoms.csv',sep=\";\")\n",
    "tmp,tmp2 = sum(cdata.temp.isna()),len(cdata.temp)\n",
    "# Drop rows where temp is NA\n",
    "cdata = cdata[~cdata.temp.isna()]\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp,tmp2, cdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.scatter(cdata.year, cdata.temp)\n",
    "plt.axhline(max(cdata.temp.values[cdata.year.values<1800]),c='red')\n",
    "plt.xlabel('Year', fontsize=17)\n",
    "plt.ylabel('March temp', fontsize=17)\n",
    "plt.savefig('blossoms.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of descriptive statistics\n",
    "cdata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we're going to put splines through this, how do they work? Well the algorithm needs to:\n",
    "\n",
    " - choose knots (places where the spline is anchored)\n",
    " - choose degree of basis functions (how wiggly)\n",
    " - find posterior distribution of the weights\n",
    " \n",
    "So to start, let's pick some arbitrary knots across equal quantiles across the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of knots\n",
    "nk = 5\n",
    "# Knot locations\n",
    "naughts = np.quantile(cdata.year, q=np.linspace(0,1, num=nk))\n",
    "naughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we chose the degree of basis functions?\n",
    "\n",
    "Well, can start with basis fucntions which are degree 1, which make them a linear combination at 2 points:\n",
    "\n",
    "![](basis1.jpg)\n",
    "\n",
    "To get our basis functions, we can use the `dmatrix` function in `patsy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Basis function matrix: https://patsy.readthedocs.io/en/latest/spline-regression.html#general-b-splines\n",
    "B = patsy.dmatrix(\"bs(year, knots=knots, degree=1, include_intercept=True)\",\n",
    "                   data={'year': cdata.year.values,\n",
    "                         'knots': naughts[1:-1]})\n",
    "# Construct the Basis function matrix: https://patsy.readthedocs.io/en/latest/spline-regression.html#general-b-splines\n",
    "B2 = patsy.dmatrix(\"bs(year, knots=knots, degree=2, include_intercept=True)\",\n",
    "                   data={'year': cdata.year.values,\n",
    "                         'knots': naughts[1:-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(b) for b in B.T[1:]]\n",
    "plt.title('1st degree basis function')\n",
    "plt.axvline(400,c='black')\n",
    "plt.savefig('basis_fun.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[plt.plot(b) for b in B2.T[1:]]\n",
    "plt.title('2nd degree basis function')\n",
    "plt.axvline(400,c='black')\n",
    "plt.savefig('basis_fun2.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The werid thing about this is that it is just a linear model, which we can calculate in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model PyMC\n",
    "with pm.Model() as temps:\n",
    "    # Priors\n",
    "    b0 = pm.Normal('b0', 6, 10)\n",
    "    w = pm.Normal('w', 0, 1, shape=B.shape[1])\n",
    "    mu = pm.Deterministic('mu', b0 + pm.math.dot(np.asarray(B), w.T))\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    T = pm.Normal('T', mu, sigma, observed=cdata.temp.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "with temps:\n",
    "    trace_t = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at our equation:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\beta_0 + w_1 B_{i,1}+ w_2 B_{i,2}+...++ w_n B_{i,n}\n",
    "$$\n",
    "\n",
    "we 'simply' need to add these all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the posterior means\n",
    "b0 = trace_t.posterior.b0.values.mean()\n",
    "w = trace_t.posterior.w.values.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for mean spline value at each observation\n",
    "spline_ = np.array([np.mean(x) for x in b0+np.dot(np.asarray(B), w.T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.scatter(cdata.year, cdata.temp)\n",
    "plt.plot(cdata.year, spline_, c='black', linewidth=3)\n",
    "plt.axhline(max(cdata.temp.values[cdata.year.values<1800]),c='red')\n",
    "plt.xlabel('Year', fontsize=17)\n",
    "plt.ylabel('March temp', fontsize=17)\n",
    "plt.savefig('blossom_spline.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at 2nd order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in PyMC\n",
    "with pm.Model() as temps2:\n",
    "    # Priors\n",
    "    b0 = pm.Normal('b0', 6, 10)\n",
    "    w = pm.Normal('w', 0, 1, shape=B2.shape[1])\n",
    "    mu = pm.Deterministic('mu', b0 + pm.math.dot(np.asarray(B2), w.T))\n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    T = pm.Normal('T', mu, sigma, observed=cdata.temp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "with temps2:\n",
    "    trace_t2 = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the posterior means\n",
    "b0 = trace_t2.posterior.b0.values.mean()\n",
    "w = trace_t2.posterior.w.values.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for mean spline value at each observation\n",
    "spline_ = np.array([np.mean(x) for x in b0+np.dot(np.asarray(B2), w.T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.scatter(cdata.year, cdata.temp)\n",
    "plt.plot(cdata.year, spline_, c='black', linewidth=3)\n",
    "plt.axhline(max(cdata.temp.values[cdata.year.values<1800]),c='red')\n",
    "plt.xlabel('Year', fontsize=17)\n",
    "plt.ylabel('March temp', fontsize=17)\n",
    "plt.savefig('blossom_spline2.jpg', dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
