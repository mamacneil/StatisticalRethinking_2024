{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Lecture 1 - Models with memory\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=SocRgsf202M&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=13\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il\n",
    "\n",
    "def invlogit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical (multi-level) models\n",
    "\n",
    "And now we have arrived at the good stuff - the most applicable aspect of Bayesian statistics. Hierarchical models. They are spectacularly useful because the world itself is hierarhical in space and time. Being able to recognize this, and build models that leverage it to improve predictive performance, lies at the heart of what we're about. So what do hierarhical models do? They share information among related groups or, put differently, they don't forget what each part a model does when they're estimating the other parts. **They should be your default approach**. Why? Because they regularize by default, helping to generalized for out of sample prediction, which is typically what we want to do. \n",
    "\n",
    "Let's begin with an example of tadpole predation where we have an experimental treatment relating to the numbers of tadpoles surviving in experimental tanks of varying sizes, with various densities of other tadpoles and predators. This kind of experiment is to tease apart the effects of density-dependence (self limitation of a population due to limitation of resources) and predation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = pd.read_csv('reedfrogs.csv')\n",
    "dfi.export(tdata.head(), 'tdata.jpg')\n",
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up our introduction to hierarhical models, we'll develop three models looking at variation in survival among tanks: \n",
    "\n",
    "1. Treats every tank as independent (the **no pooling** model of *fixed effects* for each tank)\n",
    "2. Treat every tank as all the same (the **complete pooling** model for a single, overall effect)\n",
    "3. Treat every tank as a sample (the **partial pooling** *multi-level* (*hierarchical*) model of *random effects*)\n",
    "\n",
    "I'm going to stick to using the word hierarchical because it describes (to me at least) what's going on in these models in terms of the dependencies within the data. 'Multi-level' is often used too, but it's less clear and 'random effects' are also used but this gets us further still from a word people actually use and obfuscates the nestedneess of the data. \n",
    "\n",
    "In Bayesian notation, our three models are:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_i \\sim & Bin(D_i,p_i) \\\\\n",
    "logit(p_i) = & \\beta_{t} \\\\\n",
    "\\beta_{t} = & N(0, 1.5)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with independent priors for each tank ($t$) in the no pooling model. For the complete pooling model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_i \\sim & Bin(D_i,p_i) \\\\\n",
    "logit(p_i) = & \\beta_{0} \\\\\n",
    "\\beta_{0} = & N(0, 1.5)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the partial pooling pooling model we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_i \\sim & Bin(D_i,p_i) \\\\\n",
    "logit(p_i) = & \\beta_{t} \\\\\n",
    "\\beta_{t} \\sim & N(\\mu, \\sigma) \\\\\n",
    "\\mu \\sim & N(0, 1.5) \\\\\n",
    "\\sigma \\sim & E(1).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can then translate these into three Bayesian models in PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab tank numbers\n",
    "It = tdata.index.values\n",
    "ntank = len(It)\n",
    "\n",
    "# Grab tank densities\n",
    "D = tdata.density.values\n",
    "\n",
    "# Grab tank survival\n",
    "S = tdata.surv.values\n",
    "\n",
    "# Raw proportions surviving in each tank\n",
    "psurv = S/D\n",
    "\n",
    "# Predator indicator\n",
    "Ip = (tdata.pred.values=='pred')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No pooling model\n",
    "with pm.Model() as no_pool:\n",
    "    # Individual tank intercepts\n",
    "    βt = pm.Normal('Tank', 0, 1.5, shape=ntank)\n",
    "    \n",
    "    # Linear model\n",
    "    p = pm.invlogit(βt)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Binomial('Yi',D, p, observed=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pooling model\n",
    "with pm.Model() as pooled:\n",
    "    # Overall mean\n",
    "    β0 = pm.Normal('Overall_mean', 0, 1.5)\n",
    "    \n",
    "    # Linear model\n",
    "    p = pm.invlogit(β0)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Binomial('Yi',D, p, observed=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial pooling model\n",
    "with pm.Model() as par_pool:\n",
    "    # Overall mean\n",
    "    μ = pm.Normal('Overall_mean', 0, 1.5)\n",
    "    # Among-tank variation\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Tank-level intercepts\n",
    "    βt = pm.Normal('Tank', μ, σ, shape=ntank)\n",
    "    \n",
    "    # Linear model\n",
    "    p = pm.invlogit(βt)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Binomial('Yi',D, p, observed=S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these models in place, we can hit the magic inference button:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with no_pool:\n",
    "    trace_np = pm.sample(1000)\n",
    "with pooled:\n",
    "    trace_p = pm.sample(1000)\n",
    "with par_pool:\n",
    "    trace_pp = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at what WAIC says about the relative weights of evidence for these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with no_pool:\n",
    "    pm.compute_log_likelihood(trace_np)\n",
    "with pooled:\n",
    "    pm.compute_log_likelihood(trace_p)\n",
    "with par_pool:\n",
    "    pm.compute_log_likelihood(trace_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dict = {\"no pooling\": trace_np, \"partial pooling\": trace_pp, \"pooled\":trace_p}\n",
    "tmp = az.compare(compare_dict)\n",
    "dfi.export(tmp, 'polewaic.jpg')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge win for the partial pooling model - why? Well let's dig in and take a look at the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.scatter(np.arange(1, 49)[Ip==0], psurv[Ip==0], label='No predators')\n",
    "ax.scatter(np.arange(1, 49)[Ip==1], psurv[Ip==1], c='red', label='Predators')\n",
    "ax.vlines([16.5, 32.5], -.05, 1.05, lw=.5)\n",
    "ax.text(8, 0, \"Small tanks (n=10)\", horizontalalignment='center')\n",
    "ax.text(16+8, 0, \"Medium tanks (n=25)\", horizontalalignment='center')\n",
    "ax.text(32+8, 0, \"Large tanks (n=35)\", horizontalalignment='center')\n",
    "ax.set_xlabel('Tank ID', fontsize=14)\n",
    "ax.set_ylabel('P(survival)', fontsize=14)\n",
    "ax.set_xlim(-1, 50)\n",
    "ax.set_ylim(-.05, 1.05)\n",
    "ax.legend()\n",
    "plt.savefig('tanks.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot of the raw data, it is clear that - where predators are present (red dots) - the proportion of tadpoles surviving in each tank declines as the initial density of tadpoles increases from small to large. We can also see that  that density doesn't appear to have much of an effect where predators are absent as all the blue dots (bar one low one amongst the small density tanks) are about the same. \n",
    "\n",
    "So why is the partial pooling model favoured here? Well this plot makes it clear there are differences in survival among tanks, making the complete pooling model less likely (it assumes they all have about the same mean, or at least a normal distribution of variantion on the log-odds scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sp.special.logit(psurv-0.01))\n",
    "plt.xlabel('Observed log-odd of survial')\n",
    "plt.savefig('los.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is clearly not the case). The no pooling model assumes that all of the tanks are just independent, with no relatinoship among them, in which case we'd expect to see no predator/non-predator clustering and a random assortment of survival amongst tank densities. This clearly isn't the case either. Even though we haven't included either the predator effects in our model, the partial-pooling model does better than the other extreemes because it recognizes there is some intrinisc structre in terms of expected survival - in this case that the tank densities (and therefore the amount of information present) vary among tanks. To see this more clearly we can plot the expected values under the partial pooling model against their observed proportions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab partial pooled expected values on the probability scale\n",
    "post = trace_pp.posterior['Tank'].to_dataframe().groupby(level=['Tank_dim_0']).apply(np.median)\n",
    "Esurv = invlogit(post.values)\n",
    "\n",
    "# Grab no pooling expected values on the probability scale\n",
    "postnp = trace_np.posterior['Tank'].to_dataframe().groupby(level=['Tank_dim_0']).apply(np.median)\n",
    "Esurvnp = invlogit(postnp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ax.scatter(np.arange(1, 49)[Ip==0], psurv[Ip==0])\n",
    "ax.scatter(np.arange(1, 49)[Ip==1], psurv[Ip==1], c='red')\n",
    "ax.scatter(np.arange(1, 49), Esurv, facecolors='none', edgecolors='k', lw=1, label='E(PP)')\n",
    "ax.scatter(np.arange(1, 49), Esurvnp, c='black', s=5, label='E(NP)')\n",
    "ax.hlines(invlogit(np.median(trace_pp.posterior['Overall_mean'], axis=1)), 0, 49, linestyles='--', label='Partial Pool', color='black')\n",
    "ax.hlines(invlogit(np.median(trace_p.posterior['Overall_mean'], axis=1)), 0, 49, linestyles=':', label='Pooled')\n",
    "ax.hlines(invlogit(np.median(trace_np.posterior['Tank'])), 0, 49, color='darkorange', linestyles='-.', label='No pool')\n",
    "ax.hlines(invlogit(np.median(psurv)), 0, 49, label='Observed')\n",
    "ax.vlines([16.5, 32.5, 50], -.05, 1.05, lw=.5)\n",
    "ax.text(8, 0, \"Small tanks (n=10)\", horizontalalignment='center')\n",
    "ax.text(16+8, 0, \"Medium tanks (n=25)\", horizontalalignment='center')\n",
    "ax.text(32+8, 0, \"Large tanks (n=35)\", horizontalalignment='center')\n",
    "ax.set_xlabel('Tank ID', fontsize=14)\n",
    "ax.set_ylabel('P(survival)', fontsize=14)\n",
    "ax.set_xlim(-1, 58)\n",
    "ax.set_ylim(-.05, 1.05)\n",
    "plt.legend()\n",
    "plt.title('Fits with non-pool ~N(0, 1.5) prior')\n",
    "plt.savefig('modelfits.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to think about these results:\n",
    "\n",
    "*Complete pooling is maximum* **underfitting** - ignoring all the actual heterogenity in the population; model is too simple (*i.e.* set σ to 0)\n",
    "\n",
    "*No pooling is (close to) maximum* **overfitting** - focused entirely on the heterogenity and using a tiny amount of data to fit each parameter; model is too ignorant and learns nothing from the population of tanks (*i.e.* σ goes to $\\infty$)\n",
    "\n",
    "*Partial pooling is* **adaptive regularization** - we learn the level of regularization from the data itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigma posterior\n",
    "sigma_posterior = trace_pp.posterior['Sigma'][0]\n",
    "# Sigma prior\n",
    "sigma_prior = np.random.exponential(1,len(sigma_posterior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "sns.kdeplot(sigma_posterior,label='Posterior')\n",
    "sns.kdeplot(sigma_prior,label='Prior')\n",
    "ax.tick_params(labelleft=False)\n",
    "plt.text(21.5,0.1,'Infinity (frequentism, no pooling) ->')\n",
    "plt.text(0.1,0.99,'0 = Complete pooling', rotation=90)\n",
    "plt.legend()\n",
    "plt.xlim(0,30)\n",
    "plt.ylabel('')\n",
    "plt.savefig('priorpost.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left hand side is zero - the complete pooling model - and on the right side (eventually) is infinity - the no pooling model. In between we have something that is not these things - the partial pooled model - and the question is, how much partial pooling should there be? Well the maximum entropy answer is the Normal distribution, where the only constraint is finite variance. Beyond that we can let the data decide the appropriate level of regularization, which in this case is estimated at about 1.6 in the blue line above. So why do we want to regularize? Because in doing so we can make better out of sample predictions. Let's try this by simulating a new 'mini tank' with a density of 5 tadpoles along with our other tank densities and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall mean\n",
    "b0_ = 1.4\n",
    "# Inter-tank variation \n",
    "sigma_ = 1.5\n",
    "# Number of new tanks\n",
    "ntanks = 60\n",
    "# Density in each tank\n",
    "D_ = np.repeat([5, 10, 25, 35], 15)\n",
    "\n",
    "# Linear model for new data\n",
    "mu_ = np.random.normal(b0_, sigma_, size=ntanks)\n",
    "\n",
    "# Link\n",
    "ptrue = invlogit(mu_)\n",
    "\n",
    "# New survival data\n",
    "S_ = np.random.binomial(D_, ptrue)\n",
    "\n",
    "# Survival proportion\n",
    "psurv_ = S_/D_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No pooling model\n",
    "with pm.Model() as no_pool_sim:\n",
    "    # Individual tank intercepts\n",
    "    βt = pm.Normal('Tank', 0, 1.5, shape=ntanks)\n",
    "    \n",
    "    # Linear model\n",
    "    p = pm.invlogit(βt)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Binomial('Yi',D_, p, observed=S_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial pooling model\n",
    "with pm.Model() as par_pool_sim:\n",
    "    # Overall mean\n",
    "    μ = pm.Normal('Overall_mean', 0, 1.5)\n",
    "    # Among-tank variation\n",
    "    σ = pm.Exponential('Sigma', 1)\n",
    "    \n",
    "    # Tank-level intercepts\n",
    "    βt = pm.Normal('Tank', μ, σ, shape=ntanks)\n",
    "    \n",
    "    # Linear model\n",
    "    p = pm.invlogit(βt)\n",
    "    \n",
    "    # Data likelihood\n",
    "    Yi = pm.Binomial('Yi',D_, p, observed=S_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with no_pool_sim:\n",
    "    trace_nps = pm.sample(1000)\n",
    "with par_pool_sim:\n",
    "    trace_pps = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get the expected values for the individual tank means, and plot them relative to their absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected values\n",
    "p_no = invlogit(np.median(trace_nps.posterior['Tank'],axis=0)[0])\n",
    "p_partial = invlogit(np.median(trace_pps.posterior['Tank'],axis=0)[0])\n",
    "\n",
    "# Absolute error\n",
    "ae_no = abs(p_no-ptrue)\n",
    "ae_pp = abs(p_partial-ptrue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "# No pooled absolute errors\n",
    "ax.scatter(np.arange(ntanks)+1, ae_no, label='No pool')\n",
    "# Partial pooled absolute errors\n",
    "ax.scatter(np.arange(ntanks)+1, ae_pp, facecolors='none', edgecolors='k', lw=1, label='Partial')\n",
    "# Averages per tank density\n",
    "xrange = np.arange(60)\n",
    "xrange_ = xrange.reshape((4, 15))\n",
    "for ix in range(4):\n",
    "    ax.hlines(ae_no[xrange_[ix, :]].mean(), xrange_[ix, 0]+1, xrange_[ix, -1]+1, color='C0')\n",
    "    ax.hlines(ae_pp[xrange_[ix, :]].mean(), xrange_[ix, 0]+1, xrange_[ix, -1]+1, color='k', linestyles='--')\n",
    "\n",
    "# Prettify\n",
    "ax.axhline(0,c='black', linewidth=0.5)\n",
    "ax.vlines(xrange_[1:,0]+.5, -.05, 0.35, lw=.5)\n",
    "ax.text(8, -.03, \"Mini tanks (n=5)\", horizontalalignment='center')\n",
    "ax.text(16+8, -.03, \"Small tanks (n=10)\", horizontalalignment='center')\n",
    "ax.text(32+8, -.03, \"Medium tanks (n=25)\", horizontalalignment='center')\n",
    "ax.text(48+8, -.03, \"Large tanks (n=35)\", horizontalalignment='center')\n",
    "ax.set_xlabel('Tank ID', fontsize=14)\n",
    "ax.set_ylabel('Absolute error', fontsize=14)\n",
    "ax.set_xlim(-1, 62)\n",
    "ax.set_ylim(-.05, .35)\n",
    "plt.legend()\n",
    "plt.savefig('modelfits3.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's crazy here is how much things are improved when there is little data available - in the density=5 tanks, the improvement is huge; among the others the differences decrease as the data become more information rich (due to randomness of the starting values for the simulation your results may differ slightly but re-run to see how on average this is the case)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
