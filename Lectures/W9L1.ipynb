{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 9 Lecture 1 - Adventures in covariance\n",
    "\n",
    "McElreath's lectures for today: https://www.youtube.com/watch?v=XDoAglqd7ss&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=15\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An R/Stan repo of code is available here: https://vincentarelbundock.github.io/rethinking2/\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/pymc-devs/resources/tree/master/Rethinking\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "import random as rd\n",
    "import pdb\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse, transforms\n",
    "import dataframe_image as dfi\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def stdize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo,Ix\n",
    "\n",
    "def indexall_(L):\n",
    "    Il, Ll = pd.factorize(L, sort=True)\n",
    "    return Ll, Il\n",
    "\n",
    "def Gauss2d(mu, cov, ci, ax=None, ec='k'):\n",
    "    \"\"\"Copied from statsmodel\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    v_, w = np.linalg.eigh(cov)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    anglex = 180 + 180 * angle / np.pi  # convert to degrees\n",
    "    for level in ci:\n",
    "        v = 2 * np.sqrt(\n",
    "            v_ * sp.stats.chi2.ppf(level, 2)\n",
    "        )  # get size corresponding to level\n",
    "        ell = Ellipse(mu[:2], v[0], v[1], angle=anglex, \n",
    "                      facecolor=\"None\", edgecolor=ec, alpha=(1 - level) * 0.5,\n",
    "            lw=1.5,\n",
    "        )\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying intercepts, varying slopes\n",
    "\n",
    "In linear models, we have intercepts, which describe means of groups, and we have slopes, that characterize effects of covariates. It's important to recognize that both of these (as well as their variances) can be built up hierarchicaly:\n",
    "\n",
    "1. *Varying intercepts* - means vary by cluster\n",
    "2. *Varying slopes* - effects of predictors vary by cluster\n",
    "3. *Varying variances* - variabitliy of slopes or intercepts vary by cluster\n",
    "\n",
    "And so any parameter can be a varying effect, which is simply split into a vector of parameters by cluster that have their own parent distribution.\n",
    "\n",
    "To make this concrete, let's simulate some data relating to waiting times in European cafes, where the cafes have inherrent variation in waiting times (intercepts), as well as in how much that waiting time varies from morning to afternoon (slopes). What makes this an adventure in covariance is that we're going to assert, in our simulation, that the intercepts and slopes are correlated in a predictible way ($\\rho=-0.7$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average (Global) morning waiting time\n",
    "g0 = 3.5\n",
    "# Average (Global) difference in afternoon waiting time\n",
    "g1 = -1.\n",
    "# Standard deviation of cafe-level variation in intercepts\n",
    "sigma_g0 = 1.2\n",
    "# Standard deviation of cafe-level variation in slopes\n",
    "sigma_g1 = 0.5\n",
    "# Correlation between cafe intercepts and slopes\n",
    "rho = -0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our global known parameters, applicable to any cafe in Europe. Now we can take a sample by building up the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc}\n",
    "\\sigma^{2}_{g0} & \\sigma_{g0}\\sigma_{g1}\\rho \\\\\n",
    "\\sigma_{g0}\\sigma_{g1}\\rho & \\sigma^{2}_{g1}\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build covariance matrix\n",
    "sigmas = [sigma_g0, sigma_g1]\n",
    "Rho = np.matrix([[1,rho],[rho,1]])\n",
    "COV = np.diag(sigmas)*Rho*np.diag(sigmas)\n",
    "COV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulating from a multivariate normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cafes\n",
    "ncafe = 20\n",
    "# Cafe-level parameters\n",
    "cafe_effects = np.random.multivariate_normal([g0,g1], COV, size=ncafe)\n",
    "b0 = cafe_effects[:, 0]\n",
    "b1 = cafe_effects[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have cafe-level intercepts and slopes that we know are negatively correlated by $-0.7$, which we can visualize in a bivariate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot size\n",
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# Fancy crcles to plot MvNormal\n",
    "Gauss2d([g0,g1] , np.asarray(COV), [0.1, 0.3, 0.5, 0.8, 0.99], ax=ax, ec='k')\n",
    "# Scatterplot of values\n",
    "ax.scatter(b0, b1)\n",
    "ax.set_xlim(1.5, 6.1)\n",
    "ax.set_ylim(-2, 0)\n",
    "ax.set_xlabel('Mornings (b0)')\n",
    "ax.set_ylabel('Afternoons (b1)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('cafesim.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with this in place, we can now simulate some days of observation among our cafes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observed days per cafe\n",
    "nvisit = 10\n",
    "\n",
    "# am/pm indicator\n",
    "PM = np.tile([0,1], nvisit*ncafe//2)\n",
    "\n",
    "# Keep track of cafe\n",
    "Ic = np.repeat(np.arange(0, ncafe), nvisit)\n",
    "\n",
    "# Expected value per cafe\n",
    "mu_ = b0[Ic]+b1[Ic]*PM\n",
    "\n",
    "# Variation within cafes\n",
    "sigma_ = 0.5\n",
    "\n",
    "# Observed wait times\n",
    "wait = np.random.normal(mu_, sigma_, size=nvisit*ncafe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data in place, we can now build our model, which based on what we've simulated, should be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W_i \\sim & N(\\mu_i, \\sigma)\\\\\n",
    "\\mu_i = & \\beta_{0}+\\beta_{1}PM \\\\\n",
    "\\left[\\begin{array}{c}\n",
    "\\beta_{0} \\\\\n",
    "\\beta_{1} \n",
    "\\end{array}\\right] = & MvN \\left(\\left[\\begin{array}{c}\n",
    "\\gamma_{0} \\\\\n",
    "\\gamma_{1} \n",
    "\\end{array}\\right], S \\right) \\\\\n",
    "S = & \\left(\\begin{array}{cc}\n",
    "\\sigma_{g0} & 0 \\\\\n",
    "0 & \\sigma_{g1} \n",
    "\\end{array}\\right) R \\left(\\begin{array}{cc}\n",
    "\\sigma_{g0} & 0 \\\\\n",
    "0 & \\sigma_{g1} \n",
    "\\end{array}\\right) \\\\\n",
    "\\gamma_0 \\sim & N(5,2) \\\\\n",
    "\\gamma_1 \\sim & N(-1,0.5) \\\\\n",
    "\\sigma, \\sigma_{g0}, \\sigma_{g1} \\sim & Exp(1) \\\\\n",
    "R \\sim & LKJcorr(2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There's a lot here - the first bit is the multivariate normal structure, as outlined when we simulated the data above. The middle bit should be familiar, but the key bit comes at the end, the $LKJcorr(2)$, what's that? Well it is something called the LKJ distribution, named for [Lewandowski, Kurowicka, and Joe (2009)](https://www.sciencedirect.com/science/article/pii/S0047259X09000876) who came up with it. What it is is a good general prior for Bayesian multivariate normal distributions. What does it look like? Well let's take a look by simulating some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "textloc = [[0, .56], [0, .8], [.4, .9]]\n",
    "for eta, loc in zip([1, 2, 4], textloc):\n",
    "    R = pm.LKJCorr.dist(n=2, eta=eta, size=1000).eval()\n",
    "    sns.kdeplot(data=R);\n",
    "    plt.text(loc[0], loc[1], 'η = %s'%(eta), horizontalalignment='center')\n",
    "\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xlim(-1,1)\n",
    "ax.set_xlabel('Correlation')\n",
    "ax.set_ylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.savefig('LKJcorr.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the η parameter guides the level of potential correlation, between -1 and 0. Larger η values correspond to values more skeptical of highly negative or positive correlations. \n",
    "\n",
    "With all this in place, we can now build our hierarhical model, using the multivariate normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pm.expand_packed_triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as MvCafes:\n",
    "    # Hyperprior for LKJ\n",
    "    sd_dist = pm.HalfCauchy.dist(2)\n",
    "    chol, corr, sigmas = pm.LKJCholeskyCov('chol_cov', eta=2, n=2, sd_dist=sd_dist)\n",
    "    \n",
    "    # Covariance matrix\n",
    "    cov = pm.math.dot(chol, chol.T)\n",
    "    \n",
    "    # Keep track of sigmas\n",
    "    Sigs = pm.Deterministic('sigma_cafe', sigmas)\n",
    "    \n",
    "    # Extract the standard deviations and rho so we can see how clever we are later\n",
    "    r = pm.Deterministic('Rho', corr[np.triu_indices(2, k=1)])\n",
    "    \n",
    "    # Global intercept\n",
    "    γ0 = pm.Normal('global_intercept', 5, 2)\n",
    "    # Global slope\n",
    "    γ1 = pm.Normal('global_slope', -1, 0.5)\n",
    "    \n",
    "    # Cafe intercepts and slopes\n",
    "    β = pm.MvNormal('ab_cafe', mu=[γ0, γ1], chol=chol, shape=(ncafe, 2))\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β[:, 0][Ic]+β[:, 1][Ic]*PM\n",
    "    \n",
    "    # Data likelihood\n",
    "    σ = pm.HalfCauchy('sigma', beta=2)\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MvCafes:\n",
    "    trace_c = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pm.summary(trace_c, var_names=['global_intercept','global_slope','chol_cov','sigma', 'Rho'])\n",
    "dfi.export(tmp.style.background_gradient(), 'cafemodel.png')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at how we did in estimating ρ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_c, var_names=['Rho'])\n",
    "sns.kdeplot(data=pm.LKJCorr.dist(n=2, eta=eta, size=1000).eval(),linestyle=':',label='Prior')\n",
    "plt.axvline(rho,c='red')\n",
    "plt.title('')\n",
    "plt.legend()\n",
    "plt.xlim(-1,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rho.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, off by a bit.\n",
    "\n",
    "So what else have we gained through use of our bivariate normal? Well shrinkage of course, but now in two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to use pandas\n",
    "df = pd.DataFrame(dict(cafe=Ic , pm=PM , wait=wait))\n",
    "# Unpooled estimates\n",
    "obs_mean = (df.groupby(['pm', 'cafe']).agg('mean').unstack(level=0).values)\n",
    "b0obs = obs_mean[:, 0]\n",
    "b1obs = obs_mean[:, 1] - b0obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial pooled estimates\n",
    "b0_ = trace_c.posterior['ab_cafe'].values[0].mean(0).T[0]\n",
    "b1_ = trace_c.posterior['ab_cafe'].values[0].mean(0).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior bivariate Normal SD\n",
    "chol_model = pm.expand_packed_triangular(2, trace_c.posterior['chol_cov'].values[0].mean(0),lower=True).eval()\n",
    "Sigma_est = np.dot(chol_model, chol_model.T)\n",
    "\n",
    "# Global posterior bivariate Normal slope and intercept\n",
    "Mu_est = [trace_c.posterior['global_intercept'].mean(),trace_c.posterior['global_slope'].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw contours and show shrinkage\n",
    "_, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "Gauss2d(Mu_est, np.asarray(Sigma_est), [0.1, 0.3, 0.5, 0.8, 0.99], ax=ax)\n",
    "ax.scatter(b0obs, b1obs, label='Obs')\n",
    "ax.scatter(b0_, b1_, facecolors='none', edgecolors='k', lw=1, label='Est')\n",
    "ax.plot([b0obs, b0_], [b1obs, b1_], 'k-', alpha=.5)\n",
    "ax.set_xlabel('Intercept', fontsize=14)\n",
    "ax.set_ylabel('Slope', fontsize=14)\n",
    "ax.set_xlim(1.5, 6.1)\n",
    "ax.set_ylim(-2.5, .5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('shrink.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having learned how to do the multivariate normal stuff, I'm going to sneak in some personal experience: in general, the Hamiltonian algorithms are so good with highly correlated varaibles that we often don't need the correlation matrix (unless something important is in there). So let's take a look at just the hierarchical model version on it's own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as Cafes:\n",
    "    # Global intercept\n",
    "    γ0 = pm.Normal('global_intercept', 5, 2)\n",
    "    # Global slope\n",
    "    γ1 = pm.Normal('global_slope', -1, 0.5)\n",
    "    # Variances\n",
    "    σγ0 = pm.Exponential('σγ0',1)\n",
    "    σγ1 = pm.Exponential('σγ1',1)\n",
    "    \n",
    "    # Cafe intercepts and slopes\n",
    "    β0 = pm.Normal('cafe_intercept', γ0, σγ0, shape=ncafe)\n",
    "    β1 = pm.Normal('cafe_slope', γ1, σγ1, shape=ncafe)\n",
    "    \n",
    "    # Linear model\n",
    "    μ = β0[Ic]+β1[Ic]*PM\n",
    "    \n",
    "    # Data likelihood\n",
    "    σ = pm.HalfCauchy('sigma', beta=2)\n",
    "    Yi = pm.Normal('Yi', μ, σ, observed=wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Cafes:\n",
    "    trace_c2 = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with these results we can take a look at how they compare to our multivariate normal results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate intercepts and slopes\n",
    "mv_int = trace_c.posterior['ab_cafe'].values[0].mean(0).T[0]\n",
    "mv_slope = trace_c.posterior['ab_cafe'].values[0].mean(0).T[1]\n",
    "# Hierarhical intercepts and slopes\n",
    "h_int = trace_c2.posterior['cafe_intercept'].values[0].mean(0)\n",
    "h_slope = trace_c2.posterior['cafe_slope'].values[0].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].scatter(b0,mv_int, label='MvNormal')\n",
    "ax[0].scatter(b0,h_int,c='red',label='Not')\n",
    "ax[0].set_xlabel('Intercept', fontsize=15)\n",
    "ax[0].set_ylabel('True value', fontsize=15)\n",
    "ax[0].legend()\n",
    "ax[1].scatter(b1,mv_slope)\n",
    "ax[1].scatter(b1,h_slope,c='red')\n",
    "ax[1].set_xlabel('Slope', fontsize=15)\n",
    "#plt.axhline(trace_c['global_slope'].mean(),linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hlm.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is crazy - when we look at the posteriors for one of our cafe intercepts and slopes, we can see the correlation in both kinds of posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Plot a multivariate normal slope vs intercept\n",
    "ax[0].scatter(trace_c.posterior['ab_cafe'].values[0].T[0][0],trace_c.posterior['ab_cafe'].values[0].T[1][0])\n",
    "ax[0].set_xlabel('Intercept', fontsize=15)\n",
    "ax[0].set_xlabel('Slope', fontsize=15)\n",
    "ax[0].set_title('MvNormal', fontsize=15)\n",
    "\n",
    "# Plot a hierarhical normal slope vs intercept\n",
    "ax[1].scatter(trace_c2.posterior['cafe_intercept'].values[0].T[0],trace_c2.posterior['cafe_slope'].values[0].T[0])\n",
    "ax[1].set_xlabel('Intercept', fontsize=15)\n",
    "ax[1].set_xlabel('Slope', fontsize=15)\n",
    "ax[1].set_title('Not', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happens to our beloved shrinkage? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooled estimates\n",
    "b0_ = trace_c2.posterior['cafe_intercept'].values[0].mean(0)\n",
    "b1_ = trace_c2.posterior['cafe_slope'].values[0].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw contours and show shrinkage\n",
    "_, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "Gauss2d(Mu_est, np.asarray(Sigma_est), [0.1, 0.3, 0.5, 0.8, 0.99], ax=ax, ec='lightgrey')\n",
    "ax.scatter(b0obs, b1obs)\n",
    "ax.scatter(b0_, b1_, facecolors='none', edgecolors='k', lw=1)\n",
    "ax.plot([b0obs, b0_], [b1obs, b1_], 'k-', alpha=.5)\n",
    "ax.set_xlabel('Intercept', fontsize=14)\n",
    "ax.set_ylabel('Slope', fontsize=14)\n",
    "ax.set_xlim(1.5, 6.1)\n",
    "ax.set_ylim(-2.5, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still happens - despite the lack of correlation matrix - by following the gradients. \n",
    "\n",
    "For a bit more insight into Hamiltonian MC, it's worth watching this great talk by Michael Betancourt: https://www.youtube.com/watch?v=VnNdhsm0rJQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
