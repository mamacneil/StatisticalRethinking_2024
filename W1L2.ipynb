{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Lecture 2 - The garden of forking data\n",
    "\n",
    "\n",
    "Throughout the course we will be using these [Jupyter notebooks](https://jupyter.org/) to develop, run, and share Python code. They work with R also.\n",
    "\n",
    "McElreath's lectures for the whole book are available here: https://github.com/rmcelreath/stat_rethinking_2022\n",
    "\n",
    "An excellent port to Python/PyMC Code is available here: https://github.com/dustinstansbury/statistical-rethinking-2023\n",
    "\n",
    "You are encouraged to work through both of these versions to re-enforce what we're doing in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import random as rd\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to understand how Bayes theorem works is to work through it by hand - the grid approximation is a good way to do this because you can see visually how parameters and data interact through the likelhood. It also gives a sense of how priors become posteiors.\n",
    "\n",
    "We'll use the globe tossing example because it conveys so well how probability can be built up and used from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid\n",
    "p_grid = np.linspace(0,1,20)\n",
    "p_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior\n",
    "prior = np.ones(20)\n",
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "prior = prior/sum(prior)\n",
    "prior1 = prior\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the likelihood which, because it is descrete, is given by the probability mass function (PMF)\n",
    "\n",
    "$$\n",
    "\\binom{n}{k}p^{x}(1-p)^{n-x}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\binom{n}{k} = \\frac{n!}{k!(n - k)!}\n",
    "$$\n",
    "\n",
    "which in code is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial as f\n",
    "\n",
    "# Binomial distribution\n",
    "def dbinom(x,n,p):\n",
    "    return f(n)/(f(x)*f(n-x))*p**(x)*(1-p)**(n-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prior over range of p_grid\n",
    "plt.plot(p_grid, prior)\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Plausability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New observations\n",
    "W = 0\n",
    "L = 1\n",
    "# Number of trials\n",
    "N = W+L\n",
    "\n",
    "# Calculate likelihood\n",
    "likelihood = dbinom(W,N,p_grid)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot likelihood over range of p_grid\n",
    "plt.plot(p_grid, likelihood)\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Likelihood');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood*prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing constant\n",
    "sum(likelihood*prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes theorem\n",
    "posterior = (likelihood*prior)/sum(likelihood*prior)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior over range of p_grid\n",
    "plt.plot(p_grid, posterior)\n",
    "plt.plot(p_grid, prior,linestyle=\":\")\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Posterior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update new prior\n",
    "prior = posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can do it all at once, with all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New observations\n",
    "W = \n",
    "L = \n",
    "# Number of trials\n",
    "N = W+L\n",
    "\n",
    "# Calculate likelihood\n",
    "likelihood = dbinom(W,N,p_grid)\n",
    "likelihood\n",
    "# Bayes theorem\n",
    "posterior = (likelihood*prior1)/sum(likelihood*prior1)\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior over range of p_grid\n",
    "plt.plot(p_grid, posterior)\n",
    "plt.plot(p_grid, prior1,linestyle=\":\")\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Posterior');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code we have built a Bayesian model to estimate the proportion of water on the earth's surface by recording the number of times our right hand lands on water versus land, given a default 'ignorant' prior. However our heterogenious educations give us some sense better than ignorance about what proportion of the earth is water. We can encode this information in a new prior and see what the data show. There are many ways to do this, but one way is to weight each value of p_grid by our sense of how likely they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjective prior\n",
    "my_prior = np.array([0,0,0,0,0,0,0,0,0.0,0.,0.,0.,0.,0.,0.,0.0,0.0,0.0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(my_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "my_prior = my_prior/sum(my_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(my_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot my prior over range of p_grid\n",
    "plt.plot(p_grid,my_prior)\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Prior');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes theorem\n",
    "posterior2 = (likelihood*my_prior)/sum(likelihood*my_prior)\n",
    "posterior2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior2 over range of p_grid\n",
    "plt.plot(p_grid, np.ones(20)*0.05, label='Flat prior', c='dodgerblue', linestyle=\":\")\n",
    "plt.plot(p_grid, my_prior, label='My prior', c='red', linestyle=\":\")\n",
    "plt.plot(p_grid, posterior, label='Flat posterior', c='dodgerblue')\n",
    "plt.plot(p_grid, posterior2, label='My posterior', c='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Proportion water'),plt.ylabel('Posterior');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid approximations are helpful for learning how Bayes theorem works but they do not scale well, exponentially in fact relative to the number of parameters in the model. For most models we'll use Markov Chain Monte Carlo (MCMC) methods, but before that the quadratic approximation (QA) is a useful method to learn as well as it is really fast and forms the basis of an important MCMC alternative, the [Laplace approximation](https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html), which is used in [INLA](http://www.r-inla.org/) (with a gentle intro here: https://www.precision-analytics.ca/blog/a-gentle-inla-tutorial/).\n",
    "\n",
    "McElreath has written his own quadratic approximation algorithm `quap()` for finding the normal peak and standard deviation and you can see that on pg. 42 of the book. In Python we can use the `find_MAP()` function in PyMC to do the same thing (`quap` uses `MAP` under the hood). MAP stands for maximum *a posteriori*, and reflects the fact that the algorithm estimates the mode of a posterior, rather than just the likelihood alone. Is is basically a fast optimization algorhithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic approximation using MAP in PyMC\n",
    "with pm.Model() as globe_qa:\n",
    "    p = pm.Uniform('p', 0, 1)\n",
    "    w = pm.Binomial('w', n=N, p=p, observed=W)\n",
    "    mean_q = pm.find_MAP()\n",
    "    std_q = ((1/pm.find_hessian(mean_q, vars=[p]))**0.5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of normal approximation\n",
    "mean_q['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of normal approximation\n",
    "std_q[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One oddity in the code above is the `pm.find_hessian` statement, if you've not seen a Hessian before, it is simply (!) a matrix of second-order partial derivatives that can be used to calculate variance/covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytical calculation\n",
    "y1 = dbinom(W,N,p_grid)\n",
    "plt.plot(p_grid, y1/sum(y1), label='True posterior')\n",
    "\n",
    "# quadratic approximation\n",
    "y2 = sp.stats.norm.pdf(p_grid, mean_q['p'], std_q)\n",
    "plt.plot(p_grid, y2/sum(y2), label='Quadratic approximation')\n",
    "plt.legend()\n",
    "\n",
    "plt.title('n = 9', fontsize=14)\n",
    "plt.xlabel('Proportion water', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
